{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff4423-a728-4006-a911-9edf8b6b0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "\n",
    "#Function to get end of the month date for ONI dataset\n",
    "def get_end_of_month(year, month):\n",
    "    # Get the last day of the month\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    # Return the date as a string in the format YYYY-MM-DD\n",
    "    return f\"{year}-{month:02d}-{last_day:02d}\"\n",
    "\n",
    "# Function to check if a value is non-numeric\n",
    "def is_non_numeric(value):\n",
    "    return not pd.api.types.is_numeric_dtype(type(value))\n",
    "\n",
    "#Function to bin data to monthly\n",
    "def resample(group):\n",
    "    return group.resample('ME').mean()\n",
    "\n",
    "# Get the current working directory \n",
    "current_directory = os.getcwd() \n",
    "\n",
    "# Print the current working directory \n",
    "print(current_directory)\n",
    "\n",
    "# Define the directory containing the files \n",
    "path = current_directory+\"\\\\\"\n",
    "print(path)\n",
    "\n",
    "filename = path + 'Daily_Data_All.csv'\n",
    "\n",
    "results_df = pd.read_csv(filename) #read csv data into a dataframe\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "print(results_df.columns)\n",
    "\n",
    "# Count non-numeric values in the average (mean) wind speed column\n",
    "non_numeric_count = results_df['WDSP'].apply(is_non_numeric).sum()\n",
    "\n",
    "print(f\"Number of non-numeric values: {non_numeric_count}\")\n",
    "\n",
    "# Count non-numeric values in the snow depth column\n",
    "non_numeric_count = results_df['SNDP'].apply(is_non_numeric).sum()\n",
    "\n",
    "print(f\"Number of non-numeric values: {non_numeric_count}\")\n",
    "\n",
    "# Count non-numeric values in the precipitation column\n",
    "non_numeric_count = results_df['PRCP'].apply(is_non_numeric).sum()\n",
    "\n",
    "print(f\"Number of non-numeric values: {non_numeric_count}\")\n",
    "\n",
    "#There are no non-numeric values\n",
    "\n",
    "#Find how many missing values are in each column\n",
    "count = results_df['WDSP'].value_counts().get(999.9, 0)\n",
    "print(count)\n",
    "count = results_df['SNDP'].value_counts().get(999.9, 0)\n",
    "print(count)\n",
    "count = results_df['slp'].value_counts().get(999.9, 0)\n",
    "print(count)\n",
    "count = results_df['SLP'].value_counts().get(999.9, 0)\n",
    "print(count)\n",
    "count = results_df['MXSPD'].value_counts().get(999.9, 0)\n",
    "print(count)\n",
    "count = results_df['PRCP'].value_counts().get(99.99, 0)\n",
    "print(count)\n",
    "count = results_df['GUST'].value_counts().get(999.9, 0)\n",
    "print(count)\n",
    "count = results_df['DEWP'].value_counts().get(999.9, 0)\n",
    "print(count)\n",
    "count = results_df['STP'].value_counts().get(999.9, 0)\n",
    "print(count)\n",
    "count = results_df['MIN'].value_counts().get(9999.9, 0)\n",
    "print(count)\n",
    "count = results_df['MAX'].value_counts().get(9999.9, 0)\n",
    "print(count)\n",
    "count = results_df['TEMP'].value_counts().get(9999.9, 0)\n",
    "print(count)\n",
    "\n",
    "results2_df = results_df.drop(['SLP', 'MXSPD', 'GUST', 'DEWP', 'STP', 'MIN', 'MAX', 'TEMP', 'FRSHTT', 'VISIB', 'MIN_ATTRIBUTES', 'MAX_ATTRIBUTES', 'TEMP_ATTRIBUTES', 'SLP_ATTRIBUTES', 'VISIB_ATTRIBUTES', 'STP_ATTRIBUTES', 'PRCP_ATTRIBUTES', 'DEWP_ATTRIBUTES', 'WDSP_ATTRIBUTES'], axis=1)\n",
    "results2_df = results2_df.drop(['NAME', 'WMO_ID', 'wind_speed', 'NOAA_file_csv'], axis=1) #wind_speed is from NSRDB data. We are using NOAA variable WDSP instead.\n",
    "print(results2_df)\n",
    "# Replace numeric 99.99 values with NaN in PRCP \n",
    "results2_df['PRCP'] = results2_df['PRCP'].replace({99.99: np.nan})\n",
    "# Replace numeric 999.9 values with NaN in WDSP \n",
    "results2_df['WDSP'] = results2_df['WDSP'].replace({999.9: np.nan})\n",
    "# Replace numeric 999.9 values with zero in SNDP \n",
    "results2_df['SNDP'] = results2_df['SNDP'].replace({999.9: 0})\n",
    "print(results2_df)\n",
    "# Function to check if a value is non-numeric\n",
    "def is_non_numeric(value):\n",
    "    return not pd.api.types.is_numeric_dtype(type(value))\n",
    "\n",
    "# Count non-numeric values in the GHI column\n",
    "non_numeric_count = results2_df['GHI'].apply(is_non_numeric).sum()\n",
    "\n",
    "print(f\"Number of non-numeric values: {non_numeric_count}\")\n",
    "\n",
    "# Convert the 'time' column from string to datetime\n",
    "results2_df['date'] = pd.to_datetime(results2_df['time'])\n",
    "\n",
    "results2_df[results2_df['date'] < datetime.strptime(\"2024-12-1\", \"%Y-%m-%d\")].isna().sum()\n",
    "\n",
    "# From binning data from hourly to daily, there are 65 days for which hourly data was missing \n",
    "#for all 24 hours. These rows are missing latitude, longitude, and elevation, which we will fill in\n",
    "#manually in Python using their file_id, so that binning to monthly will not skew location data.\n",
    "\n",
    "# Find rows with NaN values in column 'latitude'\n",
    "nan_rows = results2_df.loc[results2_df['latitude'].isnull()]\n",
    "index = nan_rows.index\n",
    "print(index)\n",
    "\n",
    "filename = path + 'filenames_chosen.csv'\n",
    "\n",
    "locations_df = pd.read_csv(filename) #read location data into a dataframe\n",
    "print(locations_df)\n",
    "results3_df = results2_df\n",
    "\n",
    "#”VLOOKUP” operation to get latitude, longitude, elevation, and region for missing data\n",
    "results3_df = pd.merge(results3_df, locations_df, left_on='file_id', right_on = 'file_no', how='left')\n",
    "\n",
    "print(results3_df)\n",
    "\n",
    "#Confirm location of NaN values by index\n",
    "print(results3_df.loc[22817:22819])\n",
    "\n",
    "#Clean data for location information\n",
    "results3_df['latitude'] = results3_df['latitude_y']\n",
    "results3_df['longitude'] = results3_df['longitude_y']\n",
    "results3_df['elevation'] = results3_df['elevation_y']\n",
    "results3_df = results3_df.drop(['latitude_x', 'longitude_x', 'elevation_x'], axis=1)\n",
    "results3_df = results3_df.drop(['latitude_y', 'longitude_y', 'elevation_y'], axis=1)\n",
    "print(results3_df.columns)\n",
    "\n",
    "results3_df['region'] = results3_df['region_y']\n",
    "results3_df = results3_df.drop(['region_x', 'region_y', 'filename', 'file_no'], axis=1)\n",
    "results3_df = results3_df.drop(['missing', 'start_date', 'end_date'], axis=1)\n",
    "print(results3_df.columns)\n",
    "#Check location data is now clean\n",
    "results3_df[results3_df['date'] < datetime.strptime(\"2025-1-1\", \"%Y-%m-%d\")].isna().sum()\n",
    "\n",
    "# Show GHI column has no NaN values before 2021\n",
    "results3_df[results3_df['date'] < datetime.strptime(\"2021-1-1\", \"%Y-%m-%d\")].isna().sum()\n",
    "# Replace NaN values with numeric 999999.9 values in GHI for 1-1-2021 until the end of 2024 to bin data to monthly later \n",
    "results3_df['GHI'] = results3_df['GHI'].replace({np.nan: 999999.9})\n",
    "\n",
    "#Confirm NaN values have been removed\n",
    "results3_df[results3_df['date'] < datetime.strptime(\"2025-1-1\", \"%Y-%m-%d\")].isna().sum()\n",
    "\n",
    "#Get WDC-SILSO data into a dataframe\n",
    "\n",
    "path_SILSO = current_directory+\"\\\\Data\\\\WDC-SILSO\\\\\"\n",
    "filename_SILSO = path_SILSO + 'SN_d_tot_V2.0.csv'\n",
    "\n",
    "solar_activity_df = pd.read_csv(filename_SILSO, sep=';') #read solar activity data into a dataframe\n",
    "\n",
    "# First three columns are year, month, and day. Fifth column is solar activity (number of sunspots per day)\n",
    "solar_activity_df.columns = ['Year', 'Month', 'Day', 'Year_Fraction', 'solar_activity', 'error', 'Column7', 'Column8'] \n",
    "\n",
    "#Drop last three columns\n",
    "solar_activity_df = solar_activity_df.iloc[:, :-3]\n",
    "\n",
    "solar_activity_df['date'] = pd.to_datetime(solar_activity_df[['Year', 'Month', 'Day']])\n",
    "\n",
    "solar_activity_df = solar_activity_df.drop(['Year_Fraction'], axis=1)\n",
    "\n",
    "print(solar_activity_df)\n",
    "\n",
    "combined_df = pd.merge(results3_df, solar_activity_df, on='date', how='left')\n",
    "print(combined_df)\n",
    "#Confirm there are no NaN values for solar activity\n",
    "combined_df[combined_df['date'] < datetime.strptime(\"2025-1-1\", \"%Y-%m-%d\")].isna().sum()\n",
    "\n",
    "# Count negative (missing) values\n",
    "count = (combined_df['solar_activity'] < 0).sum()\n",
    "print(count)\n",
    "\n",
    "# Set the date column as the index\n",
    "combined_df.set_index('date', inplace=True)\n",
    "\n",
    "#Drop STATION (not needed)\n",
    "combined_df = combined_df.drop(['STATION'], axis=1)\n",
    "\n",
    "# Perform linear interpolation to fill NaN values\n",
    "combined_df['temperatures'] = combined_df['temperatures'].interpolate(method='linear')\n",
    "combined_df['slp'] = combined_df['slp'].interpolate(method='linear')\n",
    "combined_df['vapor_pressure'] = combined_df['vapor_pressure'].interpolate(method='linear')\n",
    "combined_df['wet_bulb_temperature'] = combined_df['wet_bulb_temperature'].interpolate(method='linear')\n",
    "combined_df['specific_humidity'] = combined_df['specific_humidity'].interpolate(method='linear')\n",
    "combined_df['WDSP'] = combined_df['WDSP'].interpolate(method='linear')\n",
    "combined_df['PRCP'] = combined_df['PRCP'].interpolate(method='linear')\n",
    "combined_df['SNDP'] = combined_df['SNDP'].interpolate(method='linear')\n",
    "\n",
    "# Reset the index to get back the date column\n",
    "combined_df.reset_index(inplace=True)\n",
    "\n",
    "#Confirm there are no NaN values left and data is clean\n",
    "combined_df[combined_df['date'] < datetime.strptime(\"2025-1-1\", \"%Y-%m-%d\")].isna().sum()\n",
    "#Write full daily binned dataset to CSV file\n",
    "combined_df.to_csv('Final_Daily_Dataset.csv')\n",
    "\n",
    "# Set 'date' as the index\n",
    "combined_df.set_index('date', inplace=True)\n",
    "\n",
    "# Drop DATE and time columns, which can cause problems with binning\n",
    "combined_df = combined_df.drop(['DATE', 'time'], axis=1)\n",
    "\n",
    "#Bin the data to monthly\n",
    "monthly_data = combined_df.groupby('file_id').apply(resample, include_groups=False)\n",
    "\n",
    "# Reset the index to get back the date column\n",
    "monthly_data.reset_index(inplace=True)\n",
    "\n",
    "print(monthly_data)\n",
    "\n",
    "# Replace back the numeric 999999.9 values with NaN values in GHI \n",
    "monthly_data['GHI'] = monthly_data['GHI'].replace({999999.9: np.nan})\n",
    "\n",
    "print(monthly_data)\n",
    "\n",
    "# Define the directory containing the files \n",
    "path_ONI = current_directory+\"\\\\Data\\\\ONI\\\\\"\n",
    "print(path_ONI)\n",
    "\n",
    "filename_ONI = path_ONI + 'ONIData.txt'\n",
    "\n",
    "# Read ONI data into a dataframe\n",
    "ONI_df = pd.read_csv(filename_ONI, delimiter=r'\\s+')\n",
    "\n",
    "# Read ONI data into a dataframe\n",
    "ONI_df = pd.read_csv(filename_ONI, delimiter=r'\\s+')\n",
    "print(ONI_df)\n",
    "\n",
    "#create new date column to fill\n",
    "ONI_df['date'] = '2020-1-1' \n",
    "\n",
    "# Set date column to be last day of the month\n",
    "for i in range(ONI_df.shape[0]):\n",
    "    ONI_df.loc[i, 'date'] = get_end_of_month(ONI_df['YR'].values[i], ONI_df['MON'].values[i])\n",
    "\n",
    "#Drop all columns not needed for our final dataset\n",
    "ONI_df = ONI_df.iloc[:, -2:]\n",
    "\n",
    "ONI_df.columns = ['ONI', 'date']\n",
    "\n",
    "print(ONI_df)\n",
    "\n",
    "monthly_data.reset_index(inplace=True)\n",
    "ONI_df.reset_index(inplace=True)\n",
    "\n",
    "ONI_df['date'] = pd.to_datetime(ONI_df['date'])\n",
    "\n",
    "monthly_data.set_index('date', inplace=True)\n",
    "ONI_df.set_index('date', inplace=True)\n",
    "\n",
    "final_monthly_dataset_df = pd.merge(monthly_data, ONI_df, on='date', how='left')\n",
    "\n",
    "final_monthly_dataset_df.reset_index(inplace=True)\n",
    "\n",
    "final_monthly_dataset_df = final_monthly_dataset_df.drop(['index_x', 'index_y', 'level_0'], axis=1)\n",
    "\n",
    "print(final_monthly_dataset_df)\n",
    "\n",
    "#Confirm there are no NaN values left (except GHI) and data is clean\n",
    "final_monthly_dataset_df[final_monthly_dataset_df['date'] < datetime.strptime(\"2025-1-1\", \"%Y-%m-%d\")].isna().sum()\n",
    "\n",
    "# Count ONI values below -3\n",
    "count = (final_monthly_dataset_df['ONI'] < -3).sum()\n",
    "print(count)\n",
    "# Count ONI values above 3\n",
    "count = (final_monthly_dataset_df['ONI'] > 3).sum()\n",
    "print(count)\n",
    "# Count ONI values less than 3\n",
    "count = (final_monthly_dataset_df['ONI'] < 3).sum()\n",
    "print(count)\n",
    "\n",
    "#Write full monthly binned dataset to CSV file\n",
    "final_monthly_dataset_df.to_csv('Final_Monthly_Dataset.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
