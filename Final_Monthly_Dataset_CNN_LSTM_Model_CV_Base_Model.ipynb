{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baac998-16fc-4132-83cf-437ab3206a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Activation, Dropout\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from numpy import array\n",
    "import keras.backend as K\n",
    "import itertools\n",
    "#!pip install pydot\n",
    "    \n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "    \n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    \n",
    "def mape (y_true, y_pred):\n",
    "    return 100*K.mean(K.sqrt(K.square(y_true - y_pred))/y_true)\n",
    "    \n",
    "def pearson (y_true, y_pred):\n",
    "    return (K.square(K.mean((y_true - K.mean(y_true))*(y_pred - K.mean(y_pred)))))/(K.mean(K.square(y_true - K.mean(y_true)))*K.mean(K.square(y_pred - K.mean(y_pred))))\n",
    "    \n",
    "# convert time series into a supervised learning problem\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "# convert time series into a supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    cols, names = list(), list()\n",
    "    df = DataFrame(data)\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "    \tcols.append(df.shift(i))\n",
    "    \tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df[0].shift(-i)) # df[0] for temperature\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (1))] # % (1) for temperature\n",
    "        else:            \n",
    "            names += [('var%d(t+%d)' % (1, i))] # % (1) for temperature\n",
    "    \n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "\n",
    "    if dropnan:\n",
    "        #Drop rows containing NaN\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    #print(\"Agg:\")\n",
    "    agg.columns = names\n",
    "    #print(type(agg))\n",
    "    #print(agg)\n",
    "\n",
    "    #print(\"Test columns:\")\n",
    "    #print(agg.iloc[:, -36]) # Column containing response actual values (temperature) at time t\n",
    "\n",
    "    return agg\n",
    "     \n",
    "# create a differenced series\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    "     \n",
    "# transform series into training sets for supervised learning\n",
    "def prepare_training_data(data, n_lag, n_seq, n_time_steps):\n",
    "    \n",
    "    #Prepare data for time series forecasting.\n",
    "        \n",
    "    #Parameters:\n",
    "    #x (array-like): Input features.\n",
    "    #y (array-like): Target values.\n",
    "    #n_test (int): Number of test samples (rows).\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations.\n",
    "    #n_train (int): Number of training samples (rows).\n",
    "        \n",
    "    #Returns:\n",
    "    #tuple: Training and test datasets.\n",
    "    \n",
    "    n_vars = len(data[0][0])\n",
    "\n",
    "    print(n_vars)\n",
    "    print(len(data))\n",
    "    print(type(data))\n",
    "\n",
    "    # Each weather station has 227 time steps (the first 180 have no nan values)\n",
    "    # Loop through data, grabbing one weather station (ws) at a time, \n",
    "    # differencing on each ws and separating by training (first 226-n_lag-n_seq-n_test time steps) \n",
    "    # and testing (n_test time steps) to scale data on training only.\n",
    "    # We then recombine the training and testing datasets to change each ws to a supervised learning problem by taking all the first 180 time steps for all 12 predictors\n",
    "    # and changing these to (t-n_lag) to (t-1) since we lose one row through differencing. We then shift forward only one dependent variable (temperature or specific humidity)\n",
    "    # for time steps t to (t+n_seq)\n",
    "\n",
    "\n",
    "    diff_values = []\n",
    "    \n",
    "    for ws in range(84):\n",
    "        \n",
    "        # transform data to be stationary\n",
    "        diff_series = difference(data[ws], 1)\n",
    "        for i in range(len(diff_series)):\n",
    "            diff_values_row = []\n",
    "            for j in range(len(diff_series[0])):\n",
    "                diff_values_row.append(diff_series[i][j])\n",
    "            diff_values.append(diff_values_row)\n",
    "\n",
    "    #print(\"Diff values:\")\n",
    "    #print(len(diff_values))\n",
    "    #print(len(diff_values[0]))\n",
    "    #print(len(diff_values_for_training))\n",
    "    #print(len(diff_values_for_training[0]))\n",
    "    \n",
    "    # rescale values to 0, 1\n",
    "    scaler_all_features =  MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler =  MinMaxScaler(feature_range=(0, 1))\n",
    "    #diff_values_training = np.array(diff_values_for_training)\n",
    "    #diff_values_training = diff_values_training.reshape(1, -1) \n",
    "    train_scaled_values = scaler_all_features.fit_transform(diff_values)\n",
    "    response_train_values = []\n",
    "    for i in range(len(diff_values)):\n",
    "        response_train_values.append(diff_values[i][0]) # Uses first column (temperatures) as response variable\n",
    "    response_train_values = np.array(response_train_values)\n",
    "    response_train_values = response_train_values.reshape(len(response_train_values), 1)\n",
    "\n",
    "    # Fit the scaler for just the response variable for use later when forecasting\n",
    "    response_scaled_values = scaler.fit_transform(response_train_values) \n",
    "    scaled_values = scaler_all_features.transform(diff_values)\n",
    "\n",
    "    #print(\"Scaled values rows:\")\n",
    "    #print(len(scaled_values))\n",
    "\n",
    "    train = []\n",
    "\n",
    "    # Transform each weather station as a separate \"batch\"\n",
    "    for ws in range(84):\n",
    "        # transform into supervised learning problem X, y\n",
    "        first = (n_time_steps-1)*ws\n",
    "        last = (n_time_steps-1)*ws+(n_time_steps-2)\n",
    "        #print(\"Batch \"+str(ws+1)+\":\")\n",
    "        #print(\"Range: \"+str(first)+\"-\"+str(last))\n",
    "        scaled_values_batch = scaled_values[first:last]\n",
    "        supervised = series_to_supervised(scaled_values_batch, n_lag, n_seq)\n",
    "        supervised_values = supervised.values\n",
    "        train.append([supervised_values])\n",
    "        #print(\"Supervised count:\")\n",
    "        #print(len(supervised_values))\n",
    "        #print(len(supervised_values[0]))\n",
    "    \n",
    "    return scaler, scaler_all_features, train\n",
    "\n",
    "# transform series into testing and validation sets for supervised learning\n",
    "def prepare_testing_and_validation_data(data, n_lag, n_seq, n_time_steps, scaler_all_features):\n",
    "    \n",
    "    #Prepare data for time series forecasting.\n",
    "        \n",
    "    #Parameters:\n",
    "    #x (array-like): Input features.\n",
    "    #y (array-like): Target values.\n",
    "    #n_test (int): Number of test samples (rows).\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations.\n",
    "    #n_train (int): Number of training samples (rows).\n",
    "        \n",
    "    #Returns:\n",
    "    #tuple: Training and test datasets.\n",
    "    \n",
    "    n_vars = len(data[0][0])\n",
    "\n",
    "    print(n_vars)\n",
    "    print(len(data))\n",
    "    print(type(data))\n",
    "\n",
    "    # Each weather station has 227 time steps (the first 180 have no nan values)\n",
    "    # Loop through data, grabbing one weather station (ws) at a time, \n",
    "    # differencing on each ws and separating by training (first 226-n_lag-n_seq-n_test time steps) \n",
    "    # and testing (n_test time steps) to scale data on training only.\n",
    "    # We then recombine the training and testing datasets to change each ws to a supervised learning problem by taking all the first 180 time steps for all 12 predictors\n",
    "    # and changing these to (t-n_lag) to (t-1) since we lose one row through differencing. We then shift forward only one dependent variable (temperature or specific humidity)\n",
    "    # for time steps t to (t+n_seq)\n",
    "\n",
    "\n",
    "    diff_values = []\n",
    "    \n",
    "    for ws in range(21):\n",
    "        \n",
    "        # transform data to be stationary\n",
    "        diff_series = difference(data[ws], 1)\n",
    "        for i in range(len(diff_series)):\n",
    "            diff_values_row = []\n",
    "            for j in range(len(diff_series[0])):\n",
    "                diff_values_row.append(diff_series[i][j])\n",
    "            diff_values.append(diff_values_row)\n",
    "\n",
    "    #print(\"Diff values:\")\n",
    "    #print(len(diff_values))\n",
    "    #print(len(diff_values[0]))\n",
    "    #print(len(diff_values_for_training))\n",
    "    #print(len(diff_values_for_training[0]))\n",
    "    \n",
    "    # rescale values to 0, 1\n",
    "    scaled_values = scaler_all_features.transform(diff_values)\n",
    "\n",
    "    validation = []\n",
    "    test = []\n",
    "\n",
    "    # Transform each weather station as a separate \"batch\"\n",
    "    for ws in range(21):\n",
    "        # transform into supervised learning problem X, y\n",
    "        first = (n_time_steps-1)*ws\n",
    "        last = (n_time_steps-1)*ws+(n_time_steps-2)\n",
    "        #print(\"Batch \"+str(ws+1)+\":\")\n",
    "        #print(\"Range: \"+str(first)+\"-\"+str(last))\n",
    "        scaled_values_batch = scaled_values[first:last]\n",
    "        supervised = series_to_supervised(scaled_values_batch, n_lag, n_seq)\n",
    "        supervised_values = supervised.values\n",
    "        # training/test/validation split is 80%/10%/10%\n",
    "        if ws < 11:\n",
    "            test.append([supervised_values])\n",
    "        else:\n",
    "            validation.append([supervised_values])\n",
    "        #print(\"Supervised count:\")\n",
    "        #print(len(supervised_values))\n",
    "        #print(len(supervised_values[0]))\n",
    "    \n",
    "    return validation, test\n",
    "    \n",
    "def plot_kfold(cv, X, y, ax, n_splits, xlim_max=105):\n",
    "    \n",
    "    #Plots the indices for a cross-validation object.\n",
    "    #Taken from https://www.geeksforgeeks.org/cross-validation-using-k-fold-with-scikit-learn/\n",
    "    \n",
    "    #Parameters:\n",
    "    #cv: Cross-validation object\n",
    "    #X: Feature set\n",
    "    #y: Target variable\n",
    "    #ax: Matplotlib axis object\n",
    "    #n_splits: Number of folds in the cross-validation\n",
    "    #xlim_max: Maximum limit for the x-axis\n",
    "        \n",
    "    # Set color map for the plot\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "    cv_split = cv.split(X=X, y=y)\n",
    "        \n",
    "    for i_split, (train_idx, test_idx) in enumerate(cv_split):\n",
    "        # Create an array of NaNs and fill in training/testing indices\n",
    "        indices = np.full(len(X), np.nan)\n",
    "        indices[test_idx], indices[train_idx] = 1, 0\n",
    "            \n",
    "        # Plot the training and testing indices\n",
    "        ax_x = range(len(indices))\n",
    "        ax_y = [i_split + 0.5] * len(indices)\n",
    "        ax.scatter(ax_x, ax_y, c=indices, marker=\"_\", \n",
    "                   lw=10, cmap=cmap_cv, vmin=-0.2, vmax=1.2)\n",
    "    \n",
    "        # Set y-ticks and labels\n",
    "        y_ticks = np.arange(n_splits) + 0.5\n",
    "        ax.set(yticks=y_ticks, yticklabels=range(n_splits),\n",
    "               xlabel=\"Weather Station index (file_id)\", ylabel=\"Fold\",\n",
    "               ylim=[n_splits, -0.2], xlim=[0, xlim_max])\n",
    "    \n",
    "        # Set plot title and create legend\n",
    "        ax.set_title(\"KFold\", fontsize=14)\n",
    "        legend_patches = [Patch(color=cmap_cv(0.8), label=\"Testing set\"), \n",
    "                          Patch(color=cmap_cv(0.02), label=\"Training set\")]\n",
    "        ax.legend(handles=legend_patches, loc=(1.03, 0.8))\n",
    "    \n",
    "#Main\n",
    "\n",
    "#Configure\n",
    "n_seq = 60\n",
    "if n_seq > 46:\n",
    "    n_lag = 179 - n_seq + 46\n",
    "else:\n",
    "    n_lag = 179\n",
    "n_time_steps = 227\n",
    "n_test = 1\n",
    "\n",
    "print(\"Model Parameters:\")\n",
    "print(\"n_lag (number of input time steps): \"+str(n_lag))\n",
    "print(\"n_seq (number of output/future prediction time steps): \"+str(n_seq))\n",
    "\n",
    "# Create 2D array with file_ids to use for sample creation\n",
    "array = np.array([\n",
    "    6501, 6541, 6640, 6668, 6678, \n",
    "    6687, 6697, 6714, 6744, 6772, \n",
    "    6783, 6840, 6844, 6854, 6870, \n",
    "    6891, 6895, 6899, 6901, 6909, \n",
    "    6929, 6950, 6963, 6969, 6994, \n",
    "    7032, 7057, 7094, 7095, 7100, \n",
    "    7108, 7116, 7119, 7131, 7139, \n",
    "    7152, 7155, 7156, 7182, 7193, \n",
    "    7202, 7239, 7280, 7286, 7287, \n",
    "    7311, 7321, 7329, 7347, 7350, \n",
    "    7354, 7357, 7361, 7414, 7423, \n",
    "    7424, 7432, 7463, 7482, 7489, \n",
    "    7528, 7531, 7534, 7538, 7549, \n",
    "    7553, 7555, 7562, 7571, 7573, \n",
    "    7574, 7575, 7585, 7599, 7603, \n",
    "    7606, 7622, 7652, 7671, 7704, \n",
    "    7786, 7805, 7816, 7838, 7861, \n",
    "    7862, 7863, 7870, 7892, 7907, \n",
    "    7938, 7962, 7979, 7987, 7999, \n",
    "    8000, 8034, 8083, 8120, 8133, \n",
    "    8184, 8186, 8247, 8248, 9858])\n",
    "    \n",
    "#Create arrays holding the 5-fold cross-validation indices gathered for consistency across models\n",
    "train_array = []\n",
    "test_array = []\n",
    "    \n",
    "train_array.append([1, 2, 3, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, \n",
    "                        23, 24, 25, 27, 28, 29, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, \n",
    "                        43, 44, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, \n",
    "                        62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 81, \n",
    "                        82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 95, 97, 98, 100, 101, 102, 103])\n",
    "test_array.append([0, 4, 10, 12, 18, 26, 30, 31, 33, 45, 47, 53, 64, 65, 77, 80, 89, 94, 96, 99, 104])\n",
    "    \n",
    "train_array.append([0, 1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 17, 18, 19, 20, 21, 23, \n",
    "                        24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 36, 37, 38, 41, 43, 45, \n",
    "                        46, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 59, 60, 61, 63, 64, \n",
    "                        65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77, 80, 81, 82, 83, 84, \n",
    "                        86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104])\n",
    "test_array.append([5, 9, 11, 15, 16, 22, 28, 35, 39, 40, 42, 44, 55, 56, 62, 72, 76, 78, 79, 85, 103])\n",
    "    \n",
    "train_array.append([0, 1, 2, 4, 5, 9, 10, 11, 12, 14, 15, 16, 18, 20, 21, 22, 23, 26, \n",
    "                    28, 29, 30, 31, 32, 33, 35, 36, 37, 39, 40, 41, 42, 44, 45, 46, 47, \n",
    "                    48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, \n",
    "                    70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, \n",
    "                    90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104])\n",
    "test_array.append([3, 6, 7, 8, 13, 17, 19, 24, 25, 27, 34, 38, 43, 49, 66, 67, 68, 69, 73, 81, 84])\n",
    "    \n",
    "train_array.append([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n",
    "                        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, \n",
    "                        35, 37, 38, 39, 40, 42, 43, 44, 45, 47, 49, 51, 52, 53, 55, 56, \n",
    "                        60, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, \n",
    "                        80, 81, 82, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 99, 102, 103, 104])\n",
    "test_array.append([32, 36, 41, 46, 48, 50, 54, 57, 58, 59, 61, 63, 70, 75, 83, 90, 91, 97, 98, 100, 101])\n",
    "    \n",
    "train_array.append([0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 22,\n",
    "                        24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, \n",
    "                        42, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55, 56, 57, 58, 59, \n",
    "                        61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, \n",
    "                        79, 80, 81, 83, 84, 85, 89, 90, 91, 94, 96, 97, 98, 99, 100, 101, 103, 104])\n",
    "test_array.append([1, 2, 14, 20, 21, 23, 29, 37, 51, 52, 60, 71, 74, 82, 86, 87, 88, 92, 93, 95, 102])\n",
    "    \n",
    "# Equations for three Principal Components from PCA using response variables combined with other predictors\n",
    "#PC1=-0.0002714X1+0.02612X2+0.03858X3-0.007658X4+0.001592X5-0.02087X6+0.8564X7-0.1468X8+0.01192X9-0.0001049X10+0.01913X11+0.02076X12\n",
    "#PC2=0.0003944X1+0.002204X2+0.01052X3+0.3248X4-0.0009976X5-0.04421X6+2.3406X7+0.06103X8+0.08841X9+0.00009018X10+0.05678X11-0.002022X12\n",
    "#PC3=-0.00007998X1-0.0006124X2-0.001063X3-0.01855X4+0.00001956X5+0.01170X6+0.6076X7+0.4664X8-0.002995X9+0.008185X10+0.8815X11-0.0004730X12\n",
    "    \n",
    "# Equations for three Principal Components from PCA omitting both response variables,\n",
    "#PC-1=-0.0004514X1+0.03194X2-0.04343X3+0.002243X4-0.02252X5+0.9877X6-0.2265X7+0.006144X8-0.0001488X9+0.02943X10\n",
    "#PC-2=0.0001702X1+0.005484X2+0.2057X3-0.0003188X4-0.02584X5+1.6963X6-0.05890X7+0.05809X8+1.9748X9+0.03686X10\n",
    "#PC-3=-0.00006323X1-0.001180X2-0.02384X3-0.00002833X4+0.01170X5+0.5204X6+0.4791X7-0.004318X8+0.008271X9+0.8765X10\n",
    "    \n",
    "# Get the current working directory \n",
    "current_directory = os.getcwd() \n",
    "    \n",
    "# Print the current working directory \n",
    "print(current_directory)\n",
    "    \n",
    "# Define the directory containing the files \n",
    "path = current_directory+\"\\\\Modeling\\\\\"\n",
    "print(path)\n",
    "    \n",
    "filename = path + 'Final_Monthly_Dataset.csv'\n",
    "    \n",
    "# load dataset\n",
    "df = read_csv(filename, header=0, parse_dates=[0], index_col=0, date_format='%Y-%m')\n",
    "    \n",
    "df = df.rename(columns={'Unnamed: 0' : 'indices'})\n",
    "    \n",
    "#Remove unused columns\n",
    "df = df.drop(['Day', 'vapor_pressure'], axis=1)\n",
    "    \n",
    "# Round numbers in columns to reasonable precision,\n",
    "df['temperatures'] = np.round(df['temperatures'], 2)\n",
    "df['slp'] = np.round(df['slp'], 2)\n",
    "df['wet_bulb_temperature'] = np.round(df['wet_bulb_temperature'], 2)\n",
    "df['specific_humidity'] = np.round(df['specific_humidity'], 2)\n",
    "df['GHI'] = np.round(df['GHI'], 2)\n",
    "df['PRCP'] = np.round(df['PRCP'], 2)\n",
    "df['SNDP'] = np.round(df['SNDP'], 2)\n",
    "df['solar_activity'] = np.round(df['solar_activity'], 2)\n",
    "df['ONI'] = np.round(df['ONI'], 2)\n",
    "df['water'] = np.round(df['water'], 0)\n",
    "df['region'] = np.round(df['region'], 0)\n",
    "    \n",
    "df_trimmed = df[df['file_id'] != 7533] # Remove file_id 7533 so there are 105 weather stations for 5-fold CV\n",
    "df_trimmed = df_trimmed.drop(['Year', 'Month', 'date', 'latitude', 'longitude', 'elevation'], axis=1)\n",
    "    \n",
    "X = []\n",
    "y = []\n",
    "    \n",
    "for i in array:\n",
    "    add_to_X = [] # create list to store each column to add to X\n",
    "    new_df = df_trimmed[df_trimmed['file_id'] == i].drop(['file_id'], axis=1)\n",
    "    #new_df = new_df.iloc[:180, :]\n",
    "    add_to_y = []\n",
    "    for j in range(new_df.shape[0]):\n",
    "        add_to_y.append(new_df['temperatures'].iloc[j])\n",
    "    y.append(add_to_y)\n",
    "    #new_df = new_df.drop(['temperatures'], axis=1)\n",
    "    columns_list = new_df.columns.tolist()\n",
    "    for j in range(new_df.shape[0]):\n",
    "        l=0\n",
    "        new_row = []\n",
    "        for m in columns_list:\n",
    "            new_row.append(new_df.iloc[j, l])\n",
    "            l += 1\n",
    "        add_to_X.append(new_row)\n",
    "    X.append(add_to_X)\n",
    "    \n",
    "#Perform k-fold cross-validation\n",
    "#Taken from: https://www.geeksforgeeks.org/cross-validation-using-k-fold-with-scikit-learn/\n",
    "    \n",
    "#k = 5  # Number of folds\n",
    "#kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "#for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "#    print(f\"Fold {i}:\")\n",
    "#    print(f\"  Training dataset index: {train_index}\")\n",
    "#    print(f\"  Test dataset index: {test_index}\")\n",
    "    \n",
    "#for train_indices, test_indices in kf.split(X):\n",
    "#    print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    \n",
    "# Create figure and axis\n",
    "#fig, ax = plt.subplots(figsize=(6, 3))\n",
    "#plot_kfold(kf, X, y, ax, k)\n",
    "#plt.tight_layout()\n",
    "#fig.subplots_adjust(right=0.6)\n",
    "    \n",
    "#Create train and test sets for each cross-validation split\n",
    "train_X = []\n",
    "train_y = []\n",
    "val_X = []\n",
    "val_y = []\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}\")\n",
    "    #Add each corresponding sample for each entry of train index \n",
    "    train_X_rows = [] # Stores all the samples for one fold of train_X\n",
    "    train_y_rows = [] # Stores all the samples for one fold of train_y\n",
    "    for j in train_array[i]:\n",
    "        train_X_rows.append(X[j])\n",
    "        train_y_rows.append(y[j])\n",
    "    # Stores one fold of train dataset\n",
    "    train_X.append(train_X_rows)\n",
    "    train_y.append(train_y_rows)\n",
    "    #Add each corresponding sample for each entry of the validation index \n",
    "    val_X_rows = [] # Stores all the samples for one fold of val_X\n",
    "    val_y_rows = [] # Stores all the samples for one fold of val_y\n",
    "    for j in test_array[i]: \n",
    "            val_X_rows.append(X[j])\n",
    "            val_y_rows.append(y[j])\n",
    "    # Stores one fold of validation dataset\n",
    "    val_X.append(val_X_rows)\n",
    "    val_y.append(val_y_rows) \n",
    "    \n",
    "    #print(\"Train_X Fold \"+str(i)+\":\")\n",
    "    #print(len(train_X[i]))\n",
    "    #print(len(train_X[i][0]))\n",
    "    #print(len(train_X[i][0][0])) \n",
    "    #print(\"Train_y Fold \"+str(i)+\":\")\n",
    "    #print(len(train_y[i]))\n",
    "    #print(len(train_y[i][0]))\n",
    "    #print(train_y[i][0][0])\n",
    "    #print(\"Validation_X Fold \"+str(i)+\":\")\n",
    "    #print(len(val_X[i]))\n",
    "    #print(len(val_X[i][0]))\n",
    "    #print(len(val_X[i][0][0]))\n",
    "    #print(\"Validation_y Fold \"+str(i)+\":\")\n",
    "    #print(len(val_y[i]))\n",
    "    #print(len(val_y[i][0]))\n",
    "    #print(val_y[i][0][0])\n",
    "    \n",
    "#Convert 3D arrays to DataFrames\n",
    "df_X = []\n",
    "df_y = []\n",
    "val_df_X = []\n",
    "val_df_y = []\n",
    "dataset = []\n",
    "dataset_scaling = []\n",
    "dataset_test = []\n",
    "scaler = []\n",
    "scaler_all_features = []\n",
    "train = []\n",
    "test = []\n",
    "validation = []\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Fold \"+str(i+1)+\":\")\n",
    "    #Transform train_X to the correct format\n",
    "    df1 = []\n",
    "    dataset_df = [] # captures each weather station's dataset as values for training scaler mapping\n",
    "    df_X.append(pd.DataFrame(train_X[i]))\n",
    "    X_t = df_X[i].transpose()\n",
    "    for k in range(84):\n",
    "        X = np.array(X_t.iloc[:, k])\n",
    "        df = pd.DataFrame()\n",
    "        for j in range(n_time_steps):\n",
    "            new_row = pd.DataFrame(X[j]).transpose()\n",
    "            new_row.columns = new_df.columns\n",
    "            # Add the new row\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.columns = new_df.columns\n",
    "        df1.append(df)\n",
    "        dataset_df.append(df.values)\n",
    "    df_X[i] = df1\n",
    "    dataset.append(dataset_df)\n",
    "    dataset_scaling = list(itertools.chain(*dataset)) # Holds all 84 weather station rows to train the scaling function\n",
    "\n",
    "    #print(len(dataset))\n",
    "    #print(len(dataset[0]))\n",
    "    #print(len(dataset_scaling))\n",
    "    #print(len(dataset_scaling[:][0]))\n",
    "    #print(\"Stop\")\n",
    "    \n",
    "    #Transform train_y to the correct format\n",
    "    df2 = []\n",
    "    df_y.append(pd.DataFrame(train_y[i]))\n",
    "    y_t = df_y[i].transpose()\n",
    "    \n",
    "    for j in range(84):\n",
    "        y = np.array(y_t.iloc[:, j])\n",
    "        y = pd.DataFrame(y)\n",
    "        y.columns = ['temperatures']\n",
    "        df2.append(y)\n",
    "    df_y[i] = df2\n",
    "\n",
    "    #Transform val_X to the correct format\n",
    "    df3 = []\n",
    "    dataset_vl_df = [] # captures each weather station's dataset as values for training scaler mapping\n",
    "    val_df_X.append(pd.DataFrame(val_X[i]))\n",
    "    val_X_t = val_df_X[i].transpose()\n",
    "    for k in range(21):\n",
    "        vl_X = np.array(val_X_t.iloc[:, k])\n",
    "        vl_df = pd.DataFrame()\n",
    "        for j in range(n_time_steps):\n",
    "            new_row = pd.DataFrame(vl_X[j]).transpose()\n",
    "            new_row.columns = new_df.columns\n",
    "            # Add the new row\n",
    "            vl_df = pd.concat([vl_df, new_row], ignore_index=True)\n",
    "        vl_df.columns = new_df.columns\n",
    "        df3.append(vl_df)\n",
    "        dataset_vl_df.append(vl_df.values)\n",
    "    val_df_X[i] = df3\n",
    "    dataset_test.append(dataset_vl_df)    \n",
    "    dataset_testing = list(itertools.chain(*dataset_test)) # Holds remaining 21 weather station rows for testing and validation\n",
    "\n",
    "    #Transform val_y to the correct format\n",
    "    df4 = []\n",
    "    val_df_y.append(pd.DataFrame(train_y[i]))\n",
    "    val_y_t = val_df_y[i].transpose()\n",
    "    \n",
    "    for j in range(21):\n",
    "        v_y = np.array(val_y_t.iloc[:, j])\n",
    "        v_y = pd.DataFrame(v_y)\n",
    "        v_y.columns = ['temperatures']\n",
    "        df4.append(v_y)\n",
    "    val_df_y[i] = df4\n",
    "    \n",
    "    scaler.append(MinMaxScaler(feature_range=(0, 1)))\n",
    "    scaler_all_features.append(MinMaxScaler(feature_range=(0, 1)))\n",
    "    train.append([1])\n",
    "    test.append([1])\n",
    "    validation.append([1])\n",
    "    \n",
    "    # prepare data\n",
    "    scaler[i], scaler_all_features[i], train[i] = prepare_training_data(dataset_scaling, n_lag, n_seq, n_time_steps)\n",
    "\n",
    "    validation[i], test[i] = prepare_testing_and_validation_data(dataset_testing, n_lag, n_seq, n_time_steps, scaler_all_features[i])\n",
    "\n",
    "    #Reshape dimensionality\n",
    "    train1 = train[i]\n",
    "    test1 = test[i]\n",
    "    validation1 = validation[i]\n",
    "    print(np.array(train1).shape)\n",
    "    print(np.array(test1).shape)\n",
    "    print(np.array(validation1).shape)\n",
    "    train2 = []\n",
    "    test2 = []\n",
    "    validation2 = []\n",
    "\n",
    "    for k in range(84):\n",
    "        train2.append(train1[k][0])\n",
    "        \n",
    "    for k in range(11):\n",
    "        test2.append(test1[k][0])\n",
    "\n",
    "    for k in range(10):\n",
    "        validation2.append(validation1[k][0])\n",
    "\n",
    "    print(np.array(train2).shape)\n",
    "    print(np.array(test2).shape)\n",
    "    print(np.array(validation2).shape)\n",
    "\n",
    "    train[i] = train2\n",
    "    test[i] = test2\n",
    "    validation[i] = validation2\n",
    "\n",
    "    #Reshape dimensionality (again)\n",
    "    dim_size = n_seq + 12*n_lag\n",
    "    train1 = np.array(train[i]).reshape(84, dim_size)\n",
    "    test1 = np.array(test[i]).reshape(11, dim_size)\n",
    "    validation1 = np.array(validation[i]).reshape(10, dim_size)\n",
    "    train2 = pd.DataFrame(train1).values\n",
    "    test2 = pd.DataFrame(test1).values\n",
    "    validation2 = pd.DataFrame(validation1).values\n",
    "    print(np.array(train2).shape)\n",
    "    print(np.array(test2).shape)\n",
    "    print(np.array(validation2).shape)\n",
    "\n",
    "    print(train2)\n",
    "\n",
    "    train[i] = train2\n",
    "    test[i] = test2\n",
    "    validation[i] = validation2\n",
    "    \n",
    "    #X_train = train1[:][:-n_seq]\n",
    "    #y_train = train1[:][-n_seq:]\n",
    "    #X_test = test1[:][:-n_seq]\n",
    "    #y_test = test1[:][-n_seq:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74bc78-33b4-4695-9874-daa61b380616",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_seq)\n",
    "print(n_lag)\n",
    "print(np.array(train[1]).shape)\n",
    "print(np.array(test[1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ad946-c0cb-407a-8a65-8b9aef8cf7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 1\n",
    "nb_epoch = 100\n",
    "\n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def mape (y_true, y_pred):\n",
    "    return 100*K.mean(K.sqrt(K.square(y_true - y_pred))/y_true)\n",
    "    \n",
    "def pearson (y_true, y_pred):\n",
    "    return (K.square(K.mean((y_true - K.mean(y_true))*(y_pred - K.mean(y_pred)))))/(K.mean(K.square(y_true - K.mean(y_true)))*K.mean(K.square(y_pred - K.mean(y_pred))))\n",
    " \n",
    "# fit an LSTM network to training data\n",
    "#Adapted from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def create_model(trial, X, y, n_lag, n_seq, n_batch, nb_epoch):\n",
    "    \n",
    "    #Parameters:\n",
    "    #trial (array-like): Optuna parameters.\n",
    "    #train (array-like): Target values.\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations\n",
    "    #nb_epoch (int): Maximum number of epochs\n",
    "    \n",
    "    # Hyperparameters to be tuned by Optuna (taken from Javier Leon's dissertation 'Fruit Prices')\n",
    "    lr = trial.suggest_float('lr', 1e-3, 1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=lr)\n",
    "    \n",
    "    #Optuna will try either Rectified Linear Unit (ReLU) = max(0, x), tanh, or sigmoid functions\n",
    "    activation_function = trial.suggest_categorical('activation_function', ['relu', 'tanh', 'sigmoid'])\n",
    "    \n",
    "    filters = trial.suggest_categorical('filters', [256, 512, 1024]) # Used for CNN-LSTM model\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [256, 512, 1024])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=1, activation='relu', input_shape=(X.shape[1], X.shape[2]))) # CNN-LSTM only\n",
    "    model.add(MaxPooling1D(pool_size=1)) # CNN-LSTM Only\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(256, input_dim=y.shape[1], activation=activation_function))\n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    model.add(Dense(n_seq))\n",
    "    model.compile(loss=rmse, optimizer=optimizer)\n",
    "        \n",
    "    return model\n",
    "    \n",
    "def objective(trial):\n",
    "    \n",
    "    cv_accuracies = []\n",
    "        \n",
    "    for i in range(5):\n",
    "        train1 = train[i]\n",
    "        test1 = test[i]\n",
    "        validation1 = validation[i]\n",
    "\n",
    "        X, y = train1[:, 0:-n_seq], train1[:, -n_seq:]\n",
    "        X_test, y_test = test1[:, 0:-n_seq], test1[:, -n_seq:]\n",
    "        X_val, y_val = validation1[:, 0:-n_seq], validation1[:, -n_seq:]\n",
    "        X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "        X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "        X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "    \n",
    "        model = create_model(trial, X, y, n_lag, n_seq, n_batch, nb_epoch)\n",
    "\n",
    "        history = model.fit(X, y, validation_data=(X_val, y_val), epochs=nb_epoch, verbose=0)\n",
    "\n",
    "        cv_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "        loss = history.history['val_loss'][-1]\n",
    "    \n",
    "        # Plotting the training and validation loss\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        title1 = \"CNN-LSTM Training and Validation Loss for Fold \" + str(i+1)\n",
    "        plt.title(title1)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Root Mean Squared Error Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "            \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "    \n",
    "    print(\"Cross Validation Accuracies:\")\n",
    "    print(cv_accuracies)\n",
    "    print(\"Mean Cross Validation Accuracy:\")\n",
    "    print(np.mean(cv_accuracies))\n",
    "    print(\"Standard Deviation of Cross Validation Accuracy:\")\n",
    "    print(np.std(cv_accuracies))\n",
    "        \n",
    "    return np.mean(cv_accuracies)\n",
    "\n",
    "n_batch = 1\n",
    "nb_epoch = 100\n",
    "\n",
    "# optimize and fit model\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "\n",
    "print('Best trial:', study.best_trial.params)\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "best_model = create_model(optuna.trial.FixedTrial(best_params), X, y, n_lag, n_seq, n_batch, nb_epoch)\n",
    "best_model.fit(X, y, epochs=nb_epoch, batch_size=n_batch, validation_data=(X_val, y_val))\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "title1 = \"CNN-LSTM Training and Validation Loss for Best Model\"\n",
    "plt.title(title1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Root Mean Squared Error Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520d361-3a51-4ea3-be93-cd5607ab8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad1f205-9605-4f1c-b5be-b54ceb49456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = create_model(optuna.trial.FixedTrial(best_params), X, y, n_lag, n_seq, n_batch, nb_epoch)\n",
    "best_model.fit(X, y, epochs=nb_epoch, batch_size=n_batch, validation_data=(X_val, y_val))\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "title1 = \"CNN-LSTM Training and Validation Loss for Best Model\"\n",
    "plt.title(title1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Root Mean Squared Error Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e0a49-8fc2-4e2f-8bf4-7bb9dde70c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e54bcc-51b3-4db4-adf7-a8041e2b237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16289125-ffc3-4177-a011-ecba68a7ffe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ec920-1d52-457b-b12f-421c8bfa1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study, params=['optimizer', 'activation_function', 'lstm_units'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe2ed1d-7fe4-445f-91bc-135f6dec0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study, params=['lr', 'dropout_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c7a5a-83c7-4dac-9e14-04bbba312ade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# fit best LSTM model using parameters found by Optuna\n",
    "def create_best_model(X, y, n_lag, n_seq, n_batch, nb_epoch):\n",
    "    \n",
    "    #Parameters:\n",
    "    #trial (array-like): Optuna parameters.\n",
    "    #train (array-like): Target values.\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations\n",
    "    #nb_epoch (int): Maximum number of epochs\n",
    "    \n",
    "    # Hyperparameters from best model\n",
    "    filters = 512\n",
    "    lr = 0.24431694526169084\n",
    "    optimizer = SGD(learning_rate = lr)\n",
    "    activation_function = 'tanh'\n",
    "    lstm_units = 1024\n",
    "    dropout_rate = 0.266380836203013\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=1, activation='relu', input_shape=(X.shape[1], X.shape[2]))) # CNN-LSTM only\n",
    "    model.add(MaxPooling1D(pool_size=1)) # CNN-LSTM only\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(256, input_dim=y.shape[1], activation=activation_function))\n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    model.add(Dense(n_seq))\n",
    "    model.compile(loss=rmse, optimizer=optimizer, metrics=['accuracy', 'mae', rmse, mape, pearson])\n",
    "        \n",
    "    return model\n",
    "\n",
    "# make forecast with CNN-LSTM best model\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def forecast_lstm(model, X, n_seq, n_test):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, X.shape[1])\n",
    "    # make forecast\n",
    "    forecast = model.predict(X)\n",
    "    # convert to array\n",
    "    return forecast\n",
    "     \n",
    "# evaluate the persistence model\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def make_forecasts(model, X_test, y_test, n_lag, n_seq, n_test):\n",
    "    forecasts = list()\n",
    "    for i in range(len(X_test)):\n",
    "        X, y = X_test[i, :], y_test[i, :]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_seq, n_test)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    "     \n",
    "# invert differenced forecast\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    # invert first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob)\n",
    "    # propagate difference forecast using inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted\n",
    "     \n",
    "# inverse data transform on forecasts\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def inverse_transform(series, forecasts, scaler, n_test, n_seq):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = forecasts[i]\n",
    "        forecast = forecast.reshape(1, n_seq)\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i - 1\n",
    "        last_ob = series.values[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted\n",
    "     \n",
    "# evaluate the RMSE for each forecast time step\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    rmse_list = []\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    r2_list = []\n",
    "    for i in range(n_seq):\n",
    "        actual = [row[i] for row in test]\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "        mae = mean_absolute_error(actual, predicted)\n",
    "        mse = mean_squared_error(actual, predicted)\n",
    "        mape = mean_absolute_percentage_error(actual, predicted)\n",
    "        r2 = r2_score(actual, predicted) \n",
    "        rmse_list.append(rmse)\n",
    "        mae_list.append(mae)\n",
    "        mape_list.append(mape)\n",
    "        r2_list.append(r2)\n",
    "        print(\"Month at t+\"+str(i+1)+\":\")\n",
    "        print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "        print('t+%d MAE: %f' % ((i+1), mae))\n",
    "        print('t+%d MAPE: %f' % ((i+1), mape))\n",
    "        print('t+%d R2_SCORE: %f' % ((i+1), r2))\n",
    "\n",
    "    return rmse_list, mae_list, mape_list, r2_list\n",
    "     \n",
    "# plot the forecasts in the context of the original dataset\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "    # plot the entire dataset in blue\n",
    "    plt.plot(series.values)\n",
    "    # plot the forecasts in red\n",
    "    for i in range(len(forecasts)):\n",
    "        off_s = len(series) - n_test + i - 1\n",
    "        off_e = off_s + len(forecasts[i]) + 1\n",
    "        xaxis = [x for x in range(off_s, off_e)]\n",
    "        yaxis = [series.values[off_s]] + forecasts[i]\n",
    "        plt.plot(xaxis, yaxis, color='red')\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    \n",
    "n_batch = 1\n",
    "n_test = 11\n",
    "nb_epoch = 100\n",
    "    \n",
    "for i in range(5):\n",
    "    train1 = train[i]\n",
    "    test1 = test[i]\n",
    "    validation1 = validation[i]\n",
    "\n",
    "    X = train1[:, 0:-n_seq]\n",
    "    y = train1[:, -n_seq:]\n",
    "    X_test = test1[:, 0:-n_seq]\n",
    "    y_test = test1[:, -n_seq:]\n",
    "    X_val = validation1[:, 0:-n_seq]\n",
    "    y_val = validation1[:, -n_seq:]\n",
    "\n",
    "    dataset_df = pd.DataFrame(val_y[i])\n",
    "    dataset_df = dataset_df.iloc[0:10, :]\n",
    "    dataset = dataset_df.values\n",
    "\n",
    "    series = pd.Series(dataset[:, 0]) # Using first column (temperatures)\n",
    "\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    X1 = X_test\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "    X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "\n",
    "    best_model = create_best_model(X, y, n_lag, n_seq, n_batch, nb_epoch)\n",
    "    history = best_model.fit(X, y, epochs=100, batch_size=n_batch, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f'Test Loss: {loss}')\n",
    "\n",
    "    # make forecasts\n",
    "    forecasts = make_forecasts(best_model, X_test, y_test, n_lag, n_seq, n_test)\n",
    "    \n",
    "    # inverse transform forecasts and test\n",
    "    forecasts = inverse_transform(series, forecasts, scaler[i], n_test+2, n_seq)\n",
    "    actual = [row[-n_seq:] for row in test1]\n",
    "    actual = inverse_transform(series, actual, scaler[i], n_test+2, n_seq)\n",
    "\n",
    "    #print(\"Forecasts:\")\n",
    "    #print(forecasts)\n",
    "    #print(\"Actual:\")\n",
    "    #print(actual)\n",
    "\n",
    "    rmse_list = [] # list stores root mean squared errors for each future time prediction\n",
    "    mae_list = [] # list stores root mean squared errors for each future time prediction\n",
    "    mape_list = [] # list stores root mean squared errors for each future time prediction\n",
    "    r2_list = [] # list stores root mean squared errors for each future time prediction\n",
    "    \n",
    "    # evaluate forecasts\n",
    "    rmse_list, mae_list, mape_list, r2_list = evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "\n",
    "    for j in range(len(X_test)):\n",
    "        print(\"Weather Station \"+str(j+1)+\":\")\n",
    "        dataset_df = pd.DataFrame(y_test[j].flatten())\n",
    "        dataset = dataset_df.values\n",
    "        dataset = dataset[:, 0]\n",
    "        dataset = np.array(dataset).reshape(-1, 1)\n",
    "        dataset = dataset.flatten()\n",
    "        dataset = pd.DataFrame(dataset)\n",
    "        series = scaler[i].inverse_transform(dataset)\n",
    "        series = pd.Series(series.flatten())\n",
    "\n",
    "    # plot forecasts\n",
    "    plot_forecasts(series, forecasts, 1)\n",
    "    \n",
    "    best_model.summary()\n",
    "\n",
    "    # Print out table of actual and predicted values for each weather station\n",
    "    for j in range(11):\n",
    "        print(\"Weather Station \"+str(j+1)+\":\")\n",
    "        print(\"Actual Temp\\tPredicted Temp\\tDifference\")\n",
    "        print(\"-----------\\t--------------\\t----------\")\n",
    "        for k in range(n_seq):\n",
    "            diff = forecasts[j][k] - actual[j][k]\n",
    "            print(f\"{actual[j][k]:.2f}\\t\\t{forecasts[j][k]:.2f}\\t\\t{diff:.2f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('LSTM Training and Validation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f81763-f40f-44f9-8165-afc8f9fd7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the forecasts in the context of the original dataset\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "    # plot the entire dataset in blue\n",
    "    plt.plot(series.values)\n",
    "    # plot the forecasts in red\n",
    "    for i in range(len(forecasts)):\n",
    "        off_s = len(series) - n_test + i - 1\n",
    "        off_e = off_s + len(forecasts[i]) + 1\n",
    "        xaxis = [x for x in range(off_s, off_e)]\n",
    "        yaxis = [series.values[off_s]] + forecasts[i]\n",
    "        plt.plot(xaxis, yaxis, color='red')\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "\n",
    "for j in range(len(X_test)):\n",
    "    print(\"Weather Station \"+str(j+1)+\":\")\n",
    "    dataset_df = pd.DataFrame(y_test[j].flatten())\n",
    "    dataset = dataset_df.values\n",
    "    dataset = dataset[:, 0]\n",
    "    dataset = np.array(dataset).reshape(-1, 1)\n",
    "    dataset = dataset.flatten()\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    series = scaler[i].inverse_transform(dataset)\n",
    "    series = pd.Series(series.flatten())\n",
    "\n",
    "    # plot forecasts\n",
    "    plot_forecasts(series, forecasts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45a0e8-1956-4c3e-9a67-68f51f093bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dates corresponding to the test set predictions\n",
    "test_dates = df.index[-len(actual[0]):]\n",
    "\n",
    "# Print out plots of actual and predicted values for each weather station\n",
    "for j in range(11):\n",
    "    print(\"Weather Station \"+str(j+1)+\":\")\n",
    "    # Create a DataFrame for plotting\n",
    "    results_df = pd.DataFrame({\n",
    "        'Month': test_dates,\n",
    "        'Actual': actual[j],\n",
    "        'Predicted': forecasts[j]\n",
    "    })\n",
    "            \n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(results_df['Month'], results_df['Actual'], label='Actual Temp')\n",
    "    plt.plot(results_df['Month'], results_df['Predicted'], label='Predicted Temp', alpha=0.7)\n",
    "    plt.title('CNN-LSTM Model Comparison Temperature Prediction')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Temperature')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot model architecture\n",
    "    filename = \"cnn_lstm_model_optimized_ws_\"+str(j)+\".png\"\n",
    "    plot_model(best_model, to_file=filename, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06043599-c988-4bcf-8cc5-9af7ec85a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(rmse_list))\n",
    "print(np.std(rmse_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13568a5-0f3b-4562-9106-9c00449a9023",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(actual))\n",
    "print(len(actual[0]))\n",
    "print(len(forecasts))\n",
    "print(len(forecasts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2698c0-5237-41ef-b2ee-cb7e2b833453",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368e065-3599-4bac-9994-64344972671e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
