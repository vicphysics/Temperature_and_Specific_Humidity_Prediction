{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db0008f5-916a-4ef8-a990-bd9896704778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\n",
      "C:\\Users\\User\\Modeling\\\n",
      "     temperatures      slp  wet_bulb_temperature  specific_humidity  water  \\\n",
      "0           12.21  1018.53                  7.76               5.39    1.0   \n",
      "1            8.17  1021.23                  4.33               4.30    1.0   \n",
      "2           15.68  1018.97                 10.49               6.51    1.0   \n",
      "3           22.46  1014.69                 16.98              10.21    1.0   \n",
      "4           23.66  1014.24                 18.68              11.74    1.0   \n",
      "..            ...      ...                   ...                ...    ...   \n",
      "222         27.66  1016.01                 23.90              17.03    1.0   \n",
      "223         28.00  1016.53                 22.80              15.20    1.0   \n",
      "224         23.86  1013.56                 20.33              13.73    1.0   \n",
      "225         20.59  1020.43                 14.60               8.35    1.0   \n",
      "226         15.82  1017.05                 13.00               9.05    1.0   \n",
      "\n",
      "        GHI      WDSP  PRCP  SNDP  region  solar_activity   ONI  \n",
      "0    108.34  6.348387  0.23   0.0     4.0           20.90 -0.91  \n",
      "1    126.00  4.871429  0.17   0.0     4.0            5.71 -0.67  \n",
      "2    180.94  6.396774  0.17   0.0     4.0           17.29 -0.71  \n",
      "3    241.31  5.590000  0.11   0.0     4.0           50.27 -0.32  \n",
      "4    260.04  4.967742  0.11   0.0     4.0           37.23 -0.09  \n",
      "..      ...       ...   ...   ...     ...             ...   ...  \n",
      "222     NaN  3.596774  0.13   0.0     4.0          196.55  0.05  \n",
      "223     NaN  3.306452  0.03   0.0     4.0          215.52 -0.11  \n",
      "224     NaN  3.570000  0.15   0.0     4.0          141.37 -0.25  \n",
      "225     NaN  3.474194  0.00   0.0     4.0          166.39 -0.26  \n",
      "226     NaN  4.653333  0.17   0.0     4.0          152.47 -0.19  \n",
      "\n",
      "[227 rows x 12 columns]\n",
      "[[ 1.22100e+01  1.01853e+03  7.76000e+00 ...  4.00000e+00  2.09000e+01\n",
      "  -9.10000e-01]\n",
      " [ 8.17000e+00  1.02123e+03  4.33000e+00 ...  4.00000e+00  5.71000e+00\n",
      "  -6.70000e-01]\n",
      " [ 1.56800e+01  1.01897e+03  1.04900e+01 ...  4.00000e+00  1.72900e+01\n",
      "  -7.10000e-01]\n",
      " ...\n",
      " [ 2.38600e+01  1.01356e+03  2.03300e+01 ...  4.00000e+00  1.41370e+02\n",
      "  -2.50000e-01]\n",
      " [ 2.05900e+01  1.02043e+03  1.46000e+01 ...  4.00000e+00  1.66390e+02\n",
      "  -2.60000e-01]\n",
      " [ 1.58200e+01  1.01705e+03  1.30000e+01 ...  4.00000e+00  1.52470e+02\n",
      "  -1.90000e-01]]\n",
      "0      12.21\n",
      "1       8.17\n",
      "2      15.68\n",
      "3      22.46\n",
      "4      23.66\n",
      "       ...  \n",
      "222    27.66\n",
      "223    28.00\n",
      "224    23.86\n",
      "225    20.59\n",
      "226    15.82\n",
      "Length: 227, dtype: float64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2724 into shape (227,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 235\u001b[0m\n\u001b[0;32m    232\u001b[0m n_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# prepare data\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m scaler, train, test \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_lag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(train)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(test)\n",
      "Cell \u001b[1;32mIn[5], line 72\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(data, n_test, n_lag, n_seq)\u001b[0m\n\u001b[0;32m     70\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m     71\u001b[0m data_values \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 72\u001b[0m data_values \u001b[38;5;241m=\u001b[39m \u001b[43mdata_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m supervised \u001b[38;5;241m=\u001b[39m series_to_supervised(data, n_lag, n_seq)\n\u001b[0;32m     74\u001b[0m supervised_values \u001b[38;5;241m=\u001b[39m supervised\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 2724 into shape (227,1)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Activation\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "import keras.backend as K\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def mape (y_true, y_pred):\n",
    "    return 100*K.mean(K.sqrt(K.square(y_true - y_pred))/y_true)\n",
    "\n",
    "def pearson (y_true, y_pred):\n",
    "    return (K.square(K.mean((y_true - K.mean(y_true))*(y_pred - K.mean(y_pred)))))/(K.mean(K.square(y_true - K.mean(y_true)))*K.mean(K.square(y_pred - K.mean(y_pred))))\n",
    " \n",
    "# convert time series into a supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    " \n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    " \n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(data, n_test, n_lag, n_seq):\n",
    "    # transform into supervised learning problem X, Y\n",
    "    df = pd.DataFrame(data)\n",
    "    data_values = df.values\n",
    "    data_values = data_values.reshape(len(data_values), 1)\n",
    "    supervised = series_to_supervised(data, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "\t# transform training data to be stationary\n",
    "    diff_series = difference(train, 1)\n",
    "    diff_values = []\n",
    "    for i in range(len(diff_series)):\n",
    "        diff_values.append([diff_series[i][0]])\n",
    "    print(diff_series)\n",
    "    print(diff_values)\n",
    "    #diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "    # rescale values to 0, 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    return scaler, train, test\n",
    " \n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, return_sequences=True, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(16, input_dim=y.shape[0], activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=rmse, optimizer='adam', metrics=['accuracy', 'mae', rmse, mape, pearson])\n",
    "    # fit network\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model\n",
    " \n",
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "\t# reshape input pattern to [samples, timesteps, features]\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\t# make forecast\n",
    "\tforecast = model.predict(X, batch_size=n_batch)\n",
    "\t# convert to array\n",
    "\treturn [x for x in forecast[0, :]]\n",
    " \n",
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "\tforecasts = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\tX, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "\t\t# make forecast\n",
    "\t\tforecast = forecast_lstm(model, X, n_batch)\n",
    "\t\t# store the forecast\n",
    "\t\tforecasts.append(forecast)\n",
    "\treturn forecasts\n",
    " \n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "\t# invert first forecast\n",
    "\tinverted = list()\n",
    "\tinverted.append(forecast[0] + last_ob)\n",
    "\t# propagate difference forecast using inverted first value\n",
    "\tfor i in range(1, len(forecast)):\n",
    "\t\tinverted.append(forecast[i] + inverted[i-1])\n",
    "\treturn inverted\n",
    " \n",
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "\tinverted = list()\n",
    "\tfor i in range(len(forecasts)):\n",
    "\t\t# create array from forecast\n",
    "\t\tforecast = array(forecasts[i])\n",
    "\t\tforecast = forecast.reshape(1, len(forecast))\n",
    "\t\t# invert scaling\n",
    "\t\tinv_scale = scaler.inverse_transform(forecast)\n",
    "\t\tinv_scale = inv_scale[0, :]\n",
    "\t\t# invert differencing\n",
    "\t\tindex = len(series) - n_test + i - 1\n",
    "\t\tlast_ob = series.values[index]\n",
    "\t\tinv_diff = inverse_difference(last_ob, inv_scale)\n",
    "\t\t# store\n",
    "\t\tinverted.append(inv_diff)\n",
    "\treturn inverted\n",
    " \n",
    "# evaluate the RMSE for each forecast time step\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "\tfor i in range(n_seq):\n",
    "\t\tactual = [row[i] for row in test]\n",
    "\t\tpredicted = [forecast[i] for forecast in forecasts]\n",
    "\t\trmse = sqrt(mean_squared_error(actual, predicted))\n",
    "\t\tprint('t+%d RMSE: %f' % ((i+1), rmse))\n",
    " \n",
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "\t# plot the entire dataset in blue\n",
    "\tpyplot.plot(series.values)\n",
    "\t# plot the forecasts in red\n",
    "\tfor i in range(len(forecasts)):\n",
    "\t\toff_s = len(series) - n_test + i - 1\n",
    "\t\toff_e = off_s + len(forecasts[i]) + 1\n",
    "\t\txaxis = [x for x in range(off_s, off_e)]\n",
    "\t\tyaxis = [series.values[off_s]] + forecasts[i]\n",
    "\t\tpyplot.plot(xaxis, yaxis, color='red')\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\n",
    "\n",
    "# Get the current working directory \n",
    "current_directory = os.getcwd() \n",
    "\n",
    "# Print the current working directory \n",
    "print(current_directory)\n",
    "\n",
    "# Define the directory containing the files \n",
    "path = current_directory+\"\\\\Modeling\\\\\"\n",
    "print(path)\n",
    "\n",
    "filename = path + 'Final_Monthly_Dataset.csv'\n",
    "\n",
    "# load dataset\n",
    "df = read_csv(filename, header=0, parse_dates=[0], index_col=0, date_format='%Y-%m')\n",
    "\n",
    "\n",
    "df = df.rename(columns={'Unnamed: 0' : 'indices'})\n",
    "\n",
    "#Remove unused columns\n",
    "df = df.drop(['Day', 'vapor_pressure'], axis=1)\n",
    "\n",
    "# Round numbers in columns to reasonable precision\n",
    "df['temperatures'] = np.round(df['temperatures'], 2)\n",
    "df['slp'] = np.round(df['slp'], 2)\n",
    "df['wet_bulb_temperature'] = np.round(df['wet_bulb_temperature'], 2)\n",
    "df['specific_humidity'] = np.round(df['specific_humidity'], 2)\n",
    "df['GHI'] = np.round(df['GHI'], 2)\n",
    "df['PRCP'] = np.round(df['PRCP'], 2)\n",
    "df['SNDP'] = np.round(df['SNDP'], 2)\n",
    "df['solar_activity'] = np.round(df['solar_activity'], 2)\n",
    "df['ONI'] = np.round(df['ONI'], 2)\n",
    "df['water'] = np.round(df['water'], 0)\n",
    "df['region'] = np.round(df['region'], 0)\n",
    "\n",
    "df_trimmed = df[df['file_id']==6501]\n",
    "df_trimmed = df_trimmed.drop(['Year', 'Month', 'file_id', 'date', 'latitude', 'longitude', 'elevation'], axis=1)\n",
    "\n",
    "print(df_trimmed)\n",
    "\n",
    "dataset = df_trimmed.values\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "series = pd.Series(dataset[:, 0])\n",
    "\n",
    "print(series)\n",
    "\n",
    "# configure\n",
    "n_lag = 180\n",
    "n_seq = 3\n",
    "n_test = 10\n",
    "n_epochs = 500\n",
    "n_batch = 1\n",
    "\n",
    "# prepare data\n",
    "scaler, train, test = prepare_data(dataset, n_test, n_lag, n_seq)\n",
    "\n",
    "print(train)\n",
    "print(test)\n",
    "\n",
    "# fit model\n",
    "model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs)\n",
    "# make forecasts\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "# inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, forecasts, scaler, n_test+2)\n",
    "actual = [row[n_lag:] for row in test]\n",
    "actual = inverse_transform(series, actual, scaler, n_test+2)\n",
    "# evaluate forecasts\n",
    "evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "# plot forecasts\n",
    "plot_forecasts(series, forecasts, n_test+2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9fad724-0d01-457e-ae2e-2fe5abb5f39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19.75041181087494], [26.533977971076965], [29.065070939064025], [28.94993227481842], [26.841258268356324], [22.29028769493103], [16.372993183135986], [18.743392543792723], [10.081988887786865], [12.112516498565673], [16.741372995376587], [19.84004264831543], [25.637829780578613], [29.245882234573365], [30.657527141571045], [28.378454327583313], [27.007976083755494], [21.407201623916627], [15.118088626861573], [14.49697214126587], [14.826962184906005], [16.216934661865235], [18.460636472702028], [19.53598949432373], [24.946696224212648], [28.95317742347717], [29.54490233421326], [30.934776287078858], [27.261948585510254], [22.9331218624115], [16.34665949821472], [15.274687271118164], [9.591117248535156], [16.65203260421753], [17.896561851501467], [21.985059986114504], [26.642856760025026], [28.467265038490297], [29.62603238582611], [30.203965187072754]]\n"
     ]
    }
   ],
   "source": [
    "print(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09943514-ef81-41c0-81fe-2f23d2c75957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x0000025B997177C0>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea996e00-9f29-483d-92fa-f70c03ffe79e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
