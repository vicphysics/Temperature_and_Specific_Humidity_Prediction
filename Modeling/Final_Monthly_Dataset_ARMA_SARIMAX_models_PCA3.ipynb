{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e24c32d-9e57-42e1-978a-c793ab9a4a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\n",
      "C:\\Users\\User\\Modeling\\\n",
      "             date  file_id  temperatures          slp  wet_bulb_temperature  \\\n",
      "0      2006-01-31     6501     12.209677  1018.534543              7.759507   \n",
      "1      2006-02-28     6501      8.174541  1021.230347              4.326557   \n",
      "2      2006-03-31     6501     15.676613  1018.968548             10.491486   \n",
      "3      2006-04-30     6501     22.464167  1014.686944             16.981874   \n",
      "4      2006-05-31     6501     23.657258  1014.236828             18.675700   \n",
      "...           ...      ...           ...          ...                   ...   \n",
      "24057  2024-07-31     9858     28.604704  1013.969355             22.781443   \n",
      "24058  2024-08-31     9858     29.114919  1015.112097             22.875429   \n",
      "24059  2024-09-30     9858     24.570278  1014.393750             18.061455   \n",
      "24060  2024-10-31     9858     21.159140  1018.230376             13.388460   \n",
      "24061  2024-11-30     9858     12.112917  1015.400069              8.511346   \n",
      "\n",
      "       specific_humidity  water         GHI      WDSP      PRCP  SNDP  \\\n",
      "0               5.386935    1.0  108.340054  6.348387  0.234516   0.0   \n",
      "1               4.299929    1.0  126.000000  4.871429  0.166429   0.0   \n",
      "2               6.505135    1.0  180.936828  6.396774  0.170645   0.0   \n",
      "3              10.211263    1.0  241.309722  5.590000  0.108000   0.0   \n",
      "4              11.737971    1.0  260.038978  4.967742  0.113226   0.0   \n",
      "...                  ...    ...         ...       ...       ...   ...   \n",
      "24057          15.211417    1.0         NaN  7.200000  0.080323   0.0   \n",
      "24058          15.149811    1.0         NaN  7.200000  0.115161   0.0   \n",
      "24059          10.720199    1.0         NaN  7.200000  0.013667   0.0   \n",
      "24060           6.989922    1.0         NaN  7.200000  0.035806   0.0   \n",
      "24061           6.313758    1.0         NaN  7.200000  0.283667   0.0   \n",
      "\n",
      "       latitude  longitude  elevation  region    Year  Month   Day  \\\n",
      "0        33.636    -91.756       85.0     4.0  2006.0    1.0  16.0   \n",
      "1        33.636    -91.756       85.0     4.0  2006.0    2.0  14.5   \n",
      "2        33.636    -91.756       85.0     4.0  2006.0    3.0  16.0   \n",
      "3        33.636    -91.756       85.0     4.0  2006.0    4.0  15.5   \n",
      "4        33.636    -91.756       85.0     4.0  2006.0    5.0  16.0   \n",
      "...         ...        ...        ...     ...     ...    ...   ...   \n",
      "24057    36.118    -97.091      271.3     4.0  2024.0    7.0  16.0   \n",
      "24058    36.118    -97.091      271.3     4.0  2024.0    8.0  16.0   \n",
      "24059    36.118    -97.091      271.3     4.0  2024.0    9.0  15.5   \n",
      "24060    36.118    -97.091      271.3     4.0  2024.0   10.0  16.0   \n",
      "24061    36.118    -97.091      271.3     4.0  2024.0   11.0  15.5   \n",
      "\n",
      "       solar_activity   ONI  \n",
      "0           20.903226 -0.91  \n",
      "1            5.714286 -0.67  \n",
      "2           17.290323 -0.71  \n",
      "3           50.266667 -0.32  \n",
      "4           37.225806 -0.09  \n",
      "...               ...   ...  \n",
      "24057      196.548387  0.05  \n",
      "24058      215.516129 -0.11  \n",
      "24059      141.366667 -0.25  \n",
      "24060      166.387097 -0.26  \n",
      "24061      152.466667 -0.19  \n",
      "\n",
      "[24062 rows x 20 columns]\n",
      "Index(['date', 'file_id', 'temperatures', 'slp', 'wet_bulb_temperature',\n",
      "       'specific_humidity', 'water', 'GHI', 'WDSP', 'PRCP', 'SNDP', 'latitude',\n",
      "       'longitude', 'elevation', 'region', 'Year', 'Month', 'Day',\n",
      "       'solar_activity', 'ONI'],\n",
      "      dtype='object')\n",
      "           date  file_id  temperatures          slp  wet_bulb_temperature  \\\n",
      "0    2006-01-31     6501     12.209677  1018.534543              7.759507   \n",
      "1    2006-02-28     6501      8.174541  1021.230347              4.326557   \n",
      "2    2006-03-31     6501     15.676613  1018.968548             10.491486   \n",
      "3    2006-04-30     6501     22.464167  1014.686944             16.981874   \n",
      "4    2006-05-31     6501     23.657258  1014.236828             18.675700   \n",
      "..          ...      ...           ...          ...                   ...   \n",
      "175  2020-08-31     6501     26.467608  1014.157056             22.787858   \n",
      "176  2020-09-30     6501     23.412084  1016.116811             20.507403   \n",
      "177  2020-10-31     6501     16.914785  1017.843946             14.035856   \n",
      "178  2020-11-30     6501     13.667271  1022.174273             10.527751   \n",
      "179  2020-12-31     6501      8.045607  1020.147513              5.348200   \n",
      "\n",
      "     specific_humidity  water         GHI      WDSP      PRCP  SNDP  latitude  \\\n",
      "0             5.386935    1.0  108.340054  6.348387  0.234516   0.0    33.636   \n",
      "1             4.299929    1.0  126.000000  4.871429  0.166429   0.0    33.636   \n",
      "2             6.505135    1.0  180.936828  6.396774  0.170645   0.0    33.636   \n",
      "3            10.211263    1.0  241.309722  5.590000  0.108000   0.0    33.636   \n",
      "4            11.737971    1.0  260.038978  4.967742  0.113226   0.0    33.636   \n",
      "..                 ...    ...         ...       ...       ...   ...       ...   \n",
      "175          15.929425    1.0  262.030914  3.603226  0.151290   0.0    33.636   \n",
      "176          14.291576    1.0  193.822222  3.946667  0.202000   0.0    33.636   \n",
      "177           9.483893    1.0  151.596774  4.003226  0.185806   0.0    33.636   \n",
      "178           7.237310    1.0  143.177778  3.846667  0.085333   0.0    33.636   \n",
      "179           5.011661    1.0  101.201613  5.064516  0.218065   0.0    33.636   \n",
      "\n",
      "     longitude  elevation  region    Year  Month   Day  solar_activity   ONI  \n",
      "0      -91.756       85.0     4.0  2006.0    1.0  16.0       20.903226 -0.91  \n",
      "1      -91.756       85.0     4.0  2006.0    2.0  14.5        5.714286 -0.67  \n",
      "2      -91.756       85.0     4.0  2006.0    3.0  16.0       17.290323 -0.71  \n",
      "3      -91.756       85.0     4.0  2006.0    4.0  15.5       50.266667 -0.32  \n",
      "4      -91.756       85.0     4.0  2006.0    5.0  16.0       37.225806 -0.09  \n",
      "..         ...        ...     ...     ...    ...   ...             ...   ...  \n",
      "175    -91.756       85.0     4.0  2020.0    8.0  16.0        7.483871 -0.59  \n",
      "176    -91.756       85.0     4.0  2020.0    9.0  15.5        0.600000 -0.83  \n",
      "177    -91.756       85.0     4.0  2020.0   10.0  16.0       14.645161 -1.25  \n",
      "178    -91.756       85.0     4.0  2020.0   11.0  15.5       34.533333 -1.42  \n",
      "179    -91.756       85.0     4.0  2020.0   12.0  16.0       23.129032 -1.15  \n",
      "\n",
      "[180 rows x 20 columns]\n",
      "0      4.765677\n",
      "1      4.283687\n",
      "2      7.452365\n",
      "3      7.838986\n",
      "4     10.992493\n",
      "5     15.799053\n",
      "6     16.828573\n",
      "7     16.879493\n",
      "8     13.462827\n",
      "9     11.280438\n",
      "10     5.979501\n",
      "11     8.763690\n",
      "12     3.950746\n",
      "13     4.614411\n",
      "14     6.013492\n",
      "15     8.477910\n",
      "16    13.254251\n",
      "17    16.172824\n",
      "18    17.748037\n",
      "19    17.333103\n",
      "20    12.777121\n",
      "21     7.497689\n",
      "22     6.962694\n",
      "23     6.502425\n",
      "24     6.096455\n",
      "25     6.720918\n",
      "26     7.321654\n",
      "27     8.309465\n",
      "28    11.989868\n",
      "29    15.776641\n",
      "30    18.068202\n",
      "31    17.686919\n",
      "32    13.720859\n",
      "33     9.686980\n",
      "34     6.510424\n",
      "35     5.231929\n",
      "36     4.408999\n",
      "37     6.314477\n",
      "38     7.570045\n",
      "39     9.878189\n",
      "40    14.353203\n",
      "41    15.744895\n",
      "42    17.029131\n",
      "43    15.199941\n",
      "44    13.729675\n",
      "45     8.350029\n",
      "46     9.053894\n",
      "Name: specific_humidity, dtype: float64\n",
      "0      7.187231\n",
      "1      3.840030\n",
      "2     14.907059\n",
      "3     16.443261\n",
      "4     20.938914\n",
      "5     25.610556\n",
      "6     27.197446\n",
      "7     27.310619\n",
      "8     23.725139\n",
      "9     19.711694\n",
      "10    10.948194\n",
      "11    14.330242\n",
      "12     6.084677\n",
      "13     7.603539\n",
      "14    13.178898\n",
      "15    17.306173\n",
      "16    23.497581\n",
      "17    26.706636\n",
      "18    28.929301\n",
      "19    26.748253\n",
      "20    23.777917\n",
      "21    18.102688\n",
      "22    11.148056\n",
      "23     9.064919\n",
      "24    10.701142\n",
      "25    12.230454\n",
      "26    14.602285\n",
      "27    17.223611\n",
      "28    22.694220\n",
      "29    26.470416\n",
      "30    28.013038\n",
      "31    28.725807\n",
      "32    24.500408\n",
      "33    19.440457\n",
      "34    12.577500\n",
      "35     9.982124\n",
      "36     4.862903\n",
      "37    12.739080\n",
      "38    14.492339\n",
      "39    19.263055\n",
      "40    24.165919\n",
      "41    26.778472\n",
      "42    27.661962\n",
      "43    28.003092\n",
      "44    23.858681\n",
      "45    20.594332\n",
      "46    15.823157\n",
      "Name: temperatures, dtype: float64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py:230\u001b[0m, in \u001b[0;36m_coerce_method.<locals>.wrapper\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot convert the series to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot convert the series to <class 'float'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mappend(StandardScaler())\n\u001b[0;32m    102\u001b[0m     scaled_data\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0\u001b[39m])    \n\u001b[1;32m--> 103\u001b[0m     scaler[i], scaled_data[i] \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m dates \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mdate_range(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2006-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m, periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m180\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mME\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# ARMA model (for ARMA model, d=0 to remove Integrate (I) component in SARIMAX model and all seasonal parameters are zero)\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Predict temperature\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# train the normalization\u001b[39;00m\n\u001b[0;32m     22\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 23\u001b[0m scaler1 \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m, StandardDeviation: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (scaler1\u001b[38;5;241m.\u001b[39mmean_, sqrt(scaler1\u001b[38;5;241m.\u001b[39mvar_)))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# normalize the dataset and print\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:930\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    929\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 930\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    938\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    837\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import statsmodels.api as sm\n",
    "from pandas import Series\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from random import random\n",
    "#!pip install pmdarima --quiet\n",
    "import pmdarima as pm\n",
    "\n",
    "# Code adapted from https://medium.com/data-science/time-series-forecasting-with-arima-sarima-and-sarimax-ee61099e78f6\n",
    "# Plot data to view\n",
    "def plot_data(df, feature):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.title(str(feature)+\" by Month\")\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel(str(feature))\n",
    "    plt.plot(df)\n",
    "    plt.show()\n",
    "\n",
    "#Determine rolling statistics to find trends\n",
    "def rolling_statistics(df):\n",
    "    df[\"rolling_avg\"] = df.rolling(window=12).mean() #window size 12 denotes 12 months, giving rolling mean at yearly level\n",
    "    df[\"rolling_std\"] = df.rolling(window=12).std()\n",
    "\n",
    "    #Plot rolling statistics\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(df, color='#379BDB', label='Original')\n",
    "    plt.plot(df[\"rolling_avg\"], color='#D22A0D', label='Rolling Mean')\n",
    "    plt.plot(df[\"rolling_std\"], color='#142039', label='Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "\n",
    "#Augmented Dickey-Fuller Test to test if the time series is stationary\n",
    "#If ADF has p <= 0.05, data are stationary\n",
    "def ADF(df):\n",
    "    print('Results of Dickey Fuller Test for temperature:')\n",
    "    dftest = adfuller(df, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    \n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "\n",
    "    print(dfoutput)\n",
    "\n",
    "    return dfoutput\n",
    "\n",
    "#Standard ARIMA Model\n",
    "def fit_ARIMA_model(df):\n",
    "    model = pm.auto_arima(df, \n",
    "                      start_p=1, \n",
    "                      start_q=1,\n",
    "                      test='adf', # use adftest to find optimal 'd'\n",
    "                      max_p=12, max_q=12, # maximum p and q\n",
    "                      m=1, # frequency of series (if m==1, seasonal is set to FALSE automatically)\n",
    "                      d=None,# let model determine 'd'\n",
    "                      seasonal=False, # No Seasonality for standard ARIMA\n",
    "                      trace=False, #logs \n",
    "                      error_action='warn', #shows errors ('ignore' silences these)\n",
    "                      suppress_warnings=True,\n",
    "                      stepwise=True\n",
    "\n",
    "    return model\n",
    "\n",
    "# SARIMAX Model\n",
    "def fit_SARIMAX_model(df, exog):\n",
    "    model = pm.auto_arima(df, start_p=1, start_q=1, exogenous=exog,\n",
    "                         test='adf', # use adftest to find optimal 'd'\n",
    "                         max_p=12, max_q=12, m=12, #12 is the frequency of the cycle\n",
    "                         start_P=0, seasonal=True,\n",
    "                         d=None, D=1, \n",
    "                         trace=False,\n",
    "                         error_action='ignore',  \n",
    "                         suppress_warnings=True, \n",
    "                         stepwise=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_diagnostics(model):\n",
    "    model.plot_diagnostics(figsize=(15,12))\n",
    "    plt.show\n",
    "    \n",
    "def ARIMA_forecast(ARIMA_model, df, periods):\n",
    "    # Forecast\n",
    "    n_periods = periods\n",
    "    fitted, confint = ARIMA_model.predict(n_periods=n_periods, return_conf_int=True)\n",
    "    index_of_fc = pd.date_range(df.index[-1] + pd.DateOffset(months=1), periods = n_periods, freq='MS')\n",
    "\n",
    "    # make series for plotting purpose\n",
    "    fitted_series = pd.Series(fitted, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(df, color='#1f76b4')\n",
    "    plt.plot(fitted_series, color='darkgreen')\n",
    "    plt.fill_between(lower_series.index, \n",
    "                    lower_series, \n",
    "                    upper_series, \n",
    "                    color='k', alpha=.15)\n",
    "\n",
    "    plt.title(\"ARIMA Forecast\")\n",
    "    plt.show()\n",
    "\n",
    "    return fitted_series, lower_series, upper_series\n",
    "\n",
    "def SARIMAX_forecast(SARIMAX_model, df, exog, periods):\n",
    "    # Forecast\n",
    "    n_periods = periods\n",
    "\n",
    "    forecast_df = pd.DataFrame({\"month\":pd.date_range(df.index[-1], periods = n_periods, freq='MS').month},\n",
    "                    index = pd.date_range(df.index[-1] + pd.DateOffset(months=1), periods = n_periods, freq='MS'))\n",
    "\n",
    "    fitted, confint = SARIMAX_model.predict(n_periods=n_periods, \n",
    "                                            return_conf_int=True,\n",
    "                                            exogenous=exog)\n",
    "    index_of_fc = pd.date_range(df.index[-1] + pd.DateOffset(months=1), periods = n_periods, freq='MS')\n",
    "\n",
    "    # make series for plotting purpose\n",
    "    fitted_series = pd.Series(fitted, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(df, color='#1f76b4')\n",
    "    plt.plot(fitted_series, color='darkgreen')\n",
    "    plt.fill_between(lower_series.index, \n",
    "                    lower_series, \n",
    "                    upper_series, \n",
    "                    color='k', alpha=.15)\n",
    "\n",
    "    plt.title(\"SARIMAX Forecast\")\n",
    "    plt.show()\n",
    "\n",
    "    return fitted_series, lower_series, upper_series\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "    \n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    \n",
    "def mape (y_true, y_pred):\n",
    "    return 100*K.mean(K.sqrt(K.square(y_true - y_pred))/y_true)\n",
    "    \n",
    "def pearson (y_true, y_pred):\n",
    "    return (K.square(K.mean((y_true - K.mean(y_true))*(y_pred - K.mean(y_pred)))))/(K.mean(K.square(y_true - K.mean(y_true)))*K.mean(K.square(y_pred - K.mean(y_pred))))\n",
    "\n",
    "# create a differenced series\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    "\n",
    "def prepare_data(data):\n",
    "\n",
    "    # prepare data for normalization\n",
    "    series = Series(data)\n",
    "    values = series.values\n",
    "    values = values.reshape((len(values), 1))\n",
    "\n",
    "    # train the normalization\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(values)\n",
    "    print('Mean: %f, StandardDeviation: %f' % (scaler1.mean_, sqrt(scaler1.var_)))\n",
    "    \n",
    "    # normalize the dataset and print\n",
    "    standardized = scaler.transform(values)\n",
    "\n",
    "    scaled_values = np.array(standardized[:,0])\n",
    "\n",
    "    return scaler, scaled_values \n",
    "\n",
    "def plot_kfold(cv, X, y, ax, n_splits, xlim_max=105):\n",
    "    \n",
    "    #Plots the indices for a cross-validation object.\n",
    "    #Taken from https://www.geeksforgeeks.org/cross-validation-using-k-fold-with-scikit-learn/\n",
    "    \n",
    "    #Parameters:\n",
    "    #cv: Cross-validation object\n",
    "    #X: Feature set\n",
    "    #y: Target variable\n",
    "    #ax: Matplotlib axis object\n",
    "    #n_splits: Number of folds in the cross-validation\n",
    "    #xlim_max: Maximum limit for the x-axis\n",
    "        \n",
    "    # Set color map for the plot\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "    cv_split = cv.split(X=X, y=y)\n",
    "        \n",
    "    for i_split, (train_idx, test_idx) in enumerate(cv_split):\n",
    "        # Create an array of NaNs and fill in training/testing indices\n",
    "        indices = np.full(len(X), np.nan)\n",
    "        indices[test_idx], indices[train_idx] = 1, 0\n",
    "            \n",
    "        # Plot the training and testing indices\n",
    "        ax_x = range(len(indices))\n",
    "        ax_y = [i_split + 0.5] * len(indices)\n",
    "        ax.scatter(ax_x, ax_y, c=indices, marker=\"_\", \n",
    "                   lw=10, cmap=cmap_cv, vmin=-0.2, vmax=1.2)\n",
    "    \n",
    "        # Set y-ticks and labels\n",
    "        y_ticks = np.arange(n_splits) + 0.5\n",
    "        ax.set(yticks=y_ticks, yticklabels=range(n_splits),\n",
    "               xlabel=\"Weather Station index (file_id)\", ylabel=\"Fold\",\n",
    "               ylim=[n_splits, -0.2], xlim=[0, xlim_max])\n",
    "    \n",
    "        # Set plot title and create legend\n",
    "        ax.set_title(\"KFold\", fontsize=14)\n",
    "        legend_patches = [Patch(color=cmap_cv(0.8), label=\"Testing set\"), \n",
    "                          Patch(color=cmap_cv(0.02), label=\"Training set\")]\n",
    "        ax.legend(handles=legend_patches, loc=(1.03, 0.8))\n",
    "\n",
    "#Main\n",
    "\n",
    "#Configure\n",
    "n_seq = 60\n",
    "if n_seq > 46:\n",
    "    n_lag = 179 - n_seq + 46\n",
    "else:\n",
    "    n_lag = 179\n",
    "n_time_steps = 227\n",
    "n_test = 1\n",
    "\n",
    "print(\"Model Parameters:\")\n",
    "print(\"n_lag (number of input time steps): \"+str(n_lag))\n",
    "print(\"n_seq (number of output/future prediction time steps): \"+str(n_seq))\n",
    "\n",
    "# Create 2D array with file_ids to use for sample creation\n",
    "array = np.array([\n",
    "    6501, 6541, 6640, 6668, 6678, \n",
    "    6687, 6697, 6714, 6744, 6772, \n",
    "    6783, 6840, 6844, 6854, 6870, \n",
    "    6891, 6895, 6899, 6901, 6909, \n",
    "    6929, 6950, 6963, 6969, 6994, \n",
    "    7032, 7057, 7094, 7095, 7100, \n",
    "    7108, 7116, 7119, 7131, 7139, \n",
    "    7152, 7155, 7156, 7182, 7193, \n",
    "    7202, 7239, 7280, 7286, 7287, \n",
    "    7311, 7321, 7329, 7347, 7350, \n",
    "    7354, 7357, 7361, 7414, 7423, \n",
    "    7424, 7432, 7463, 7482, 7489, \n",
    "    7528, 7531, 7534, 7538, 7549, \n",
    "    7553, 7555, 7562, 7571, 7573, \n",
    "    7574, 7575, 7585, 7599, 7603, \n",
    "    7606, 7622, 7652, 7671, 7704, \n",
    "    7786, 7805, 7816, 7838, 7861, \n",
    "    7862, 7863, 7870, 7892, 7907, \n",
    "    7938, 7962, 7979, 7987, 7999, \n",
    "    8000, 8034, 8083, 8120, 8133, \n",
    "    8184, 8186, 8247, 8248, 9858])\n",
    "\n",
    "#Create arrays holding the 5-fold cross-validation indices gathered for consistency across models\n",
    "train_array = []\n",
    "test_array = []\n",
    "    \n",
    "train_array.append([1, 2, 3, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, \n",
    "                        23, 24, 25, 27, 28, 29, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, \n",
    "                        43, 44, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, \n",
    "                        62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 81, \n",
    "                        82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 95, 97, 98, 100, 101, 102, 103])\n",
    "test_array.append([0, 4, 10, 12, 18, 26, 30, 31, 33, 45, 47, 53, 64, 65, 77, 80, 89, 94, 96, 99, 104])\n",
    "    \n",
    "train_array.append([0, 1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 17, 18, 19, 20, 21, 23, \n",
    "                        24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 36, 37, 38, 41, 43, 45, \n",
    "                        46, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 59, 60, 61, 63, 64, \n",
    "                        65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77, 80, 81, 82, 83, 84, \n",
    "                        86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104])\n",
    "test_array.append([5, 9, 11, 15, 16, 22, 28, 35, 39, 40, 42, 44, 55, 56, 62, 72, 76, 78, 79, 85, 103])\n",
    "    \n",
    "train_array.append([0, 1, 2, 4, 5, 9, 10, 11, 12, 14, 15, 16, 18, 20, 21, 22, 23, 26, \n",
    "                    28, 29, 30, 31, 32, 33, 35, 36, 37, 39, 40, 41, 42, 44, 45, 46, 47, \n",
    "                    48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, \n",
    "                    70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, \n",
    "                    90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104])\n",
    "test_array.append([3, 6, 7, 8, 13, 17, 19, 24, 25, 27, 34, 38, 43, 49, 66, 67, 68, 69, 73, 81, 84])\n",
    "\n",
    "train_array.append([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n",
    "                        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, \n",
    "                        35, 37, 38, 39, 40, 42, 43, 44, 45, 47, 49, 51, 52, 53, 55, 56, \n",
    "                        60, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, \n",
    "                        80, 81, 82, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 99, 102, 103, 104])\n",
    "test_array.append([32, 36, 41, 46, 48, 50, 54, 57, 58, 59, 61, 63, 70, 75, 83, 90, 91, 97, 98, 100, 101])\n",
    "    \n",
    "train_array.append([0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 22,\n",
    "                        24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, \n",
    "                        42, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55, 56, 57, 58, 59, \n",
    "                        61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, \n",
    "                        79, 80, 81, 83, 84, 85, 89, 90, 91, 94, 96, 97, 98, 99, 100, 101, 103, 104])\n",
    "test_array.append([1, 2, 14, 20, 21, 23, 29, 37, 51, 52, 60, 71, 74, 82, 86, 87, 88, 92, 93, 95, 102])\n",
    "    \n",
    "# Equations for three Principal Components from PCA using response variables combined with other predictors\n",
    "#PC1=-0.0002714X1+0.02612X2+0.03858X3-0.007658X4+0.001592X5-0.02087X6+0.8564X7-0.1468X8+0.01192X9-0.0001049X10+0.01913X11+0.02076X12\n",
    "#PC2=0.0003944X1+0.002204X2+0.01052X3+0.3248X4-0.0009976X5-0.04421X6+2.3406X7+0.06103X8+0.08841X9+0.00009018X10+0.05678X11-0.002022X12\n",
    "#PC3=-0.00007998X1-0.0006124X2-0.001063X3-0.01855X4+0.00001956X5+0.01170X6+0.6076X7+0.4664X8-0.002995X9+0.008185X10+0.8815X11-0.0004730X12\n",
    "    \n",
    "# Equations for three Principal Components from PCA omitting both response variables,\n",
    "#PC-1=-0.0004514X1+0.03194X2-0.04343X3+0.002243X4-0.02252X5+0.9877X6-0.2265X7+0.006144X8-0.0001488X9+0.02943X10\n",
    "#PC-2=0.0001702X1+0.005484X2+0.2057X3-0.0003188X4-0.02584X5+1.6963X6-0.05890X7+0.05809X8+1.9748X9+0.03686X10\n",
    "#PC-3=-0.00006323X1-0.001180X2-0.02384X3-0.00002833X4+0.01170X5+0.5204X6+0.4791X7-0.004318X8+0.008271X9+0.8765X10\n",
    "    \n",
    "# Get the current working directory \n",
    "current_directory = os.getcwd() \n",
    "\n",
    "# Print the current working directory \n",
    "print(current_directory)\n",
    "\n",
    "# Define the directory containing the files \n",
    "path = current_directory+\"\\\\Modeling\\\\\"\n",
    "print(path)\n",
    "\n",
    "filename = path + 'Final_Monthly_Dataset.csv'\n",
    "\n",
    "results_df = pd.read_csv(filename) #read csv data into a dataframe\n",
    "\n",
    "results_df = results_df.drop(['Unnamed: 0', 'vapor_pressure'], axis=1)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "print(results_df.columns)\n",
    "\n",
    "results_full_df = results_df #grab full dataset before removing last 4 years\n",
    "\n",
    "results_df = results_df[pd.to_datetime(results_df['date']) < datetime.strptime(\"2021-1-1\", \"%Y-%m-%d\")]\n",
    "\n",
    "# Get test dataframe\n",
    "results_test_df = results_full_df\n",
    "results_test_df = results_test_df[pd.to_datetime(results_test_df['date']) > datetime.strptime(\"2020-12-31\", \"%Y-%m-%d\")]\n",
    "results_test_df = results_test_df.reset_index()\n",
    "\n",
    "#Get one weather station\n",
    "results_df = results_df[results_df['file_id'] == 6501]\n",
    "results_test_df = results_test_df[results_test_df['file_id'] == 6501]\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# Get test values for specific humidity and temperature\n",
    "specific_humidity_test_values = results_test_df['specific_humidity']\n",
    "temperature_test_values = results_test_df['temperatures']\n",
    "\n",
    "print(specific_humidity_test_values)\n",
    "print(temperature_test_values)\n",
    "\n",
    "# define a series for each column in the data frame that needs to be normalized\n",
    "# Normalized columns: temperatures, specific_humidity, slp, water, region, wet_bulb_temperature,\n",
    "# GHI, SNDP, latitude, longitude, elevation, Year, Month, Day, solar_activity, ONI\n",
    "\n",
    "data = []\n",
    "scaler = []\n",
    "scaled_data = []\n",
    "\n",
    "# Get Data\n",
    "data.append(results_df['temperatures'])\n",
    "data.append(results_df['specific_humidity'])\n",
    "data.append(results_df['slp'])\n",
    "data.append(results_df['water'])\n",
    "data.append(results_df['region'])\n",
    "data.append(results_df['wet_bulb_temperature'])\n",
    "data.append(results_df['GHI'])\n",
    "data.append(results_df['SNDP'])\n",
    "data.append(results_df['solar_activity'])\n",
    "data.append(results_df['ONI'])\n",
    "#data11 = results_df['latitude'])\n",
    "#data12 = results_df['longitude'])\n",
    "#data13 = results_df['elevation'])\n",
    "#data14 = results_df['Year'])\n",
    "#data15 = results_df['Month'])\n",
    "#data16 = results_df['Day'])\n",
    "\n",
    "# Data Preparation: Scale Data\n",
    "for i in range(10):\n",
    "    scaler.append(StandardScaler())\n",
    "    scaled_data.append([0])    \n",
    "    scaler[i], scaled_data[i] = prepare_data(data)\n",
    "\n",
    "dates = pd.date_range(start='2006-01-01', periods=180, freq='ME')\n",
    "\n",
    "# ARMA model (for ARMA model, d=0 to remove Integrate (I) component in SARIMAX model and all seasonal parameters are zero)\n",
    "\n",
    "# Predict temperature\n",
    "\n",
    "print(\"ARMA Temperature Predictions:\")\n",
    "\n",
    "y = scaled_data[0] # y = temperature\n",
    "\n",
    "for i in range(9):\n",
    "    j=i+1\n",
    "    exog.append(scaled_data[j])\n",
    "\n",
    "# Reconstruct the data frame with standardized values\n",
    "data = pd.DataFrame({'y': y, 'exog1': exog[0], 'exog2': exog[1], 'exog3': exog[2], 'exog4': exog[3], \n",
    "                     'exog5': exog[4], 'exog6': exog[5], 'exog7': exog[6], 'exog8': exog[7], \n",
    "                     'exog9': exog[8]}, index=dates)\n",
    "\n",
    "# Fit the ARMA model\n",
    "model_temp_ARMA = SARIMAX(data['y'], exog=None, order=(1, 0, 1), seasonal_order=(0, 0, 0, 0))\n",
    "results_temp_ARMA = model_temp_ARMA.fit(disp=False)\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results_temp_ARMA.summary())\n",
    "\n",
    "# Forecasting\n",
    "n_forecast = 47\n",
    "forecast_temp_ARMA = results_temp_ARMA.get_forecast(steps=n_forecast, exog=exog[-n_forecast:])\n",
    "forecast_temp_ARMA_mean = forecast_temp_ARMA.predicted_mean\n",
    "forecast_temp_ARMA_ci = forecast_temp_ARMA.conf_int()\n",
    "\n",
    "# Print the forecasted values\n",
    "print(forecast_temp_ARMA_mean)\n",
    "print(forecast_temp_ARMA_ci)\n",
    "\n",
    "# Reshape data\n",
    "forecast_temp_ARMA_mean = np.array(forecast_temp_ARMA_mean)\n",
    "forecast_temp_ARMA_mean = forecast_temp_ARMA_mean.reshape(-1,1)\n",
    "\n",
    "# Inverse transform and print forecast\n",
    "inversed_temp_ARMA_mean = scaler1.inverse_transform(forecast_temp_ARMA_mean)\n",
    "inversed_temp_ARMA_ci = scaler1.inverse_transform(forecast_temp_ARMA_ci)\n",
    "print(inversed_temp_ARMA_mean)\n",
    "print(inversed_temp_ARMA_ci)\n",
    "print(temperature_test_values)\n",
    "\n",
    "dates_predicted = pd.date_range(start='2021-01-01', periods=47, freq='ME')\n",
    "\n",
    "combined_temp_ARMA = []\n",
    "for i in range(len(temperature_test_values)):\n",
    "    combined_temp_ARMA.append([dates_predicted[i], inversed_temp_ARMA_mean[i, 0], temperature_test_values[i]])\n",
    "\n",
    "combined_temp_ARMA = pd.DataFrame(combined_temp_ARMA)\n",
    "combined_temp_ARMA.columns = ['prediction_date', 'predicted_temp', 'actual_temp']\n",
    "\n",
    "combined_temp_ARMA['error_pct'] = 100 * (combined_temp_ARMA['actual_temp'] - combined_temp_ARMA['predicted_temp'])/combined_temp_ARMA['actual_temp']\n",
    "\n",
    "# Set display option to show all rows\n",
    "pd.set_option('display.max_rows', 47)\n",
    "\n",
    "print(combined_temp_ARMA.head(47))\n",
    "\n",
    "\n",
    "# Predict specific humidity\n",
    "\n",
    "print(\"ARMA Specific Humidity Predictions:\")\n",
    "\n",
    "y = scaled_data[1] # y = specific humidity\n",
    "exog[0] = scaled_data[0] # exog[0] = temperature\n",
    "\n",
    "# Reconstruct the data frame with standardized values\n",
    "data = pd.DataFrame({'y': y, 'exog1': exog[0], 'exog2': exog[1], 'exog3': exog[2], 'exog4': exog[3], \n",
    "                     'exog5': exog[4], 'exog6': exog[5], 'exog7': exog[6], 'exog8': exog[7], \n",
    "                     'exog9': exog[8]}, index=dates)\n",
    "\n",
    "# Fit the SARIMAX model\n",
    "model_sh_ARMA = SARIMAX(data['y'], exog=None, order=(1, 0, 1), seasonal_order=(0, 0, 0, 0))\n",
    "results_sh_ARMA = model_sh_ARMA.fit(disp=False)\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results_sh_ARMA.summary())\n",
    "\n",
    "# Forecasting\n",
    "n_forecast = 47\n",
    "forecast_sh_ARMA = results_sh_ARMA.get_forecast(steps=n_forecast, exog=exog[-n_forecast:])\n",
    "forecast_sh_ARMA_mean = forecast_sh_ARMA.predicted_mean\n",
    "forecast_sh_ARMA_ci = forecast_sh_ARMA.conf_int()\n",
    "\n",
    "# Print the forecasted values\n",
    "print(forecast_sh_ARMA_mean)\n",
    "print(forecast_sh_ARMA_ci)\n",
    "\n",
    "# Reshape data\n",
    "forecast_sh_ARMA_mean = np.array(forecast_sh_ARMA_mean)\n",
    "forecast_sh_ARMA_mean = forecast_sh_ARMA_mean.reshape(-1,1)\n",
    "\n",
    "# Inverse transform and print forecast\n",
    "inversed_sh_ARMA_mean = scaler1.inverse_transform(forecast_sh_ARMA_mean)\n",
    "inversed_sh_ARMA_ci = scaler1.inverse_transform(forecast_sh_ARMA_ci)\n",
    "print(inversed_sh_ARMA_mean)\n",
    "print(inversed_sh_ARMA_ci)\n",
    "print(specific_humidity_test_values)\n",
    "\n",
    "dates_predicted = pd.date_range(start='2021-01-01', periods=47, freq='ME')\n",
    "\n",
    "combined_sh_ARMA = []\n",
    "for i in range(len(specific_humidity_test_values)):\n",
    "    combined_sh_ARMA.append([dates_predicted[i], inversed_sh_ARMA_mean[i, 0], specific_humidity_test_values[i]])\n",
    "\n",
    "combined_sh_ARMA = pd.DataFrame(combined_sh_ARMA)\n",
    "combined_sh_ARMA.columns = ['prediction_date', 'predicted_sh', 'actual_sh']\n",
    "\n",
    "combined_sh_ARMA['error_pct'] = 100 * (combined_sh_ARMA['actual_sh'] - combined_sh_ARMA['predicted_sh'])/combined_sh_ARMA['actual_sh']\n",
    "\n",
    "# Set display option to show all rows\n",
    "pd.set_option('display.max_rows', 47)\n",
    "\n",
    "print(combined_sh_ARMA.head(47))\n",
    "\n",
    "# SARIMAX model\n",
    "\n",
    "# Predict temperature\n",
    "\n",
    "print(\"SARIMAX Temperature Predictions:\")\n",
    "\n",
    "y = scaled_data[0] # y = temperature\n",
    "exog[0] = scaled_data[1] # exog[0] = specific humidity\n",
    "\n",
    "# Reconstruct the data frame with standardized values\n",
    "data = pd.DataFrame({'y': y, 'exog1': exog[0], 'exog2': exog[1], 'exog3': exog[2], 'exog4': exog[3], \n",
    "                     'exog5': exog[4], 'exog6': exog[5], 'exog7': exog[6], 'exog8': exog[7], \n",
    "                     'exog9': exog[8]}, index=dates)\n",
    "\n",
    "# Define the exogenous variables\n",
    "exog = data[['exog1', 'exog2', 'exog3', 'exog4', 'exog5', 'exog6', 'exog7', 'exog8', \n",
    "             'exog9']]\n",
    "\n",
    "print(exog)\n",
    "\n",
    "# Fit the SARIMAX model\n",
    "model_temp_SARIMAX = SARIMAX(data['y'], exog=exog, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "results_temp_SARIMAX = model_temp_SARIMAX.fit(disp=False)\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results_temp_SARIMAX.summary())\n",
    "\n",
    "# Forecasting\n",
    "n_forecast = 47\n",
    "forecast_temp_SARIMAX = results_temp_SARIMAX.get_forecast(steps=n_forecast, exog=exog[-n_forecast:])\n",
    "forecast_temp_SARIMAX_mean = forecast_temp_SARIMAX.predicted_mean\n",
    "forecast_temp_SARIMAX_ci = forecast_temp_SARIMAX.conf_int()\n",
    "\n",
    "# Print the forecasted values\n",
    "print(forecast_temp_SARIMAX_mean)\n",
    "print(forecast_temp_SARIMAX_ci)\n",
    "\n",
    "# Reshape data\n",
    "forecast_temp_SARIMAX_mean = np.array(forecast_temp_SARIMAX_mean)\n",
    "forecast_temp_SARIMAX_mean = forecast_temp_SARIMAX_mean.reshape(-1,1)\n",
    "\n",
    "# Inverse transform and print forecast\n",
    "inversed_temp_SARIMAX_mean = scaler1.inverse_transform(forecast_temp_SARIMAX_mean)\n",
    "inversed_temp_SARIMAX_ci = scaler1.inverse_transform(forecast_temp_SARIMAX_ci)\n",
    "print(inversed_temp_SARIMAX_mean)\n",
    "print(inversed_temp_SARIMAX_ci)\n",
    "print(temperature_test_values)\n",
    "\n",
    "dates_predicted = pd.date_range(start='2021-01-01', periods=47, freq='ME')\n",
    "\n",
    "combined_temp_SARIMAX = []\n",
    "for i in range(len(temperature_test_values)):\n",
    "    combined_temp_SARIMAX.append([dates_predicted[i], inversed_temp_SARIMAX_mean[i, 0], temperature_test_values[i]])\n",
    "\n",
    "combined_temp_SARIMAX = pd.DataFrame(combined_temp_SARIMAX)\n",
    "combined_temp_SARIMAX.columns = ['prediction_date', 'predicted_temp', 'actual_temp']\n",
    "\n",
    "combined_temp_SARIMAX['error_pct'] = 100 * (combined_temp_SARIMAX['actual_temp'] - combined_temp_SARIMAX['predicted_temp'])/combined_temp_SARIMAX['actual_temp']\n",
    "\n",
    "# Set display option to show all rows\n",
    "pd.set_option('display.max_rows', 47)\n",
    "\n",
    "print(combined_temp_SARIMAX.head(47))\n",
    "\n",
    "\n",
    "# Predict specific humidity\n",
    "\n",
    "print(\"SARIMAX Specific Humidity Predictions:\")\n",
    "\n",
    "y = scaled_data[1] # y = specific humidity\n",
    "exog[0] = scaled_data[0] # exog[0] = temperature\n",
    "\n",
    "# Reconstruct the data frame with standardized values\n",
    "data = pd.DataFrame({'y': y, 'exog1': exog[0], 'exog2': exog[1], 'exog3': exog[2], 'exog4': exog[3], \n",
    "                     'exog5': exog[4], 'exog6': exog[5], 'exog7': exog[6], 'exog8': exog[7], \n",
    "                     'exog9': exog[8]}, index=dates)\n",
    "\n",
    "# Define the exogenous variables\n",
    "exog = data[['exog1', 'exog2', 'exog3', 'exog4', 'exog5', 'exog6', 'exog7', 'exog8', \n",
    "             'exog9']]\n",
    "\n",
    "print(exog)\n",
    "\n",
    "# Fit the SARIMAX model\n",
    "model_sh_SARIMAX = SARIMAX(data['y'], exog=exog, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "results_sh_SARIMAX = model_sh_SARIMAX.fit(disp=False)\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results_sh_SARIMAX.summary())\n",
    "\n",
    "# Forecasting\n",
    "n_forecast = 47\n",
    "forecast_sh_SARIMAX = results_sh_SARIMAX.get_forecast(steps=n_forecast, exog=exog[-n_forecast:])\n",
    "forecast_sh_SARIMAX_mean = forecast_sh_SARIMAX.predicted_mean\n",
    "forecast_sh_SARIMAX_ci = forecast_sh_SARIMAX.conf_int()\n",
    "\n",
    "# Print the forecasted values\n",
    "print(forecast_sh_SARIMAX_mean)\n",
    "print(forecast_sh_SARIMAX_ci)\n",
    "\n",
    "# Reshape data\n",
    "forecast_sh_SARIMAX_mean = np.array(forecast_sh_SARIMAX_mean)\n",
    "forecast_sh_SARIMAX_mean = forecast_sh_SARIMAX_mean.reshape(-1,1)\n",
    "\n",
    "# Inverse transform and print forecast\n",
    "inversed_sh_SARIMAX_mean = scaler1.inverse_transform(forecast_sh_SARIMAX_mean)\n",
    "inversed_sh_SARIMAX_ci = scaler1.inverse_transform(forecast_sh_SARIMAX_ci)\n",
    "print(inversed_sh_SARIMAX_mean)\n",
    "print(inversed_sh_SARIMAX_ci)\n",
    "print(specific_humidity_test_values)\n",
    "\n",
    "dates_predicted = pd.date_range(start='2021-01-01', periods=47, freq='ME')\n",
    "\n",
    "combined_sh_SARIMAX = []\n",
    "for i in range(len(specific_humidity_test_values)):\n",
    "    combined_sh_SARIMAX.append([dates_predicted[i], inversed_sh_SARIMAX_mean[i, 0], specific_humidity_test_values[i]])\n",
    "\n",
    "combined_sh_SARIMAX = pd.DataFrame(combined_sh_SARIMAX)\n",
    "combined_sh_SARIMAX.columns = ['prediction_date', 'predicted_sh', 'actual_sh']\n",
    "\n",
    "combined_sh_SARIMAX['error_pct'] = 100 * (combined_sh_SARIMAX['actual_sh'] - combined_sh_SARIMAX['predicted_sh'])/combined_sh_SARIMAX['actual_sh']\n",
    "\n",
    "# Set display option to show all rows\n",
    "pd.set_option('display.max_rows', 47)\n",
    "\n",
    "print(combined_sh_SARIMAX.head(47))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0f7eb-c53b-4001-8e9d-eba947a0697f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70956c2e-a3d3-43de-990f-ff0d4a96de54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
