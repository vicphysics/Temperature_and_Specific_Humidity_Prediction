{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e24c32d-9e57-42e1-978a-c793ab9a4a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "n_lag (number of input time steps): 165\n",
      "n_seq (number of output/future prediction time steps): 60\n",
      "C:\\Users\\User\n",
      "C:\\Users\\User\\Modeling\\\n",
      "             date  file_id  temperatures  specific_humidity  latitude  \\\n",
      "0      2006-01-31     6501     12.209677           5.386935    33.636   \n",
      "1      2006-02-28     6501      8.174541           4.299929    33.636   \n",
      "2      2006-03-31     6501     15.676613           6.505135    33.636   \n",
      "3      2006-04-30     6501     22.464167          10.211263    33.636   \n",
      "4      2006-05-31     6501     23.657258          11.737971    33.636   \n",
      "...           ...      ...           ...                ...       ...   \n",
      "24057  2024-07-31     9858     28.604704          15.211417    36.118   \n",
      "24058  2024-08-31     9858     29.114919          15.149811    36.118   \n",
      "24059  2024-09-30     9858     24.570278          10.720199    36.118   \n",
      "24060  2024-10-31     9858     21.159140           6.989922    36.118   \n",
      "24061  2024-11-30     9858     12.112917           6.313758    36.118   \n",
      "\n",
      "       longitude  elevation    Year  Month   Day       PC1       PC2       PC3  \n",
      "0        -91.756       85.0  2006.0    1.0  16.0 -0.155317  0.223027 -0.026191  \n",
      "1        -91.756       85.0  2006.0    2.0  14.5 -0.193310  0.237194 -0.040092  \n",
      "2        -91.756       85.0  2006.0    3.0  16.0  0.093492  0.213688 -0.030933  \n",
      "3        -91.756       85.0  2006.0    4.0  15.5  0.456312  0.250153 -0.049471  \n",
      "4        -91.756       85.0  2006.0    5.0  16.0  0.566639  0.269474 -0.059252  \n",
      "...          ...        ...     ...    ...   ...       ...       ...       ...  \n",
      "24057    -97.091      271.3  2024.0    7.0  16.0       NaN       NaN       NaN  \n",
      "24058    -97.091      271.3  2024.0    8.0  16.0       NaN       NaN       NaN  \n",
      "24059    -97.091      271.3  2024.0    9.0  15.5       NaN       NaN       NaN  \n",
      "24060    -97.091      271.3  2024.0   10.0  16.0       NaN       NaN       NaN  \n",
      "24061    -97.091      271.3  2024.0   11.0  15.5       NaN       NaN       NaN  \n",
      "\n",
      "[24062 rows x 13 columns]\n",
      "Index(['date', 'file_id', 'temperatures', 'specific_humidity', 'latitude',\n",
      "       'longitude', 'elevation', 'Year', 'Month', 'Day', 'PC1', 'PC2', 'PC3'],\n",
      "      dtype='object')\n",
      "           date  file_id  temperatures  specific_humidity  latitude  \\\n",
      "0    2006-01-31     6501     12.209677           5.386935    33.636   \n",
      "1    2006-02-28     6501      8.174541           4.299929    33.636   \n",
      "2    2006-03-31     6501     15.676613           6.505135    33.636   \n",
      "3    2006-04-30     6501     22.464167          10.211263    33.636   \n",
      "4    2006-05-31     6501     23.657258          11.737971    33.636   \n",
      "..          ...      ...           ...                ...       ...   \n",
      "175  2020-08-31     6501     26.467608          15.929425    33.636   \n",
      "176  2020-09-30     6501     23.412084          14.291576    33.636   \n",
      "177  2020-10-31     6501     16.914785           9.483893    33.636   \n",
      "178  2020-11-30     6501     13.667271           7.237310    33.636   \n",
      "179  2020-12-31     6501      8.045607           5.011661    33.636   \n",
      "\n",
      "     longitude  elevation    Year  Month   Day       PC1       PC2       PC3  \n",
      "0      -91.756       85.0  2006.0    1.0  16.0 -0.155317  0.223027 -0.026191  \n",
      "1      -91.756       85.0  2006.0    2.0  14.5 -0.193310  0.237194 -0.040092  \n",
      "2      -91.756       85.0  2006.0    3.0  16.0  0.093492  0.213688 -0.030933  \n",
      "3      -91.756       85.0  2006.0    4.0  15.5  0.456312  0.250153 -0.049471  \n",
      "4      -91.756       85.0  2006.0    5.0  16.0  0.566639  0.269474 -0.059252  \n",
      "..         ...        ...     ...    ...   ...       ...       ...       ...  \n",
      "175    -91.756       85.0  2020.0    8.0  16.0  0.733214  0.326635 -0.080120  \n",
      "176    -91.756       85.0  2020.0    9.0  15.5  0.498766  0.327333 -0.071603  \n",
      "177    -91.756       85.0  2020.0   10.0  16.0  0.195299  0.304137 -0.062218  \n",
      "178    -91.756       85.0  2020.0   11.0  15.5  0.065938  0.292365 -0.059945  \n",
      "179    -91.756       85.0  2020.0   12.0  16.0 -0.220161  0.245528 -0.038267  \n",
      "\n",
      "[180 rows x 13 columns]\n",
      "0      4.765677\n",
      "1      4.283687\n",
      "2      7.452365\n",
      "3      7.838986\n",
      "4     10.992493\n",
      "5     15.799053\n",
      "6     16.828573\n",
      "7     16.879493\n",
      "8     13.462827\n",
      "9     11.280438\n",
      "10     5.979501\n",
      "11     8.763690\n",
      "12     3.950746\n",
      "13     4.614411\n",
      "14     6.013492\n",
      "15     8.477910\n",
      "16    13.254251\n",
      "17    16.172824\n",
      "18    17.748037\n",
      "19    17.333103\n",
      "20    12.777121\n",
      "21     7.497689\n",
      "22     6.962694\n",
      "23     6.502425\n",
      "24     6.096455\n",
      "25     6.720918\n",
      "26     7.321654\n",
      "27     8.309465\n",
      "28    11.989868\n",
      "29    15.776641\n",
      "30    18.068202\n",
      "31    17.686919\n",
      "32    13.720859\n",
      "33     9.686980\n",
      "34     6.510424\n",
      "35     5.231929\n",
      "36     4.408999\n",
      "37     6.314477\n",
      "38     7.570045\n",
      "39     9.878189\n",
      "40    14.353203\n",
      "41    15.744895\n",
      "42    17.029131\n",
      "43    15.199941\n",
      "44    13.729675\n",
      "45     8.350029\n",
      "46     9.053894\n",
      "Name: specific_humidity, dtype: float64\n",
      "0      7.187231\n",
      "1      3.840030\n",
      "2     14.907059\n",
      "3     16.443261\n",
      "4     20.938914\n",
      "5     25.610556\n",
      "6     27.197446\n",
      "7     27.310619\n",
      "8     23.725139\n",
      "9     19.711694\n",
      "10    10.948194\n",
      "11    14.330242\n",
      "12     6.084677\n",
      "13     7.603539\n",
      "14    13.178898\n",
      "15    17.306173\n",
      "16    23.497581\n",
      "17    26.706636\n",
      "18    28.929301\n",
      "19    26.748253\n",
      "20    23.777917\n",
      "21    18.102688\n",
      "22    11.148056\n",
      "23     9.064919\n",
      "24    10.701142\n",
      "25    12.230454\n",
      "26    14.602285\n",
      "27    17.223611\n",
      "28    22.694220\n",
      "29    26.470416\n",
      "30    28.013038\n",
      "31    28.725807\n",
      "32    24.500408\n",
      "33    19.440457\n",
      "34    12.577500\n",
      "35     9.982124\n",
      "36     4.862903\n",
      "37    12.739080\n",
      "38    14.492339\n",
      "39    19.263055\n",
      "40    24.165919\n",
      "41    26.778472\n",
      "42    27.661962\n",
      "43    28.003092\n",
      "44    23.858681\n",
      "45    20.594332\n",
      "46    15.823157\n",
      "Name: temperatures, dtype: float64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py:230\u001b[0m, in \u001b[0;36m_coerce_method.<locals>.wrapper\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot convert the series to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot convert the series to <class 'float'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 412\u001b[0m\n\u001b[0;32m    410\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mappend(StandardScaler())\n\u001b[0;32m    411\u001b[0m     scaled_data\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0\u001b[39m])    \n\u001b[1;32m--> 412\u001b[0m     scaler[i], scaled_data[i] \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m dates \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mdate_range(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2006-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m, periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m180\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mME\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# ARIMA model (for ARIMA model, all the SARIMAX model seasonal parameters are zero)\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# Predict temperature\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 174\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# train the normalization\u001b[39;00m\n\u001b[0;32m    173\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m--> 174\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m, StandardDeviation: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (scaler1\u001b[38;5;241m.\u001b[39mmean_, sqrt(scaler1\u001b[38;5;241m.\u001b[39mvar_)))\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# normalize the dataset and print\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:930\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    929\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 930\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    938\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    837\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import keras.backend as K\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from random import random\n",
    "#!pip install pmdarima --quiet\n",
    "import pmdarima as pm\n",
    "\n",
    "# Code adapted from https://medium.com/data-science/time-series-forecasting-with-arima-sarima-and-sarimax-ee61099e78f6\n",
    "# Plot data to view\n",
    "def plot_data(df, feature):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.title(str(feature)+\" by Month\")\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel(str(feature))\n",
    "    plt.plot(df)\n",
    "    plt.show()\n",
    "\n",
    "#Determine rolling statistics to find trends\n",
    "def rolling_statistics(df):\n",
    "    df[\"rolling_avg\"] = df.rolling(window=12).mean() #window size 12 denotes 12 months, giving rolling mean at yearly level\n",
    "    df[\"rolling_std\"] = df.rolling(window=12).std()\n",
    "\n",
    "    #Plot rolling statistics\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(df, color='#379BDB', label='Original')\n",
    "    plt.plot(df[\"rolling_avg\"], color='#D22A0D', label='Rolling Mean')\n",
    "    plt.plot(df[\"rolling_std\"], color='#142039', label='Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "\n",
    "#Augmented Dickey-Fuller Test to test if the time series is stationary\n",
    "#If ADF has p <= 0.05, data are stationary\n",
    "def ADF(df):\n",
    "    print('Results of Dickey Fuller Test for temperature:')\n",
    "    dftest = adfuller(df, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    \n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "\n",
    "    print(dfoutput)\n",
    "\n",
    "    return dfoutput\n",
    "\n",
    "#Standard ARIMA Model\n",
    "def fit_ARIMA_model(df):\n",
    "    model = pm.auto_arima(df, start_p=1, start_q=1, test='adf', # use adftest to find optimal 'd' \n",
    "                          max_p=12, max_q=12, # maximum p and q\n",
    "                          m=1, # frequency of series (if m==1, seasonal is set to FALSE automatically)\n",
    "                          d=None,# let model determine 'd'\n",
    "                          seasonal=False, # No Seasonality for standard ARIMA\n",
    "                          trace=False, #logs \n",
    "                          error_action='warn', #shows errors ('ignore' silences these)\n",
    "                          suppress_warnings=True,\n",
    "                          stepwise=True\n",
    "                         )\n",
    "    return model\n",
    "\n",
    "# SARIMAX Model\n",
    "def fit_SARIMAX_model(df, exog):\n",
    "    model = pm.auto_arima(df, start_p=1, start_q=1, exogenous=exog,\n",
    "                         test='adf', # use adftest to find optimal 'd'\n",
    "                         max_p=12, max_q=12, m=12, #12 is the frequency of the cycle\n",
    "                         start_P=0, seasonal=True,\n",
    "                         d=None, D=1, \n",
    "                         trace=False,\n",
    "                         error_action='ignore',  \n",
    "                         suppress_warnings=True, \n",
    "                         stepwise=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_diagnostics(model):\n",
    "    model.plot_diagnostics(figsize=(15,12))\n",
    "    plt.show\n",
    "    \n",
    "def ARIMA_forecast(ARIMA_model, df, periods):\n",
    "    # Forecast\n",
    "    n_periods = periods\n",
    "    fitted, confint = ARIMA_model.predict(n_periods=n_periods, return_conf_int=True)\n",
    "    index_of_fc = pd.date_range(df.index[-1] + pd.DateOffset(months=1), periods = n_periods, freq='MS')\n",
    "\n",
    "    # make series for plotting purpose\n",
    "    fitted_series = pd.Series(fitted, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(df, color='#1f76b4')\n",
    "    plt.plot(fitted_series, color='darkgreen')\n",
    "    plt.fill_between(lower_series.index, \n",
    "                    lower_series, \n",
    "                    upper_series, \n",
    "                    color='k', alpha=.15)\n",
    "\n",
    "    plt.title(\"ARIMA Forecast\")\n",
    "    plt.show()\n",
    "\n",
    "    return fitted_series, lower_series, upper_series\n",
    "\n",
    "def SARIMAX_forecast(SARIMAX_model, df, exog, periods):\n",
    "    # Forecast\n",
    "    n_periods = periods\n",
    "\n",
    "    forecast_df = pd.DataFrame({\"month\":pd.date_range(df.index[-1], periods = n_periods, freq='MS').month},\n",
    "                    index = pd.date_range(df.index[-1] + pd.DateOffset(months=1), periods = n_periods, freq='MS'))\n",
    "\n",
    "    fitted, confint = SARIMAX_model.predict(n_periods=n_periods, \n",
    "                                            return_conf_int=True,\n",
    "                                            exogenous=exog)\n",
    "    index_of_fc = pd.date_range(df.index[-1] + pd.DateOffset(months=1), periods = n_periods, freq='MS')\n",
    "\n",
    "    # make series for plotting purpose\n",
    "    fitted_series = pd.Series(fitted, index=index_of_fc)\n",
    "    lower_series = pd.Series(confint[:, 0], index=index_of_fc)\n",
    "    upper_series = pd.Series(confint[:, 1], index=index_of_fc)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(df, color='#1f76b4')\n",
    "    plt.plot(fitted_series, color='darkgreen')\n",
    "    plt.fill_between(lower_series.index, \n",
    "                    lower_series, \n",
    "                    upper_series, \n",
    "                    color='k', alpha=.15)\n",
    "\n",
    "    plt.title(\"SARIMAX Forecast\")\n",
    "    plt.show()\n",
    "\n",
    "    return fitted_series, lower_series, upper_series\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "    \n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    \n",
    "def mape (y_true, y_pred):\n",
    "    return 100*K.mean(K.sqrt(K.square(y_true - y_pred))/y_true)\n",
    "    \n",
    "def pearson (y_true, y_pred):\n",
    "    return (K.square(K.mean((y_true - K.mean(y_true))*(y_pred - K.mean(y_pred)))))/(K.mean(K.square(y_true - K.mean(y_true)))*K.mean(K.square(y_pred - K.mean(y_pred))))\n",
    "\n",
    "# create a differenced series\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff\n",
    "\n",
    "# transform series into training sets\n",
    "def prepare_training_data(data, n_lag, n_seq, n_time_steps):\n",
    "    \n",
    "    #Prepare data for time series forecasting.\n",
    "        \n",
    "    #Parameters:\n",
    "    #x (array-like): Input features.\n",
    "    #y (array-like): Target values.\n",
    "    #n_test (int): Number of test samples (rows).\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations.\n",
    "    #n_train (int): Number of training samples (rows).\n",
    "        \n",
    "    #Returns:\n",
    "    #tuple: Training and test datasets.\n",
    "    \n",
    "    n_vars = len(data[0][0])\n",
    "\n",
    "    # Each weather station has 227 time steps (the first 180 have no nan values)\n",
    "    # Loop through data, grabbing one weather station (ws) at a time, \n",
    "    # differencing on each ws and separating by training (first 226-n_lag-n_seq-n_test time steps) \n",
    "    # and testing (n_test time steps) to scale data on training only.\n",
    "    # We then recombine the training and testing datasets to change each ws to a supervised learning problem by taking all the first 180 time steps for all 12 predictors\n",
    "    # and changing these to (t-n_lag) to (t-1) since we lose one row through differencing. We then shift forward only one dependent variable (temperature or specific humidity)\n",
    "    # for time steps t to (t+n_seq)\n",
    "\n",
    "\n",
    "    diff_values = []\n",
    "    \n",
    "    for ws in range(84):\n",
    "        \n",
    "        # transform data to be stationary\n",
    "        diff_series = difference(data[ws], 1)\n",
    "        for i in range(len(diff_series)):\n",
    "            diff_values_row = []\n",
    "            for j in range(len(diff_series[0])):\n",
    "                diff_values_row.append(diff_series[i][j])\n",
    "            diff_values.append(diff_values_row)\n",
    "    \n",
    "    # rescale values to 0, 1\n",
    "    scaler_all_features =  MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler =  MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled_values = scaler_all_features.fit_transform(diff_values)\n",
    "    response_train_values = []\n",
    "    for i in range(len(diff_values)):\n",
    "        response_train_values.append(diff_values[i][0]) # Uses first column (temperatures) as response variable\n",
    "    response_train_values = np.array(response_train_values)\n",
    "    response_train_values = response_train_values.reshape(len(response_train_values), 1)\n",
    "\n",
    "    # Fit the scaler for just the response variable for use later when forecasting\n",
    "    response_scaled_values = scaler.fit_transform(response_train_values) \n",
    "    scaled_values = scaler_all_features.transform(diff_values)\n",
    "\n",
    "    train = []\n",
    "\n",
    "    # Transform each weather station as a separate \"batch\"\n",
    "    for ws in range(84):\n",
    "        # transform into supervised learning problem X, y\n",
    "        first = (n_time_steps-1)*ws\n",
    "        last = (n_time_steps-1)*ws+(n_time_steps-2)\n",
    "        scaled_values_batch = scaled_values[first:last]\n",
    "        supervised = series_to_supervised(scaled_values_batch, n_lag, n_seq)\n",
    "        supervised_values = supervised.values\n",
    "        train.append([supervised_values])\n",
    "    \n",
    "    return scaler, scaler_all_features, train\n",
    "\n",
    "# transform series into testing and validation sets\n",
    "def prepare_testing_and_validation_data(data, n_lag, n_seq, n_time_steps, scaler_all_features):\n",
    "    \n",
    "    #Prepare data for time series forecasting.\n",
    "        \n",
    "    #Parameters:\n",
    "    #x (array-like): Input features.\n",
    "    #y (array-like): Target values.\n",
    "    #n_test (int): Number of test samples (rows).\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations.\n",
    "    #n_train (int): Number of training samples (rows).\n",
    "        \n",
    "    #Returns:\n",
    "    #tuple: Training and test datasets.\n",
    "    \n",
    "    n_vars = len(data[0][0])\n",
    "\n",
    "    # Each weather station has 227 time steps (the first 180 have no nan values)\n",
    "    # Loop through data, grabbing one weather station (ws) at a time, \n",
    "    # differencing on each ws and separating by training (first 226-n_lag-n_seq-n_test time steps) \n",
    "    # and testing (n_test time steps) to scale data on training only.\n",
    "    # We then recombine the training and testing datasets to change each ws to a supervised learning problem by taking all the first 180 time steps for all 12 predictors\n",
    "    # and changing these to (t-n_lag) to (t-1) since we lose one row through differencing. We then shift forward only one dependent variable (temperature or specific humidity)\n",
    "    # for time steps t to (t+n_seq)\n",
    "\n",
    "\n",
    "    diff_values = []\n",
    "    \n",
    "    for ws in range(21):\n",
    "        \n",
    "        # transform data to be stationary\n",
    "        diff_series = difference(data[ws], 1)\n",
    "        for i in range(len(diff_series)):\n",
    "            diff_values_row = []\n",
    "            for j in range(len(diff_series[0])):\n",
    "                diff_values_row.append(diff_series[i][j])\n",
    "            diff_values.append(diff_values_row)\n",
    "\n",
    "    # rescale values to 0, 1\n",
    "    scaled_values = scaler_all_features.transform(diff_values)\n",
    "\n",
    "    validation = []\n",
    "    test = []\n",
    "\n",
    "    # Transform each weather station as a separate \"batch\"\n",
    "    for ws in range(21):\n",
    "        # transform into supervised learning problem X, y\n",
    "        first = (n_time_steps-1)*ws\n",
    "        last = (n_time_steps-1)*ws+(n_time_steps-2)\n",
    "        scaled_values_batch = scaled_values[first:last]\n",
    "        supervised = series_to_supervised(scaled_values_batch, n_lag, n_seq)\n",
    "        supervised_values = supervised.values\n",
    "        # training/test/validation split is 80%/10%/10%\n",
    "        if ws < 11:\n",
    "            test.append([supervised_values])\n",
    "        else:\n",
    "            validation.append([supervised_values])\n",
    "    \n",
    "    return validation, test\n",
    "\n",
    "def prepare_data(data):\n",
    "\n",
    "    # prepare data for normalization\n",
    "    series = Series(data)\n",
    "    values = series.values\n",
    "    values = values.reshape((len(values), 1))\n",
    "\n",
    "    # train the normalization\n",
    "    scaler = StandardScaler()\n",
    "    scaler_all_features = StandardScaler()\n",
    "    scaler = scaler.fit(values)\n",
    "    print('Mean: %f, StandardDeviation: %f' % (scaler1.mean_, sqrt(scaler1.var_)))\n",
    "    \n",
    "    # normalize the dataset and print\n",
    "    standardized = scaler.transform(values)\n",
    "\n",
    "    scaled_values = np.array(standardized[:,0])\n",
    "\n",
    "    return scaler, scaler_all_features, scaled_values \n",
    "\n",
    "def plot_kfold(cv, X, y, ax, n_splits, xlim_max=105):\n",
    "    \n",
    "    #Plots the indices for a cross-validation object.\n",
    "    #Taken from https://www.geeksforgeeks.org/cross-validation-using-k-fold-with-scikit-learn/\n",
    "    \n",
    "    #Parameters:\n",
    "    #cv: Cross-validation object\n",
    "    #X: Feature set\n",
    "    #y: Target variable\n",
    "    #ax: Matplotlib axis object\n",
    "    #n_splits: Number of folds in the cross-validation\n",
    "    #xlim_max: Maximum limit for the x-axis\n",
    "        \n",
    "    # Set color map for the plot\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "    cv_split = cv.split(X=X, y=y)\n",
    "        \n",
    "    for i_split, (train_idx, test_idx) in enumerate(cv_split):\n",
    "        # Create an array of NaNs and fill in training/testing indices\n",
    "        indices = np.full(len(X), np.nan)\n",
    "        indices[test_idx], indices[train_idx] = 1, 0\n",
    "            \n",
    "        # Plot the training and testing indices\n",
    "        ax_x = range(len(indices))\n",
    "        ax_y = [i_split + 0.5] * len(indices)\n",
    "        ax.scatter(ax_x, ax_y, c=indices, marker=\"_\", \n",
    "                   lw=10, cmap=cmap_cv, vmin=-0.2, vmax=1.2)\n",
    "    \n",
    "        # Set y-ticks and labels\n",
    "        y_ticks = np.arange(n_splits) + 0.5\n",
    "        ax.set(yticks=y_ticks, yticklabels=range(n_splits),\n",
    "               xlabel=\"Weather Station index (file_id)\", ylabel=\"Fold\",\n",
    "               ylim=[n_splits, -0.2], xlim=[0, xlim_max])\n",
    "    \n",
    "        # Set plot title and create legend\n",
    "        ax.set_title(\"KFold\", fontsize=14)\n",
    "        legend_patches = [Patch(color=cmap_cv(0.8), label=\"Testing set\"), \n",
    "                          Patch(color=cmap_cv(0.02), label=\"Training set\")]\n",
    "        ax.legend(handles=legend_patches, loc=(1.03, 0.8))\n",
    "\n",
    "#Main\n",
    "\n",
    "#Configure\n",
    "n_seq = 60\n",
    "if n_seq > 46:\n",
    "    n_lag = 179 - n_seq + 46\n",
    "else:\n",
    "    n_lag = 179\n",
    "n_time_steps = 227\n",
    "n_test = 1\n",
    "\n",
    "print(\"Model Parameters:\")\n",
    "print(\"n_lag (number of input time steps): \"+str(n_lag))\n",
    "print(\"n_seq (number of output/future prediction time steps): \"+str(n_seq))\n",
    "\n",
    "# Create 2D array with file_ids to use for sample creation\n",
    "array = np.array([\n",
    "    6501, 6541, 6640, 6668, 6678, \n",
    "    6687, 6697, 6714, 6744, 6772, \n",
    "    6783, 6840, 6844, 6854, 6870, \n",
    "    6891, 6895, 6899, 6901, 6909, \n",
    "    6929, 6950, 6963, 6969, 6994, \n",
    "    7032, 7057, 7094, 7095, 7100, \n",
    "    7108, 7116, 7119, 7131, 7139, \n",
    "    7152, 7155, 7156, 7182, 7193, \n",
    "    7202, 7239, 7280, 7286, 7287, \n",
    "    7311, 7321, 7329, 7347, 7350, \n",
    "    7354, 7357, 7361, 7414, 7423, \n",
    "    7424, 7432, 7463, 7482, 7489, \n",
    "    7528, 7531, 7534, 7538, 7549, \n",
    "    7553, 7555, 7562, 7571, 7573, \n",
    "    7574, 7575, 7585, 7599, 7603, \n",
    "    7606, 7622, 7652, 7671, 7704, \n",
    "    7786, 7805, 7816, 7838, 7861, \n",
    "    7862, 7863, 7870, 7892, 7907, \n",
    "    7938, 7962, 7979, 7987, 7999, \n",
    "    8000, 8034, 8083, 8120, 8133, \n",
    "    8184, 8186, 8247, 8248, 9858])\n",
    "\n",
    "#Create arrays holding the 5-fold cross-validation indices gathered for consistency across models\n",
    "train_array = []\n",
    "test_array = []\n",
    "    \n",
    "train_array.append([1, 2, 3, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, \n",
    "                        23, 24, 25, 27, 28, 29, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, \n",
    "                        43, 44, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, \n",
    "                        62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 81, \n",
    "                        82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 95, 97, 98, 100, 101, 102, 103])\n",
    "test_array.append([0, 4, 10, 12, 18, 26, 30, 31, 33, 45, 47, 53, 64, 65, 77, 80, 89, 94, 96, 99, 104])\n",
    "    \n",
    "train_array.append([0, 1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 17, 18, 19, 20, 21, 23, \n",
    "                        24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 36, 37, 38, 41, 43, 45, \n",
    "                        46, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 59, 60, 61, 63, 64, \n",
    "                        65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77, 80, 81, 82, 83, 84, \n",
    "                        86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104])\n",
    "test_array.append([5, 9, 11, 15, 16, 22, 28, 35, 39, 40, 42, 44, 55, 56, 62, 72, 76, 78, 79, 85, 103])\n",
    "    \n",
    "train_array.append([0, 1, 2, 4, 5, 9, 10, 11, 12, 14, 15, 16, 18, 20, 21, 22, 23, 26, \n",
    "                    28, 29, 30, 31, 32, 33, 35, 36, 37, 39, 40, 41, 42, 44, 45, 46, 47, \n",
    "                    48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, \n",
    "                    70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, \n",
    "                    90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104])\n",
    "test_array.append([3, 6, 7, 8, 13, 17, 19, 24, 25, 27, 34, 38, 43, 49, 66, 67, 68, 69, 73, 81, 84])\n",
    "\n",
    "train_array.append([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n",
    "                        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, \n",
    "                        35, 37, 38, 39, 40, 42, 43, 44, 45, 47, 49, 51, 52, 53, 55, 56, \n",
    "                        60, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, \n",
    "                        80, 81, 82, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 99, 102, 103, 104])\n",
    "test_array.append([32, 36, 41, 46, 48, 50, 54, 57, 58, 59, 61, 63, 70, 75, 83, 90, 91, 97, 98, 100, 101])\n",
    "    \n",
    "train_array.append([0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 22,\n",
    "                        24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, \n",
    "                        42, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55, 56, 57, 58, 59, \n",
    "                        61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, \n",
    "                        79, 80, 81, 83, 84, 85, 89, 90, 91, 94, 96, 97, 98, 99, 100, 101, 103, 104])\n",
    "test_array.append([1, 2, 14, 20, 21, 23, 29, 37, 51, 52, 60, 71, 74, 82, 86, 87, 88, 92, 93, 95, 102])\n",
    "    \n",
    "# Equations for three Principal Components from PCA using response variables combined with other predictors\n",
    "#PC1=-0.0002714X1+0.02612X2+0.03858X3-0.007658X4+0.001592X5-0.02087X6+0.8564X7-0.1468X8+0.01192X9-0.0001049X10+0.01913X11+0.02076X12\n",
    "#PC2=0.0003944X1+0.002204X2+0.01052X3+0.3248X4-0.0009976X5-0.04421X6+2.3406X7+0.06103X8+0.08841X9+0.00009018X10+0.05678X11-0.002022X12\n",
    "#PC3=-0.00007998X1-0.0006124X2-0.001063X3-0.01855X4+0.00001956X5+0.01170X6+0.6076X7+0.4664X8-0.002995X9+0.008185X10+0.8815X11-0.0004730X12\n",
    "    \n",
    "# Equations for three Principal Components from PCA omitting both response variables,\n",
    "#PC-1=-0.0004514X1+0.03194X2-0.04343X3+0.002243X4-0.02252X5+0.9877X6-0.2265X7+0.006144X8-0.0001488X9+0.02943X10\n",
    "#PC-2=0.0001702X1+0.005484X2+0.2057X3-0.0003188X4-0.02584X5+1.6963X6-0.05890X7+0.05809X8+1.9748X9+0.03686X10\n",
    "#PC-3=-0.00006323X1-0.001180X2-0.02384X3-0.00002833X4+0.01170X5+0.5204X6+0.4791X7-0.004318X8+0.008271X9+0.8765X10\n",
    "\n",
    "#X1 = slp\t\n",
    "#X2 = wbt\t\n",
    "#X3 = water\t\n",
    "#X4 = GHI\n",
    "#X5 = WDSP\n",
    "#X6 = PRCP\t\n",
    "#X7 = SNDP\t\n",
    "#X8 = Region\t\n",
    "#X9 = SA\t\n",
    "#X10 = ONI\n",
    "\n",
    "# Get the current working directory \n",
    "current_directory = os.getcwd() \n",
    "\n",
    "# Print the current working directory \n",
    "print(current_directory)\n",
    "\n",
    "# Define the directory containing the files \n",
    "path = current_directory+\"\\\\Modeling\\\\\"\n",
    "print(path)\n",
    "\n",
    "filename = path + 'Final_Monthly_Dataset.csv'\n",
    "\n",
    "# load dataset\n",
    "df = read_csv(filename, header=0, parse_dates=[0], index_col=0, date_format='%Y-%m')\n",
    "    \n",
    "df = df.rename(columns={'Unnamed: 0' : 'indices'})\n",
    "    \n",
    "#Remove unused columns\n",
    "df = df.drop(['Day', 'vapor_pressure'], axis=1)\n",
    "\n",
    "# Round numbers in columns to reasonable precision,\n",
    "df['temperatures'] = np.round(df['temperatures'], 2)\n",
    "df['slp'] = np.round(df['slp'], 2)\n",
    "df['wet_bulb_temperature'] = np.round(df['wet_bulb_temperature'], 2)\n",
    "df['specific_humidity'] = np.round(df['specific_humidity'], 2)\n",
    "df['GHI'] = np.round(df['GHI'], 2)\n",
    "df['PRCP'] = np.round(df['PRCP'], 2)\n",
    "df['SNDP'] = np.round(df['SNDP'], 2)\n",
    "df['solar_activity'] = np.round(df['solar_activity'], 2)\n",
    "df['ONI'] = np.round(df['ONI'], 2)\n",
    "df['water'] = np.round(df['water'], 0)\n",
    "df['region'] = np.round(df['region'], 0)\n",
    "    \n",
    "df_trimmed = df[df['file_id'] != 7533] # Remove file_id 7533 so there are 105 weather stations for 5-fold CV\n",
    "df_trimmed = df_trimmed.drop(['Year', 'Month', 'date', 'latitude', 'longitude', 'elevation'], axis=1)\n",
    "\n",
    "results_df = df_trimmed\n",
    "\n",
    "# Calculate principal components excluding response variables\n",
    "results_df['PC1']=-0.0004514*results_df['slp']+0.03194*results_df['wet_bulb_temperature']-0.04343*results_df['water']+0.002243*results_df['GHI']-0.02252*results_df['WDSP']\n",
    "+0.9877*results_df['PRCP']-0.2265*results_df['SNDP']+0.006144*results_df['region']-0.0001488*results_df['solar_activity']+0.02943*results_df['ONI']\n",
    "results_df['PC2']=0.0001702*results_df['slp']+0.005484*results_df['wet_bulb_temperature']+0.2057*results_df['water']-0.0003188*results_df['GHI']-0.02584*results_df['WDSP']\n",
    "+1.6963*results_df['PRCP']-0.05890*results_df['SNDP']+0.05809*results_df['region']+1.9748*results_df['solar_activity']+0.03686*results_df['ONI']\n",
    "results_df['PC3']=-0.00006323*results_df['slp']-0.001180*results_df['wet_bulb_temperature']-0.02384*results_df['water']-0.00002833*results_df['GHI']+0.01170*results_df['WDSP']\n",
    "+0.5204*results_df['PRCP']+0.4791*results_df['SNDP']-0.004318*results_df['region']+0.008271*results_df['solar_activity']+0.8765*results_df['ONI']\n",
    "\n",
    "# Drop columns that compose principal components\n",
    "results_df = results_df.drop(['slp', 'wet_bulb_temperature', 'water', 'GHI', 'WDSP', 'PRCP', 'SNDP', 'region', 'solar_activity', 'ONI'], axis=1)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "print(results_df.columns)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "    \n",
    "for i in array:\n",
    "    add_to_X = [] # create list to store each column to add to X\n",
    "    new_df = results_df[results_df['file_id'] == i].drop(['file_id'], axis=1)\n",
    "    add_to_y = []\n",
    "    for j in range(new_df.shape[0]):\n",
    "        add_to_y.append(new_df['temperatures'].iloc[j])\n",
    "    y.append(add_to_y)\n",
    "    #new_df = new_df.drop(['temperatures'], axis=1)\n",
    "    columns_list = new_df.columns.tolist()\n",
    "    for j in range(new_df.shape[0]):\n",
    "        l=0\n",
    "        new_row = []\n",
    "        for m in columns_list:\n",
    "            new_row.append(new_df.iloc[j, l])\n",
    "            l += 1\n",
    "        add_to_X.append(new_row)\n",
    "    X.append(add_to_X)\n",
    "    \n",
    "#Perform k-fold cross-validation\n",
    "#Taken from: https://www.geeksforgeeks.org/cross-validation-using-k-fold-with-scikit-learn/\n",
    "    \n",
    "#k = 5  # Number of folds\n",
    "#kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "#for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "#    print(f\"Fold {i}:\")\n",
    "#    print(f\"  Training dataset index: {train_index}\")\n",
    "#    print(f\"  Test dataset index: {test_index}\")\n",
    "    \n",
    "#for train_indices, test_indices in kf.split(X):\n",
    "#    print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    \n",
    "# Create figure and axis\n",
    "#fig, ax = plt.subplots(figsize=(6, 3))\n",
    "#plot_kfold(kf, X, y, ax, k)\n",
    "#plt.tight_layout()\n",
    "#fig.subplots_adjust(right=0.6)\n",
    "    \n",
    "#Create train and test sets for each cross-validation split\n",
    "train_X = []\n",
    "train_y = []\n",
    "val_X = []\n",
    "val_y = []\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}\")\n",
    "    #Add each corresponding sample for each entry of train index \n",
    "    train_X_rows = [] # Stores all the samples for one fold of train_X\n",
    "    train_y_rows = [] # Stores all the samples for one fold of train_y\n",
    "    for j in train_array[i]:\n",
    "        train_X_rows.append(X[j])\n",
    "        train_y_rows.append(y[j])\n",
    "    # Stores one fold of train dataset\n",
    "    train_X.append(train_X_rows)\n",
    "    train_y.append(train_y_rows)\n",
    "    #Add each corresponding sample for each entry of the validation index \n",
    "    val_X_rows = [] # Stores all the samples for one fold of val_X\n",
    "    val_y_rows = [] # Stores all the samples for one fold of val_y\n",
    "    for j in test_array[i]: \n",
    "            val_X_rows.append(X[j])\n",
    "            val_y_rows.append(y[j])\n",
    "    # Stores one fold of validation dataset\n",
    "    val_X.append(val_X_rows)\n",
    "    val_y.append(val_y_rows) \n",
    "\n",
    "#Convert 3D arrays to DataFrames\n",
    "df_X = []\n",
    "df_y = []\n",
    "val_df_X = []\n",
    "val_df_y = []\n",
    "dataset = []\n",
    "dataset_scaling = []\n",
    "dataset_test = []\n",
    "scaler = []\n",
    "scaler_all_features = []\n",
    "train = []\n",
    "test = []\n",
    "validation = []\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Fold \"+str(i+1)+\":\")\n",
    "    #Transform train_X to the correct format\n",
    "    df1 = []\n",
    "    dataset_df = [] # captures each weather station's dataset as values for training scaler mapping\n",
    "    df_X.append(pd.DataFrame(train_X[i]))\n",
    "    X_t = df_X[i].transpose()\n",
    "    for k in range(84):\n",
    "        X = np.array(X_t.iloc[:, k])\n",
    "        df = pd.DataFrame()\n",
    "        for j in range(n_time_steps):\n",
    "            new_row = pd.DataFrame(X[j]).transpose()\n",
    "            new_row.columns = new_df.columns\n",
    "            # Add the new row\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.columns = new_df.columns\n",
    "        df1.append(df)\n",
    "        dataset_df.append(df.values)\n",
    "    df_X[i] = df1\n",
    "    dataset.append(dataset_df)\n",
    "    dataset_scaling = list(itertools.chain(*dataset)) # Holds all 84 weather station rows to train the scaling function\n",
    "\n",
    "    #Transform train_y to the correct format\n",
    "    df2 = []\n",
    "    df_y.append(pd.DataFrame(train_y[i]))\n",
    "    y_t = df_y[i].transpose()\n",
    "    \n",
    "    for j in range(84):\n",
    "        y = np.array(y_t.iloc[:, j])\n",
    "        y = pd.DataFrame(y)\n",
    "        y.columns = ['temperatures']\n",
    "        df2.append(y)\n",
    "    df_y[i] = df2\n",
    "\n",
    "    #Transform val_X to the correct format\n",
    "    df3 = []\n",
    "    dataset_vl_df = [] # captures each weather station's dataset as values for training scaler mapping\n",
    "    val_df_X.append(pd.DataFrame(val_X[i]))\n",
    "    val_X_t = val_df_X[i].transpose()\n",
    "    for k in range(21):\n",
    "        vl_X = np.array(val_X_t.iloc[:, k])\n",
    "        vl_df = pd.DataFrame()\n",
    "        for j in range(n_time_steps):\n",
    "            new_row = pd.DataFrame(vl_X[j]).transpose()\n",
    "            new_row.columns = new_df.columns\n",
    "            # Add the new row\n",
    "            vl_df = pd.concat([vl_df, new_row], ignore_index=True)\n",
    "        vl_df.columns = new_df.columns\n",
    "        df3.append(vl_df)\n",
    "        dataset_vl_df.append(vl_df.values)\n",
    "    val_df_X[i] = df3\n",
    "    dataset_test.append(dataset_vl_df)    \n",
    "    dataset_testing = list(itertools.chain(*dataset_test)) # Holds remaining 21 weather station rows for testing and validation\n",
    "\n",
    "    #Transform val_y to the correct format\n",
    "    df4 = []\n",
    "    val_df_y.append(pd.DataFrame(train_y[i]))\n",
    "    val_y_t = val_df_y[i].transpose()\n",
    "    \n",
    "    for j in range(21):\n",
    "        v_y = np.array(val_y_t.iloc[:, j])\n",
    "        v_y = pd.DataFrame(v_y)\n",
    "        v_y.columns = ['temperatures']\n",
    "        df4.append(v_y)\n",
    "    val_df_y[i] = df4\n",
    "    \n",
    "    scaler.append(MinMaxScaler(feature_range=(0, 1)))\n",
    "    scaler_all_features.append(MinMaxScaler(feature_range=(0, 1)))\n",
    "    train.append([1])\n",
    "    test.append([1])\n",
    "    validation.append([1])\n",
    "    \n",
    "    # prepare data\n",
    "    scaler[i], scaler_all_features[i], train[i] = prepare_training_data(dataset_scaling, n_lag, n_seq, n_time_steps)\n",
    "\n",
    "    validation[i], test[i] = prepare_testing_and_validation_data(dataset_testing, n_lag, n_seq, n_time_steps, scaler_all_features[i])\n",
    "\n",
    "    #Reshape dimensionality\n",
    "    train1 = train[i]\n",
    "    test1 = test[i]\n",
    "    validation1 = validation[i]\n",
    "    print(np.array(train1).shape)\n",
    "    print(np.array(test1).shape)\n",
    "    print(np.array(validation1).shape)\n",
    "    train2 = []\n",
    "    test2 = []\n",
    "    validation2 = []\n",
    "\n",
    "    for k in range(84):\n",
    "        train2.append(train1[k][0])\n",
    "        \n",
    "    for k in range(11):\n",
    "        test2.append(test1[k][0])\n",
    "\n",
    "    for k in range(10):\n",
    "        validation2.append(validation1[k][0])\n",
    "\n",
    "    print(np.array(train2).shape)\n",
    "    print(np.array(test2).shape)\n",
    "    print(np.array(validation2).shape)\n",
    "\n",
    "    train[i] = train2\n",
    "    test[i] = test2\n",
    "    validation[i] = validation2\n",
    "\n",
    "    #Reshape dimensionality (again)\n",
    "    dim_size = n_seq + 12*n_lag\n",
    "    train1 = np.array(train[i]).reshape(84, dim_size)\n",
    "    test1 = np.array(test[i]).reshape(11, dim_size)\n",
    "    validation1 = np.array(validation[i]).reshape(10, dim_size)\n",
    "    train2 = pd.DataFrame(train1).values\n",
    "    test2 = pd.DataFrame(test1).values\n",
    "    validation2 = pd.DataFrame(validation1).values\n",
    "    print(np.array(train2).shape)\n",
    "    print(np.array(test2).shape)\n",
    "    print(np.array(validation2).shape)\n",
    "\n",
    "    print(train2)\n",
    "\n",
    "    train[i] = train2\n",
    "    test[i] = test2\n",
    "    validation[i] = validation2\n",
    "    \n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0f7eb-c53b-4001-8e9d-eba947a0697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full_df = results_df #grab full dataset before removing last 4 years\n",
    "\n",
    "results_df = results_df[pd.to_datetime(results_df['date']) < datetime.strptime(\"2021-1-1\", \"%Y-%m-%d\")]\n",
    "\n",
    "# Get test dataframe\n",
    "results_test_df = results_full_df\n",
    "results_test_df = results_test_df[pd.to_datetime(results_test_df['date']) > datetime.strptime(\"2020-12-31\", \"%Y-%m-%d\")]\n",
    "results_test_df = results_test_df.reset_index()\n",
    "\n",
    "#Get one weather station\n",
    "results_df = results_df[results_df['file_id'] == 6501]\n",
    "results_test_df = results_test_df[results_test_df['file_id'] == 6501]\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# Get test values for specific humidity and temperature\n",
    "specific_humidity_test_values = results_test_df['specific_humidity']\n",
    "temperature_test_values = results_test_df['temperatures']\n",
    "\n",
    "print(specific_humidity_test_values)\n",
    "print(temperature_test_values)\n",
    "\n",
    "# define a series for each column in the data frame that needs to be normalized\n",
    "# Normalized columns: temperatures, specific_humidity, slp, water, region, wet_bulb_temperature,\n",
    "# GHI, SNDP, latitude, longitude, elevation, Year, Month, Day, solar_activity, ONI\n",
    "\n",
    "data = []\n",
    "scaler = []\n",
    "scaled_data = []\n",
    "\n",
    "# Get Data\n",
    "data.append(results_df['temperatures'])\n",
    "data.append(results_df['specific_humidity'])\n",
    "data.append(results_df['PC1'])\n",
    "data.append(results_df['PC2'])\n",
    "data.append(results_df['PC3'])\n",
    "\n",
    "#data.append(results_df['slp'])\n",
    "#data.append(results_df['water'])\n",
    "#data.append(results_df['region'])\n",
    "#data.append(results_df['wet_bulb_temperature'])\n",
    "#data.append(results_df['GHI'])\n",
    "#data.append(results_df['SNDP'])\n",
    "#data.append(results_df['solar_activity'])\n",
    "#data.append(results_df['ONI'])\n",
    "#data.append(results_df['latitude'])\n",
    "#data.append(results_df['longitude'])\n",
    "#data.append(results_df['elevation'])\n",
    "#data.append(results_df['Year'])\n",
    "#data.append(results_df['Month'])\n",
    "#data.append(results_df['Day'])\n",
    "\n",
    "n_vars = 5 # Number of variables/predictors\n",
    "\n",
    "# Data Preparation: Scale Data\n",
    "for i in range(n_vars):\n",
    "    scaler.append(StandardScaler())\n",
    "    scaled_data.append([0])    \n",
    "    \n",
    "scaler, scaler_all_features, scaled_data = prepare_data(data)\n",
    "\n",
    "dates = pd.date_range(start='2006-01-01', periods=180, freq='ME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70956c2e-a3d3-43de-990f-ff0d4a96de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA model (for ARIMA model, all the SARIMAX model seasonal parameters are zero)\n",
    "\n",
    "# Predict temperature\n",
    "\n",
    "print(\"ARIMA Temperature Predictions:\")\n",
    "\n",
    "y = scaled_data[0] # y = temperature\n",
    "\n",
    "for i in range(n_vars-1):\n",
    "    j=i+1\n",
    "    exog.append(scaled_data[j])\n",
    "\n",
    "# Reconstruct the data frame with standardized values\n",
    "data = pd.DataFrame({'y': y}, index=dates)\n",
    "\n",
    "for i in range(n_vars-1):\n",
    "    data['temp_col'] = exog[i]\n",
    "    colname = 'exog' + str(i+1)\n",
    "    data.rename(columns={'temp_col': colname}, inplace=True)\n",
    "\n",
    "print(\"ARIMA Temperature Dataframe:\")\n",
    "print(data)\n",
    "\n",
    "# Fit the ARIMA model\n",
    "model_temp_ARMA = SARIMAX(data['y'], exog=None, order=(1, 0, 1), seasonal_order=(0, 0, 0, 0))\n",
    "results_temp_ARMA = model_temp_ARMA.fit(disp=False)\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results_temp_ARMA.summary())\n",
    "\n",
    "# Forecasting\n",
    "n_forecast = 47\n",
    "forecast_temp_ARMA = results_temp_ARMA.get_forecast(steps=n_forecast, exog=exog[-n_forecast:])\n",
    "forecast_temp_ARMA_mean = forecast_temp_ARMA.predicted_mean\n",
    "forecast_temp_ARMA_ci = forecast_temp_ARMA.conf_int()\n",
    "\n",
    "# Print the forecasted values\n",
    "print(forecast_temp_ARMA_mean)\n",
    "print(forecast_temp_ARMA_ci)\n",
    "\n",
    "# Reshape data\n",
    "forecast_temp_ARMA_mean = np.array(forecast_temp_ARMA_mean)\n",
    "forecast_temp_ARMA_mean = forecast_temp_ARMA_mean.reshape(-1,1)\n",
    "\n",
    "# Inverse transform and print forecast\n",
    "inversed_temp_ARMA_mean = scaler1.inverse_transform(forecast_temp_ARMA_mean)\n",
    "inversed_temp_ARMA_ci = scaler1.inverse_transform(forecast_temp_ARMA_ci)\n",
    "print(inversed_temp_ARMA_mean)\n",
    "print(inversed_temp_ARMA_ci)\n",
    "print(temperature_test_values)\n",
    "\n",
    "dates_predicted = pd.date_range(start='2021-01-01', periods=47, freq='ME')\n",
    "\n",
    "combined_temp_ARMA = []\n",
    "for i in range(len(temperature_test_values)):\n",
    "    combined_temp_ARMA.append([dates_predicted[i], inversed_temp_ARMA_mean[i, 0], temperature_test_values[i]])\n",
    "\n",
    "combined_temp_ARMA = pd.DataFrame(combined_temp_ARMA)\n",
    "combined_temp_ARMA.columns = ['prediction_date', 'predicted_temp', 'actual_temp']\n",
    "\n",
    "combined_temp_ARMA['error_pct'] = 100 * (combined_temp_ARMA['actual_temp'] - combined_temp_ARMA['predicted_temp'])/combined_temp_ARMA['actual_temp']\n",
    "\n",
    "# Set display option to show all rows\n",
    "pd.set_option('display.max_rows', 47)\n",
    "\n",
    "print(combined_temp_ARMA.head(47))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b161e8-333d-434a-b5be-12292106c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMAX model\n",
    "\n",
    "print(\"SARIMAX Temperature Predictions:\")\n",
    "\n",
    "# Fit the SARIMAX model\n",
    "model_temp_SARIMAX = SARIMAX(data['y'], exog=exog, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "results_temp_SARIMAX = model_temp_SARIMAX.fit(disp=False)\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results_temp_SARIMAX.summary())\n",
    "\n",
    "# Forecasting\n",
    "n_forecast = 47\n",
    "forecast_temp_SARIMAX = results_temp_SARIMAX.get_forecast(steps=n_forecast, exog=exog[-n_forecast:])\n",
    "forecast_temp_SARIMAX_mean = forecast_temp_SARIMAX.predicted_mean\n",
    "forecast_temp_SARIMAX_ci = forecast_temp_SARIMAX.conf_int()\n",
    "\n",
    "# Print the forecasted values\n",
    "print(forecast_temp_SARIMAX_mean)\n",
    "print(forecast_temp_SARIMAX_ci)\n",
    "\n",
    "# Reshape data\n",
    "forecast_temp_SARIMAX_mean = np.array(forecast_temp_SARIMAX_mean)\n",
    "forecast_temp_SARIMAX_mean = forecast_temp_SARIMAX_mean.reshape(-1,1)\n",
    "\n",
    "# Inverse transform and print forecast\n",
    "inversed_temp_SARIMAX_mean = scaler1.inverse_transform(forecast_temp_SARIMAX_mean)\n",
    "inversed_temp_SARIMAX_ci = scaler1.inverse_transform(forecast_temp_SARIMAX_ci)\n",
    "print(inversed_temp_SARIMAX_mean)\n",
    "print(inversed_temp_SARIMAX_ci)\n",
    "print(temperature_test_values)\n",
    "\n",
    "dates_predicted = pd.date_range(start='2021-01-01', periods=47, freq='ME')\n",
    "\n",
    "combined_temp_SARIMAX = []\n",
    "for i in range(len(temperature_test_values)):\n",
    "    combined_temp_SARIMAX.append([dates_predicted[i], inversed_temp_SARIMAX_mean[i, 0], temperature_test_values[i]])\n",
    "\n",
    "combined_temp_SARIMAX = pd.DataFrame(combined_temp_SARIMAX)\n",
    "combined_temp_SARIMAX.columns = ['prediction_date', 'predicted_temp', 'actual_temp']\n",
    "\n",
    "combined_temp_SARIMAX['error_pct'] = 100 * (combined_temp_SARIMAX['actual_temp'] - combined_temp_SARIMAX['predicted_temp'])/combined_temp_SARIMAX['actual_temp']\n",
    "\n",
    "# Set display option to show all rows\n",
    "pd.set_option('display.max_rows', 47)\n",
    "\n",
    "print(combined_temp_SARIMAX.head(47))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3ca1b-afcf-48f4-9cba-6f48643a41b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict specific humidity\n",
    "\n",
    "# ARIMA model\n",
    "\n",
    "print(\"ARIMA Specific Humidity Predictions:\")\n",
    "\n",
    "# Swap temperature and specific humidity in the dataframe\n",
    "data['y'] = pd.DataFrame(scaled_data[1]) # y = specific humidity\n",
    "data['exog1'] = pd.DataFrame(scaled_data[0]) # exog1 = temperature\n",
    "\n",
    "# Define the exogenous variables\n",
    "exog = (data.drop(['y'])).values\n",
    "\n",
    "print(exog)\n",
    "\n",
    "# Fit the SARIMAX model\n",
    "model_sh_ARMA = SARIMAX(data['y'], exog=None, order=(1, 0, 1), seasonal_order=(0, 0, 0, 0))\n",
    "results_sh_ARMA = model_sh_ARMA.fit(disp=False)\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results_sh_ARMA.summary())\n",
    "\n",
    "# Forecasting\n",
    "n_forecast = 47\n",
    "forecast_sh_ARMA = results_sh_ARMA.get_forecast(steps=n_forecast, exog=exog[-n_forecast:])\n",
    "forecast_sh_ARMA_mean = forecast_sh_ARMA.predicted_mean\n",
    "forecast_sh_ARMA_ci = forecast_sh_ARMA.conf_int()\n",
    "\n",
    "# Print the forecasted values\n",
    "print(forecast_sh_ARMA_mean)\n",
    "print(forecast_sh_ARMA_ci)\n",
    "\n",
    "# Reshape data\n",
    "forecast_sh_ARMA_mean = np.array(forecast_sh_ARMA_mean)\n",
    "forecast_sh_ARMA_mean = forecast_sh_ARMA_mean.reshape(-1,1)\n",
    "\n",
    "# Inverse transform and print forecast\n",
    "inversed_sh_ARMA_mean = scaler1.inverse_transform(forecast_sh_ARMA_mean)\n",
    "inversed_sh_ARMA_ci = scaler1.inverse_transform(forecast_sh_ARMA_ci)\n",
    "print(inversed_sh_ARMA_mean)\n",
    "print(inversed_sh_ARMA_ci)\n",
    "print(specific_humidity_test_values)\n",
    "\n",
    "dates_predicted = pd.date_range(start='2021-01-01', periods=47, freq='ME')\n",
    "\n",
    "combined_sh_ARMA = []\n",
    "for i in range(len(specific_humidity_test_values)):\n",
    "    combined_sh_ARMA.append([dates_predicted[i], inversed_sh_ARMA_mean[i, 0], specific_humidity_test_values[i]])\n",
    "\n",
    "combined_sh_ARMA = pd.DataFrame(combined_sh_ARMA)\n",
    "combined_sh_ARMA.columns = ['prediction_date', 'predicted_sh', 'actual_sh']\n",
    "\n",
    "combined_sh_ARMA['error_pct'] = 100 * (combined_sh_ARMA['actual_sh'] - combined_sh_ARMA['predicted_sh'])/combined_sh_ARMA['actual_sh']\n",
    "\n",
    "# Set display option to show all rows\n",
    "pd.set_option('display.max_rows', 47)\n",
    "\n",
    "print(combined_sh_ARMA.head(47))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a5dd5-cef4-46ea-a2f2-cab166ddaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMAX model\n",
    "\n",
    "print(\"SARIMAX Specific Humidity Predictions:\")\n",
    "\n",
    "# Fit the SARIMAX model\n",
    "model_sh_SARIMAX = SARIMAX(data['y'], exog=exog, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "results_sh_SARIMAX = model_sh_SARIMAX.fit(disp=False)\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results_sh_SARIMAX.summary())\n",
    "\n",
    "# Forecasting\n",
    "n_forecast = 47\n",
    "forecast_sh_SARIMAX = results_sh_SARIMAX.get_forecast(steps=n_forecast, exog=exog[-n_forecast:])\n",
    "forecast_sh_SARIMAX_mean = forecast_sh_SARIMAX.predicted_mean\n",
    "forecast_sh_SARIMAX_ci = forecast_sh_SARIMAX.conf_int()\n",
    "\n",
    "# Print the forecasted values\n",
    "print(forecast_sh_SARIMAX_mean)\n",
    "print(forecast_sh_SARIMAX_ci)\n",
    "\n",
    "# Reshape data\n",
    "forecast_sh_SARIMAX_mean = np.array(forecast_sh_SARIMAX_mean)\n",
    "forecast_sh_SARIMAX_mean = forecast_sh_SARIMAX_mean.reshape(-1,1)\n",
    "\n",
    "# Inverse transform and print forecast\n",
    "inversed_sh_SARIMAX_mean = scaler1.inverse_transform(forecast_sh_SARIMAX_mean)\n",
    "inversed_sh_SARIMAX_ci = scaler1.inverse_transform(forecast_sh_SARIMAX_ci)\n",
    "print(inversed_sh_SARIMAX_mean)\n",
    "print(inversed_sh_SARIMAX_ci)\n",
    "print(specific_humidity_test_values)\n",
    "\n",
    "dates_predicted = pd.date_range(start='2021-01-01', periods=47, freq='ME')\n",
    "\n",
    "combined_sh_SARIMAX = []\n",
    "for i in range(len(specific_humidity_test_values)):\n",
    "    combined_sh_SARIMAX.append([dates_predicted[i], inversed_sh_SARIMAX_mean[i, 0], specific_humidity_test_values[i]])\n",
    "\n",
    "combined_sh_SARIMAX = pd.DataFrame(combined_sh_SARIMAX)\n",
    "combined_sh_SARIMAX.columns = ['prediction_date', 'predicted_sh', 'actual_sh']\n",
    "\n",
    "combined_sh_SARIMAX['error_pct'] = 100 * (combined_sh_SARIMAX['actual_sh'] - combined_sh_SARIMAX['predicted_sh'])/combined_sh_SARIMAX['actual_sh']\n",
    "\n",
    "# Set display option to show all rows\n",
    "pd.set_option('display.max_rows', 47)\n",
    "\n",
    "print(combined_sh_SARIMAX.head(47))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d4b0c-6ca3-4604-a0e4-84d081cfe77c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
