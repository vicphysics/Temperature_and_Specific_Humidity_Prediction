{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5baac998-16fc-4132-83cf-437ab3206a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "n_lag (number of input time steps): 22\n",
      "n_seq (number of output/future prediction time steps): 1\n",
      "C:\\Users\\User\n",
      "C:\\Users\\User\\Modeling\\\n",
      "     file_id        date  specific_humidity          slp  vapor_pressure  \\\n",
      "0       6678  1998-12-31          12.816968  1016.202508       20.726068   \n",
      "1       6678  1999-12-31          12.330412  1017.480181       19.960131   \n",
      "2       6678  2000-12-31          12.136613  1018.259826       19.655271   \n",
      "3       6678  2001-12-31          11.897962  1018.414173       19.285320   \n",
      "4       6678  2002-12-31          12.751085  1017.916636       20.638060   \n",
      "..       ...         ...                ...          ...             ...   \n",
      "643     8000  2020-12-31           5.146134  1016.944161        8.168785   \n",
      "644     8000  2021-12-31           5.239193  1016.159983        8.312383   \n",
      "645     8000  2022-12-31           5.362553  1017.898453        8.512894   \n",
      "646     8000  2023-12-31           5.430746  1016.166709        8.612808   \n",
      "647     8000  2024-12-31           5.324428  1015.116823        8.440246   \n",
      "\n",
      "     wet_bulb_temperature  temperatures  water         GHI      WDSP  \\\n",
      "0               18.084148     20.844628    1.0  217.311494  6.212756   \n",
      "1               17.712631     20.694956    1.0  222.891338  5.525413   \n",
      "2               17.290597     20.472813    1.0  228.091193  5.557681   \n",
      "3               17.198062     20.303551    1.0  221.557765  6.402008   \n",
      "4               17.752707     20.461064    1.0  217.380397  6.154090   \n",
      "..                    ...           ...    ...         ...       ...   \n",
      "643              7.635774     13.015177    1.0  184.806420  7.025180   \n",
      "644              7.876378     13.466771    1.0         NaN  6.802803   \n",
      "645              7.127421     11.838611    1.0         NaN  5.979051   \n",
      "646              7.656528     12.861839    1.0         NaN  5.971951   \n",
      "647              8.109050     13.580201    1.0         NaN  6.623975   \n",
      "\n",
      "         PRCP      SNDP  latitude  longitude  elevation  region    Year  \\\n",
      "0    0.117309  0.000000    30.393    -86.467        5.4     6.0  1998.0   \n",
      "1    0.075740  0.047043    30.393    -86.467        5.4     6.0  1999.0   \n",
      "2    0.092590  0.000000    30.393    -86.467        5.4     6.0  2000.0   \n",
      "3    0.085106  0.000000    30.393    -86.467        5.4     6.0  2001.0   \n",
      "4    0.155374  0.000000    30.393    -86.467        5.4     6.0  2002.0   \n",
      "..        ...       ...       ...        ...        ...     ...     ...   \n",
      "643  0.011634  0.000000    45.826   -119.261      195.1     1.0  2020.0   \n",
      "644  0.018381  0.000000    45.826   -119.261      195.1     1.0  2021.0   \n",
      "645  0.033357  0.000000    45.826   -119.261      195.1     1.0  2022.0   \n",
      "646  0.020517  0.000000    45.826   -119.261      195.1     1.0  2023.0   \n",
      "647  0.025229  0.000000    45.826   -119.261      195.1     1.0  2024.0   \n",
      "\n",
      "     solar_activity       ONI  \n",
      "0         88.051370 -0.055000  \n",
      "1        136.154122 -1.224167  \n",
      "2        173.809078 -0.845000  \n",
      "3        170.276235 -0.310833  \n",
      "4        163.605562  0.660000  \n",
      "..              ...       ...  \n",
      "643        8.797259 -0.361667  \n",
      "644       29.468216 -0.721667  \n",
      "645       83.036034 -0.939167  \n",
      "646      125.331426  0.822500  \n",
      "647      154.371939  0.445455  \n",
      "\n",
      "[648 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_417992\\1118364967.py:409: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['solar_activity'].loc[df['Year'] > 2020] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     file_id  specific_humidity      slp  wet_bulb_temperature  temperatures  \\\n",
      "0       6678              12.82  1016.20                 18.08         20.84   \n",
      "1       6678              12.33  1017.48                 17.71         20.69   \n",
      "2       6678              12.14  1018.26                 17.29         20.47   \n",
      "3       6678              11.90  1018.41                 17.20         20.30   \n",
      "4       6678              12.75  1017.92                 17.75         20.46   \n",
      "..       ...                ...      ...                   ...           ...   \n",
      "643     8000               5.15  1016.94                  7.64         13.02   \n",
      "644     8000               5.24  1016.16                  7.88         13.47   \n",
      "645     8000               5.36  1017.90                  7.13         11.84   \n",
      "646     8000               5.43  1016.17                  7.66         12.86   \n",
      "647     8000               5.32  1015.12                  8.11         13.58   \n",
      "\n",
      "     water      WDSP  PRCP  SNDP  region  solar_activity   ONI  \n",
      "0      1.0  6.212756  0.12  0.00     6.0           88.05 -0.05  \n",
      "1      1.0  5.525413  0.08  0.05     6.0          136.15 -1.22  \n",
      "2      1.0  5.557681  0.09  0.00     6.0          173.81 -0.85  \n",
      "3      1.0  6.402008  0.09  0.00     6.0          170.28 -0.31  \n",
      "4      1.0  6.154090  0.16  0.00     6.0          163.61  0.66  \n",
      "..     ...       ...   ...   ...     ...             ...   ...  \n",
      "643    1.0  7.025180  0.01  0.00     1.0            8.80 -0.36  \n",
      "644    1.0  6.802803  0.02  0.00     1.0             NaN -0.72  \n",
      "645    1.0  5.979051  0.03  0.00     1.0             NaN -0.94  \n",
      "646    1.0  5.971951  0.02  0.00     1.0             NaN  0.82  \n",
      "647    1.0  6.623975  0.03  0.00     1.0             NaN  0.45  \n",
      "\n",
      "[648 rows x 12 columns]\n",
      "Empty DataFrame\n",
      "Columns: [specific_humidity, slp, wet_bulb_temperature, temperatures, water, WDSP, PRCP, SNDP, region, solar_activity, ONI]\n",
      "Index: []\n",
      "[]\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 1:\n",
      "11\n",
      "19\n",
      "<class 'list'>\n",
      "11\n",
      "5\n",
      "<class 'list'>\n",
      "(19, 1, 1, 243)\n",
      "(2, 1, 1, 243)\n",
      "(3, 1, 1, 243)\n",
      "(19, 1, 243)\n",
      "(2, 1, 243)\n",
      "(3, 1, 243)\n",
      "(19, 243)\n",
      "(2, 243)\n",
      "(3, 243)\n",
      "[[0.25167038 0.46007605 0.35622318 ... 0.57954168 0.11262799 0.27616927]\n",
      " [0.25389755 0.46007605 0.38769671 ... 0.57954168 0.11262799 0.33407572]\n",
      " [0.06681514 0.5095057  0.17310443 ... 0.57954168 0.11262799 0.50334076]\n",
      " ...\n",
      " [0.33407572 0.40969582 0.32474964 ... 0.57954168 0.11262799 0.48775056]\n",
      " [0.22271715 0.81939163 0.16452074 ... 0.57954168 0.11262799 0.33184855]\n",
      " [0.18262806 1.         0.25751073 ... 0.57954168 0.11262799 0.4454343 ]]\n",
      "Fold 2:\n",
      "11\n",
      "19\n",
      "<class 'list'>\n",
      "11\n",
      "5\n",
      "<class 'list'>\n",
      "(19, 1, 1, 243)\n",
      "(2, 1, 1, 243)\n",
      "(3, 1, 1, 243)\n",
      "(19, 1, 243)\n",
      "(2, 1, 243)\n",
      "(3, 1, 243)\n",
      "(19, 243)\n",
      "(2, 243)\n",
      "(3, 243)\n",
      "[[0.33115468 0.44771863 0.50704225 ... 0.57954168 0.11262799 0.39651416]\n",
      " [0.27015251 0.46007605 0.45198464 ... 0.57954168 0.11262799 0.34858388]\n",
      " [0.24183007 0.45057034 0.41997439 ... 0.57954168 0.11262799 0.34204793]\n",
      " ...\n",
      " [0.23965142 0.81939163 0.25224072 ... 0.57954168 0.11262799 0.34640523]\n",
      " [0.32679739 0.57889734 0.45326504 ... 0.57954168 0.11262799 0.4335512 ]\n",
      " [0.20043573 1.         0.33546735 ... 0.57954168 0.11262799 0.45751634]]\n",
      "Fold 3:\n",
      "11\n",
      "19\n",
      "<class 'list'>\n",
      "11\n",
      "5\n",
      "<class 'list'>\n",
      "(19, 1, 1, 243)\n",
      "(2, 1, 1, 243)\n",
      "(3, 1, 1, 243)\n",
      "(19, 1, 243)\n",
      "(2, 1, 243)\n",
      "(3, 1, 243)\n",
      "(19, 243)\n",
      "(2, 243)\n",
      "(3, 243)\n",
      "[[0.33115468 0.40895219 0.50704225 ... 0.57954168 0.11262799 0.39651416]\n",
      " [0.26797386 0.42217701 0.42381562 ... 0.57954168 0.11262799 0.291939  ]\n",
      " [0.24183007 0.41200407 0.41997439 ... 0.57954168 0.11262799 0.34204793]\n",
      " ...\n",
      " [0.23965142 0.80671414 0.25224072 ... 0.57954168 0.11262799 0.34640523]\n",
      " [0.32679739 0.54933876 0.45326504 ... 0.57954168 0.11262799 0.4335512 ]\n",
      " [0.20043573 1.         0.33546735 ... 0.57954168 0.11262799 0.45751634]]\n",
      "Fold 4:\n",
      "11\n",
      "20\n",
      "<class 'list'>\n",
      "11\n",
      "4\n",
      "<class 'list'>\n",
      "(20, 1, 1, 243)\n",
      "(2, 1, 1, 243)\n",
      "(2, 1, 1, 243)\n",
      "(20, 1, 243)\n",
      "(2, 1, 243)\n",
      "(2, 1, 243)\n",
      "(20, 243)\n",
      "(2, 243)\n",
      "(2, 243)\n",
      "[[0.45783133 0.44771863 0.64285714 ... 0.57954168 0.11262799 0.54819277]\n",
      " [0.37048193 0.46007605 0.53733766 ... 0.57954168 0.11262799 0.40361446]\n",
      " [0.37349398 0.46007605 0.57305195 ... 0.57954168 0.11262799 0.48192771]\n",
      " ...\n",
      " [0.3313253  0.81939163 0.31980519 ... 0.57954168 0.11262799 0.47891566]\n",
      " [0.45180723 0.57889734 0.57467532 ... 0.57954168 0.11262799 0.59939759]\n",
      " [0.27710843 1.         0.42532468 ... 0.57954168 0.11262799 0.63253012]]\n",
      "Fold 5:\n",
      "11\n",
      "19\n",
      "<class 'list'>\n",
      "11\n",
      "5\n",
      "<class 'list'>\n",
      "(19, 1, 1, 243)\n",
      "(2, 1, 1, 243)\n",
      "(3, 1, 1, 243)\n",
      "(19, 1, 243)\n",
      "(2, 1, 243)\n",
      "(3, 1, 243)\n",
      "(19, 243)\n",
      "(2, 243)\n",
      "(3, 243)\n",
      "[[0.33115468 0.7536     0.50704225 ... 0.57954168 0.11262799 0.39651416]\n",
      " [0.26797386 0.7744     0.42381562 ... 0.57954168 0.11262799 0.291939  ]\n",
      " [0.27015251 0.7744     0.45198464 ... 0.57954168 0.11262799 0.34858388]\n",
      " ...\n",
      " [0.35947712 0.6912     0.42381562 ... 0.57954168 0.11262799 0.50762527]\n",
      " [0.34858388 0.6896     0.39564661 ... 0.57954168 0.11262799 0.49891068]\n",
      " [0.32679739 0.9744     0.45326504 ... 0.57954168 0.11262799 0.4335512 ]]\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Activation, Dropout\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from numpy import array\n",
    "import keras.backend as K\n",
    "import itertools\n",
    "#!pip install pydot\n",
    "    \n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "    \n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    \n",
    "def mape (y_true, y_pred):\n",
    "    return 100*K.mean(K.sqrt(K.square(y_true - y_pred))/y_true)\n",
    "    \n",
    "def pearson (y_true, y_pred):\n",
    "    return (K.square(K.mean((y_true - K.mean(y_true))*(y_pred - K.mean(y_pred)))))/(K.mean(K.square(y_true - K.mean(y_true)))*K.mean(K.square(y_pred - K.mean(y_pred))))\n",
    "    \n",
    "# convert time series into a supervised learning problem\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "# Convert time series into a supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    cols, names = list(), list()\n",
    "    df = DataFrame(data)\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "    \tcols.append(df.shift(i))\n",
    "    \tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df[0].shift(-i)) # df[0] for specific humidity\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (1))] # % (1) for specific humidity\n",
    "        else:            \n",
    "            names += [('var%d(t+%d)' % (1, i))] # % (1) for specific humidity\n",
    "    \n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "\n",
    "    if dropnan:\n",
    "        #Drop rows containing NaN\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    #print(\"Agg:\")\n",
    "    agg.columns = names\n",
    "    #print(type(agg))\n",
    "    #print(agg)\n",
    "\n",
    "    #print(\"Test columns:\")\n",
    "    #print(agg.iloc[:, -37]) # Column containing response actual values (specific humidity) at time t-3\n",
    "    #print(agg.iloc[:, -25]) # Column containing response actual values (specific humidity) at time t-2\n",
    "    #print(agg.iloc[:, -13]) # Column containing response actual values (specific humidity) at time t-1\n",
    "    #print(agg.iloc[:, -1]) # Column containing response actual values (specific humidity) at time t\n",
    "    \n",
    "    return agg\n",
    "     \n",
    "# create a differenced series\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    "     \n",
    "# transform series into training sets for supervised learning\n",
    "def prepare_training_data(data, n_lag, n_seq, n_time_steps):\n",
    "    \n",
    "    #Prepare data for time series forecasting.\n",
    "        \n",
    "    #Parameters:\n",
    "    #x (array-like): Input features.\n",
    "    #y (array-like): Target values.\n",
    "    #n_test (int): Number of test samples (rows).\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations.\n",
    "    #n_train (int): Number of training samples (rows).\n",
    "        \n",
    "    #Returns:\n",
    "    #tuple: Training and test datasets.\n",
    "    \n",
    "    n_vars = len(data[0][0])\n",
    "\n",
    "    print(n_vars)\n",
    "    print(len(data))\n",
    "    print(type(data))\n",
    "\n",
    "    # Each weather station has 27 time steps (the first 23 have no nan values)\n",
    "    # Loop through data, grabbing one weather station (ws) at a time, \n",
    "    # differencing on each ws and separating by training (first 26-n_lag-n_seq-n_test time steps) \n",
    "    # and testing (n_test time steps) to scale data on training only.\n",
    "    # We then recombine the training and testing datasets to change each ws to a supervised learning problem by taking all the first 23 time steps for all 12 predictors\n",
    "    # and changing these to (t-n_lag) to (t-1) since we lose one row through differencing. We then shift forward only one dependent variable (temperature or specific humidity)\n",
    "    # for time steps t to (t+n_seq)\n",
    "\n",
    "\n",
    "    diff_values = []\n",
    "    \n",
    "    for ws in range(len(data)):\n",
    "        \n",
    "        # transform data to be stationary\n",
    "        diff_series = difference(data[ws], 1)\n",
    "        for i in range(len(diff_series)):\n",
    "            diff_values_row = []\n",
    "            for j in range(len(diff_series[0])):\n",
    "                diff_values_row.append(diff_series[i][j])\n",
    "            diff_values.append(diff_values_row)\n",
    "\n",
    "    #print(\"Diff values:\")\n",
    "    #print(len(diff_values))\n",
    "    #print(len(diff_values[0]))\n",
    "    #print(len(diff_values_for_training))\n",
    "    #print(len(diff_values_for_training[0]))\n",
    "    \n",
    "    # rescale values to 0, 1\n",
    "    scaler_all_features =  MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler =  MinMaxScaler(feature_range=(0, 1))\n",
    "    #diff_values_training = np.array(diff_values_for_training)\n",
    "    #diff_values_training = diff_values_training.reshape(1, -1) \n",
    "    train_scaled_values = scaler_all_features.fit_transform(diff_values)\n",
    "    response_train_values = []\n",
    "    for i in range(len(diff_values)):\n",
    "        response_train_values.append(diff_values[i][0]) # Uses first column (specific_humidity) as response variable\n",
    "    response_train_values = np.array(response_train_values)\n",
    "    response_train_values = response_train_values.reshape(len(response_train_values), 1)\n",
    "\n",
    "    # Fit the scaler for just the response variable for use later when forecasting\n",
    "    response_scaled_values = scaler.fit_transform(response_train_values) \n",
    "    scaled_values = scaler_all_features.transform(diff_values)\n",
    "\n",
    "    #print(\"Scaled values rows:\")\n",
    "    #print(len(scaled_values))\n",
    "\n",
    "    train = []\n",
    "\n",
    "    # Transform each weather station as a separate \"batch\"\n",
    "    for ws in range(len(data)):\n",
    "        # transform into a supervised learning problem X, y\n",
    "        first = (n_time_steps-1)*ws\n",
    "        last = (n_time_steps-1)*ws+(n_time_steps-2)\n",
    "        #print(\"Batch \"+str(ws+1)+\":\")\n",
    "        #print(\"Range: \"+str(first)+\"-\"+str(last))\n",
    "        scaled_values_batch = scaled_values[first:last]\n",
    "        supervised = series_to_supervised(scaled_values_batch, n_lag, n_seq)\n",
    "        supervised_values = supervised.values\n",
    "        train.append([supervised_values])\n",
    "        #print(\"Supervised count:\")\n",
    "        #print(len(supervised_values))\n",
    "        #print(len(supervised_values[0]))\n",
    "    \n",
    "    return scaler, scaler_all_features, train\n",
    "\n",
    "# transform series into testing and validation sets for supervised learning\n",
    "def prepare_testing_and_validation_data(data, n_lag, n_seq, n_time_steps, scaler_all_features):\n",
    "    \n",
    "    # Prepare data for time series forecasting.\n",
    "        \n",
    "    #Parameters:\n",
    "    #x (array-like): Input features.\n",
    "    #y (array-like): Target values.\n",
    "    #n_test (int): Number of test samples (rows).\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations.\n",
    "    #n_train (int): Number of training samples (rows).\n",
    "        \n",
    "    #Returns:\n",
    "    #tuple: Training and test datasets.\n",
    "    \n",
    "    n_vars = len(data[0][0])\n",
    "\n",
    "    print(n_vars)\n",
    "    print(len(data))\n",
    "    print(type(data))\n",
    "\n",
    "    # Each weather station has 227 time steps (the first 180 have no nan values)\n",
    "    # Loop through data, grabbing one weather station (ws) at a time, \n",
    "    # differencing on each ws and separating by training (first 226-n_lag-n_seq-n_test time steps) \n",
    "    # and testing (n_test time steps) to scale data on training only.\n",
    "    # We then recombine the training and testing datasets to change each ws to a supervised learning problem by taking all the first 180 time steps for all 12 predictors\n",
    "    # and changing these to (t-n_lag) to (t-1) since we lose one row through differencing. We then shift forward only one dependent variable (temperature or specific humidity)\n",
    "    # for time steps t to (t+n_seq)\n",
    "\n",
    "\n",
    "    diff_values = []\n",
    "    \n",
    "    for ws in range(len(data)):\n",
    "        \n",
    "        # transform data to be stationary\n",
    "        diff_series = difference(data[ws], 1)\n",
    "        for i in range(len(diff_series)):\n",
    "            diff_values_row = []\n",
    "            for j in range(len(diff_series[0])):\n",
    "                diff_values_row.append(diff_series[i][j])\n",
    "            diff_values.append(diff_values_row)\n",
    "\n",
    "    #print(\"Diff values:\")\n",
    "    #print(len(diff_values))\n",
    "    #print(len(diff_values[0]))\n",
    "    #print(len(diff_values_for_training))\n",
    "    #print(len(diff_values_for_training[0]))\n",
    "    \n",
    "    # rescale values to 0, 1\n",
    "    scaled_values = scaler_all_features.transform(diff_values)\n",
    "\n",
    "    validation = []\n",
    "    test = []\n",
    "\n",
    "    \n",
    "    \n",
    "    # Transform each weather station as a separate \"batch\"\n",
    "    for ws in range(len(data)):\n",
    "        # transform into a supervised learning problem X, y\n",
    "        first = (n_time_steps-1)*ws\n",
    "        last = (n_time_steps-1)*ws+(n_time_steps-2)\n",
    "        #print(\"Batch \"+str(ws+1)+\":\")\n",
    "        #print(\"Range: \"+str(first)+\"-\"+str(last))\n",
    "        scaled_values_batch = scaled_values[first:last]\n",
    "        supervised = series_to_supervised(scaled_values_batch, n_lag, n_seq)\n",
    "        supervised_values = supervised.values\n",
    "        # training/test/validation split is 80%/10%/10%\n",
    "        if ws < 2:\n",
    "            test.append([supervised_values])\n",
    "        else:\n",
    "            validation.append([supervised_values])\n",
    "        #print(\"Supervised count:\")\n",
    "        #print(len(supervised_values))\n",
    "        #print(len(supervised_values[0]))\n",
    "    \n",
    "    return validation, test\n",
    "    \n",
    "def plot_kfold(cv, X, y, ax, n_splits, xlim_max=105):\n",
    "    \n",
    "    #Plots the indices for a cross-validation object.\n",
    "    #Taken from https://www.geeksforgeeks.org/cross-validation-using-k-fold-with-scikit-learn/\n",
    "    \n",
    "    #Parameters:\n",
    "    #cv: Cross-validation object\n",
    "    #X: Feature set\n",
    "    #y: Target variable\n",
    "    #ax: Matplotlib axis object\n",
    "    #n_splits: Number of folds in the cross-validation\n",
    "    #xlim_max: Maximum limit for the x-axis\n",
    "        \n",
    "    # Set color map for the plot\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "    cv_split = cv.split(X=X, y=y)\n",
    "        \n",
    "    for i_split, (train_idx, test_idx) in enumerate(cv_split):\n",
    "        # Create an array of NaNs and fill in training/testing indices\n",
    "        indices = np.full(len(X), np.nan)\n",
    "        indices[test_idx], indices[train_idx] = 1, 0\n",
    "            \n",
    "        # Plot the training and testing indices\n",
    "        ax_x = range(len(indices))\n",
    "        ax_y = [i_split + 0.5] * len(indices)\n",
    "        ax.scatter(ax_x, ax_y, c=indices, marker=\"_\", \n",
    "                   lw=10, cmap=cmap_cv, vmin=-0.2, vmax=1.2)\n",
    "    \n",
    "        # Set y-ticks and labels\n",
    "        y_ticks = np.arange(n_splits) + 0.5\n",
    "        ax.set(yticks=y_ticks, yticklabels=range(n_splits),\n",
    "               xlabel=\"Weather Station index (file_id)\", ylabel=\"Fold\",\n",
    "               ylim=[n_splits, -0.2], xlim=[0, xlim_max])\n",
    "    \n",
    "        # Set plot title and create legend\n",
    "        ax.set_title(\"KFold\", fontsize=14)\n",
    "        legend_patches = [Patch(color=cmap_cv(0.8), label=\"Testing set\"), \n",
    "                          Patch(color=cmap_cv(0.02), label=\"Training set\")]\n",
    "        ax.legend(handles=legend_patches, loc=(1.03, 0.8))\n",
    "    \n",
    "#Main\n",
    "\n",
    "#Configure\n",
    "n_seq = 1\n",
    "if n_seq > 3:\n",
    "    n_lag = 25 - n_seq\n",
    "else:\n",
    "    n_lag = 22\n",
    "n_time_steps = 27\n",
    "n_test = 1\n",
    "\n",
    "print(\"Model Parameters:\")\n",
    "print(\"n_lag (number of input time steps): \"+str(n_lag))\n",
    "print(\"n_seq (number of output/future prediction time steps): \"+str(n_seq))\n",
    "\n",
    "# Create 2D array with file_ids to use for sample creation\n",
    "array = np.array([\n",
    "    6501, 6541, 6640, 6668, 6678, \n",
    "    6687, 6697, 6714, 6744, 6772, \n",
    "    6783, 6840, 6844, 6854, 6870, \n",
    "    6891, 6895, 6899, 6901, 6909, \n",
    "    6929, 6950, 6963, 6969, 6994, \n",
    "    7032, 7057, 7094, 7095, 7100, \n",
    "    7108, 7116, 7119, 7131, 7139, \n",
    "    7152, 7155, 7156, 7182, 7193, \n",
    "    7202, 7239, 7280, 7286, 7287, \n",
    "    7311, 7321, 7329, 7347, 7350, \n",
    "    7354, 7357, 7361, 7414, 7423, \n",
    "    7424, 7432, 7463, 7482, 7489, \n",
    "    7528, 7531, 7534, 7538, 7549, \n",
    "    7553, 7555, 7562, 7571, 7573, \n",
    "    7574, 7575, 7585, 7599, 7603, \n",
    "    7606, 7622, 7652, 7671, 7704, \n",
    "    7786, 7805, 7816, 7838, 7861, \n",
    "    7862, 7863, 7870, 7892, 7907, \n",
    "    7938, 7962, 7979, 7987, 7999, \n",
    "    8000, 8034, 8083, 8120, 8133, \n",
    "    8184, 8186, 8247, 8248, 9858])\n",
    "    \n",
    "#Create arrays holding the 5-fold cross-validation indices gathered for consistency across models\n",
    "train_array = []\n",
    "test_array = []\n",
    "    \n",
    "train_array.append([5, 9, 21, 22, 41, 42, 45, 46, 52, 66, 68, 72, 79, 81, 82, 83, 90, 92, 95])\n",
    "test_array.append([4, 10, 47, 53, 94])\n",
    "    \n",
    "train_array.append([4, 9, 10, 21, 41, 45, 46, 47, 52, 53, 66, 68, 81, 82, 83, 90, 92, 94, 95])\n",
    "test_array.append([5, 22, 42, 72, 79])\n",
    "    \n",
    "train_array.append([4, 5, 10, 21, 22, 41, 42, 46, 47, 52, 53, 72, 79, 82, 83, 90, 92, 94, 95])\n",
    "test_array.append([9, 45, 66, 68, 81])\n",
    "    \n",
    "train_array.append([4, 5, 9, 10, 21, 22, 42, 45, 47, 52, 53, 66, 68, 72, 79, 81, 82, 92, 94, 95])\n",
    "test_array.append([41, 46, 83, 90])\n",
    "    \n",
    "train_array.append([4, 5, 9, 10, 22, 41, 42, 45, 46, 47, 53, 66, 68, 72, 79, 81, 83, 90, 94])\n",
    "test_array.append([21, 52, 82, 92, 95])\n",
    "    \n",
    "# Equations for three Principal Components from PCA using response variables combined with other predictors\n",
    "#PC1=-0.0002714X1+0.02612X2+0.03858X3-0.007658X4+0.001592X5-0.02087X6+0.8564X7-0.1468X8+0.01192X9-0.0001049X10+0.01913X11+0.02076X12\n",
    "#PC2=0.0003944X1+0.002204X2+0.01052X3+0.3248X4-0.0009976X5-0.04421X6+2.3406X7+0.06103X8+0.08841X9+0.00009018X10+0.05678X11-0.002022X12\n",
    "#PC3=-0.00007998X1-0.0006124X2-0.001063X3-0.01855X4+0.00001956X5+0.01170X6+0.6076X7+0.4664X8-0.002995X9+0.008185X10+0.8815X11-0.0004730X12\n",
    "    \n",
    "# Equations for three Principal Components from PCA omitting both response variables,\n",
    "#PC-1=-0.0004514X1+0.03194X2-0.04343X3+0.002243X4-0.02252X5+0.9877X6-0.2265X7+0.006144X8-0.0001488X9+0.02943X10\n",
    "#PC-2=0.0001702X1+0.005484X2+0.2057X3-0.0003188X4-0.02584X5+1.6963X6-0.05890X7+0.05809X8+1.9748X9+0.03686X10\n",
    "#PC-3=-0.00006323X1-0.001180X2-0.02384X3-0.00002833X4+0.01170X5+0.5204X6+0.4791X7-0.004318X8+0.008271X9+0.8765X10\n",
    "    \n",
    "# Get the current working directory \n",
    "current_directory = os.getcwd() \n",
    "    \n",
    "# Print the current working directory \n",
    "print(current_directory)\n",
    "    \n",
    "# Define the directory containing the files \n",
    "path = current_directory+\"\\\\Modeling\\\\\"\n",
    "print(path)\n",
    "    \n",
    "filename = path + 'Final_Yearly_Dataset.csv'\n",
    "    \n",
    "# load dataset\n",
    "df = read_csv(filename, header=0, parse_dates=[0], index_col=0, date_format='%Y-%m')\n",
    "\n",
    "# Swap columns 'temperatures' and 'specific_humidity' directly\n",
    "df['A'] = df['temperatures']\n",
    "df['temperatures'] = df['specific_humidity']\n",
    "df['specific_humidity'] = df['A']\n",
    "df = df.drop(columns=['A'])\n",
    "# Rename columns\n",
    "df = df.rename(columns={'temperatures': 'specific_humidity', 'specific_humidity': 'temperatures'})\n",
    "\n",
    "print(df)\n",
    "    \n",
    "df = df.rename(columns={'Unnamed: 0' : 'indices'})\n",
    "    \n",
    "#Remove unused columns\n",
    "df = df.drop(['vapor_pressure'], axis=1)\n",
    "    \n",
    "# Round numbers in columns to reasonable precision,\n",
    "df['temperatures'] = np.round(df['temperatures'], 2)\n",
    "df['slp'] = np.round(df['slp'], 2)\n",
    "df['wet_bulb_temperature'] = np.round(df['wet_bulb_temperature'], 2)\n",
    "df['specific_humidity'] = np.round(df['specific_humidity'], 2)\n",
    "df['GHI'] = np.round(df['GHI'], 2)\n",
    "df['PRCP'] = np.round(df['PRCP'], 2)\n",
    "df['SNDP'] = np.round(df['SNDP'], 2)\n",
    "df['solar_activity'] = np.round(df['solar_activity'], 2)\n",
    "df['ONI'] = np.round(df['ONI'], 2)\n",
    "df['water'] = np.round(df['water'], 0)\n",
    "df['region'] = np.round(df['region'], 0)\n",
    "\n",
    "#Since the NaN values of GHI are gone which is used in data preparation, simulate these in solar activity instead\n",
    "df['solar_activity'].loc[df['Year'] > 2020] = np.nan\n",
    "    \n",
    "df_trimmed = df[df['file_id'] != 7533] # Remove file_id 7533 so there are 105 weather stations for 5-fold CV\n",
    "df_trimmed = df_trimmed.drop(['Year', 'date', 'latitude', 'longitude', 'elevation'], axis=1)\n",
    "\n",
    "#Test model and accuracy when GHI is removed as a predictor\n",
    "df_trimmed = df_trimmed.drop(['GHI'], axis=1)\n",
    "\n",
    "n_vars = df_trimmed.shape[1]-1\n",
    "    \n",
    "X = []\n",
    "y = []\n",
    "    \n",
    "for i in array:\n",
    "    add_to_X = [] # create list to store each column to add to X\n",
    "    new_df = df_trimmed[df_trimmed['file_id'] == i].drop(['file_id'], axis=1)\n",
    "    add_to_y = []\n",
    "    for j in range(new_df.shape[0]):\n",
    "        add_to_y.append(new_df['specific_humidity'].iloc[j])\n",
    "    y.append(add_to_y)\n",
    "    #new_df = new_df.drop(['specific_humidity'], axis=1)\n",
    "    columns_list = new_df.columns.tolist()\n",
    "    for j in range(new_df.shape[0]):\n",
    "        l=0\n",
    "        new_row = []\n",
    "        for m in columns_list:\n",
    "            new_row.append(new_df.iloc[j, l])\n",
    "            l += 1\n",
    "        add_to_X.append(new_row)\n",
    "    X.append(add_to_X)\n",
    "\n",
    "print(df_trimmed)\n",
    "print(new_df)\n",
    "print(X[0])\n",
    "\n",
    "#Perform k-fold cross-validation\n",
    "#Taken from: https://www.geeksforgeeks.org/cross-validation-using-k-fold-with-scikit-learn/\n",
    "    \n",
    "#k = 5  # Number of folds\n",
    "#kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "#for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "#    print(f\"Fold {i}:\")\n",
    "#    print(f\"  Training dataset index: {train_index}\")\n",
    "#    print(f\"  Test dataset index: {test_index}\")\n",
    "    \n",
    "#for train_indices, test_indices in kf.split(X):\n",
    "#    print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    \n",
    "# Create figure and axis\n",
    "#fig, ax = plt.subplots(figsize=(6, 3))\n",
    "#plot_kfold(kf, X, y, ax, k)\n",
    "#plt.tight_layout()\n",
    "#fig.subplots_adjust(right=0.6)\n",
    "    \n",
    "#Create train and test sets for each cross-validation split\n",
    "train_X = []\n",
    "train_y = []\n",
    "val_X = []\n",
    "val_y = []\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}\")\n",
    "    #Add each corresponding sample for each entry of train index \n",
    "    train_X_rows = [] # Stores all the samples for one fold of train_X\n",
    "    train_y_rows = [] # Stores all the samples for one fold of train_y\n",
    "    for j in train_array[i]:\n",
    "        train_X_rows.append(X[j])\n",
    "        train_y_rows.append(y[j])\n",
    "    # Stores one fold of the train dataset\n",
    "    train_X.append(train_X_rows)\n",
    "    train_y.append(train_y_rows)\n",
    "    #Add each corresponding sample for each entry of the validation index \n",
    "    val_X_rows = [] # Stores all the samples for one fold of val_X\n",
    "    val_y_rows = [] # Stores all the samples for one fold of val_y\n",
    "    for j in test_array[i]: \n",
    "            val_X_rows.append(X[j])\n",
    "            val_y_rows.append(y[j])\n",
    "    # Stores one fold of the validation dataset\n",
    "    val_X.append(val_X_rows)\n",
    "    val_y.append(val_y_rows) \n",
    "    \n",
    "    #print(\"Train_X Fold \"+str(i)+\":\")\n",
    "    #print(len(train_X[i]))\n",
    "    #print(len(train_X[i][0]))\n",
    "    #print(len(train_X[i][0][0])) \n",
    "    #print(\"Train_y Fold \"+str(i)+\":\")\n",
    "    #print(len(train_y[i]))\n",
    "    #print(len(train_y[i][0]))\n",
    "    #print(train_y[i][0][0])\n",
    "    #print(\"Validation_X Fold \"+str(i)+\":\")\n",
    "    #print(len(val_X[i]))\n",
    "    #print(len(val_X[i][0]))\n",
    "    #print(len(val_X[i][0][0]))\n",
    "    #print(\"Validation_y Fold \"+str(i)+\":\")\n",
    "    #print(len(val_y[i]))\n",
    "    #print(len(val_y[i][0]))\n",
    "    #print(val_y[i][0][0])\n",
    "    \n",
    "#Convert 3D arrays to DataFrames\n",
    "df_X = []\n",
    "df_y = []\n",
    "val_df_X = []\n",
    "val_df_y = []\n",
    "dataset = []\n",
    "dataset_test = []\n",
    "scaler = []\n",
    "scaler_all_features = []\n",
    "train = []\n",
    "test = []\n",
    "validation = []\n",
    "\n",
    "for i in range(5):\n",
    "    dataset_scaling = [] # Holds all 84 weather station rows to train the scaling function\n",
    "    dataset_testing = [] # Holds remaining 21 weather station rows for testing and validation\n",
    "    print(\"Fold \"+str(i+1)+\":\")\n",
    "    #Transform train_X to the correct format\n",
    "    df1 = []\n",
    "    dataset_df = [] # captures each weather station's dataset as values for training scaler mapping\n",
    "    df_X.append(pd.DataFrame(train_X[i]))\n",
    "    X_t = df_X[i].transpose()\n",
    "    if i==3:\n",
    "        train_size = 20\n",
    "        val_size = 4\n",
    "    else:\n",
    "        train_size = 19\n",
    "        val_size = 5\n",
    "        \n",
    "    for k in range(train_size):\n",
    "        X = np.array(X_t.iloc[:, k])\n",
    "        df = pd.DataFrame()\n",
    "        for j in range(n_time_steps):\n",
    "            new_row = pd.DataFrame(X[j]).transpose()\n",
    "            new_row.columns = new_df.columns\n",
    "            # Add the new row\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.columns = new_df.columns\n",
    "        df1.append(df)\n",
    "        dataset_df.append(df.values)\n",
    "        dataset_scaling.append(df.values)\n",
    "    df_X[i] = df1\n",
    "    dataset.append(dataset_df)\n",
    "     \n",
    "\n",
    "    #print(len(dataset))\n",
    "    #print(len(dataset[0]))\n",
    "    #print(len(dataset_scaling))\n",
    "    #print(len(dataset_scaling[:][0]))\n",
    "    #print(\"Stop\")\n",
    "    \n",
    "    #Transform train_y to the correct format\n",
    "    df2 = []\n",
    "    df_y.append(pd.DataFrame(train_y[i]))\n",
    "    y_t = df_y[i].transpose()\n",
    "    \n",
    "    for j in range(train_size):\n",
    "        y = np.array(y_t.iloc[:, j])\n",
    "        y = pd.DataFrame(y)\n",
    "        y.columns = ['specific_humidity']\n",
    "        df2.append(y)\n",
    "    df_y[i] = df2\n",
    "\n",
    "    #Transform val_X to the correct format\n",
    "    df3 = []\n",
    "    dataset_vl_df = [] # captures each weather station's dataset as values for training scaler mapping\n",
    "    val_df_X.append(pd.DataFrame(val_X[i]))\n",
    "    val_X_t = val_df_X[i].transpose()\n",
    "    for k in range(val_size):\n",
    "        vl_X = np.array(val_X_t.iloc[:, k])\n",
    "        vl_df = pd.DataFrame()\n",
    "        for j in range(n_time_steps):\n",
    "            new1_row = pd.DataFrame(vl_X[j]).transpose()\n",
    "            new1_row.columns = new_df.columns\n",
    "            # Add the new row\n",
    "            vl_df = pd.concat([vl_df, new1_row], ignore_index=True)\n",
    "        vl_df.columns = new_df.columns\n",
    "        df3.append(vl_df)\n",
    "        dataset_vl_df.append(vl_df.values)\n",
    "        dataset_testing.append(vl_df.values)\n",
    "    val_df_X[i] = df3\n",
    "    dataset_test.append(dataset_vl_df)\n",
    "\n",
    "    #Transform val_y to the correct format\n",
    "    df4 = []\n",
    "    val_df_y.append(pd.DataFrame(train_y[i]))\n",
    "    val_y_t = val_df_y[i].transpose()\n",
    "    \n",
    "    for j in range(val_size):\n",
    "        v_y = np.array(val_y_t.iloc[:, j])\n",
    "        v_y = pd.DataFrame(v_y)\n",
    "        v_y.columns = ['specific_humidity']\n",
    "        df4.append(v_y)\n",
    "    val_df_y[i] = df4\n",
    "    \n",
    "    scaler.append(MinMaxScaler(feature_range=(0, 1)))\n",
    "    scaler_all_features.append(MinMaxScaler(feature_range=(0, 1)))\n",
    "    train.append([1])\n",
    "    test.append([1])\n",
    "    validation.append([1])\n",
    "    \n",
    "    # prepare data\n",
    "    scaler[i], scaler_all_features[i], train[i] = prepare_training_data(dataset_scaling, n_lag, n_seq, n_time_steps)\n",
    "\n",
    "    validation[i], test[i] = prepare_testing_and_validation_data(dataset_testing, n_lag, n_seq, n_time_steps, scaler_all_features[i])\n",
    "\n",
    "    #Reshape dimensionality\n",
    "    train1 = train[i]\n",
    "    test1 = test[i]\n",
    "    validation1 = validation[i]\n",
    "    print(np.array(train1).shape)\n",
    "    print(np.array(test1).shape)\n",
    "    print(np.array(validation1).shape)\n",
    "    train2 = []\n",
    "    test2 = []\n",
    "    validation2 = []\n",
    "\n",
    "    for k in range(train_size):\n",
    "        train2.append(train1[k][0])\n",
    "        \n",
    "    for k in range(2):\n",
    "        test2.append(test1[k][0])\n",
    "\n",
    "    for k in range(val_size-2):\n",
    "        validation2.append(validation1[k][0])\n",
    "\n",
    "    print(np.array(train2).shape)\n",
    "    print(np.array(test2).shape)\n",
    "    print(np.array(validation2).shape)\n",
    "\n",
    "    train[i] = train2\n",
    "    test[i] = test2\n",
    "    validation[i] = validation2\n",
    "\n",
    "    #Reshape dimensionality (again)\n",
    "    dim_size = n_seq + n_vars*n_lag\n",
    "    train1 = np.array(train[i]).reshape(train_size, dim_size)\n",
    "    test1 = np.array(test[i]).reshape(2, dim_size)\n",
    "    validation1 = np.array(validation[i]).reshape(val_size-2, dim_size)\n",
    "    train2 = pd.DataFrame(train1).values\n",
    "    test2 = pd.DataFrame(test1).values\n",
    "    validation2 = pd.DataFrame(validation1).values\n",
    "    print(np.array(train2).shape)\n",
    "    print(np.array(test2).shape)\n",
    "    print(np.array(validation2).shape)\n",
    "\n",
    "    print(train2)\n",
    "\n",
    "    train[i] = train2\n",
    "    test[i] = test2\n",
    "    validation[i] = validation2\n",
    "    \n",
    "    #X_train = train1[:][:-n_seq]\n",
    "    #y_train = train1[:][-n_seq:]\n",
    "    #X_test = test1[:][:-n_seq]\n",
    "    #y_test = test1[:][-n_seq:]\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be74bc78-33b4-4695-9874-daa61b380616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "22\n",
      "(19, 243)\n",
      "(2, 243)\n"
     ]
    }
   ],
   "source": [
    "print(n_seq)\n",
    "print(n_lag)\n",
    "print(np.array(train[1]).shape)\n",
    "print(np.array(test[1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ad946-c0cb-407a-8a65-8b9aef8cf7cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 07:17:25,258] A new study created in memory with name: no-name-f78b9235-f3eb-41fc-b6fa-3a8bac7eff14\n",
      "[I 2025-06-07 07:19:01,413] Trial 0 finished with value: 0.09912821501493455 and parameters: {'lr': 0.6275076932223186, 'optimizer': 'Adam', 'activation_function': 'relu', 'filters': 1024, 'lstm_units': 256, 'dropout_rate': 0.35481226196803994}. Best is trial 0 with value: 0.09912821501493455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.10530322045087814, 0.12897783517837524, 0.07742753624916077, 0.13076873123645782, 0.05316375195980072]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09912821501493455\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.03004631127324006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 07:20:53,567] Trial 1 finished with value: 6.8006988048553465 and parameters: {'lr': 0.5049261003194517, 'optimizer': 'Adam', 'activation_function': 'tanh', 'filters': 256, 'lstm_units': 256, 'dropout_rate': 0.4921614571910129}. Best is trial 0 with value: 0.09912821501493455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[7.771005153656006, 3.692298173904419, 7.777169704437256, 8.365581512451172, 6.397439479827881]\n",
      "Mean Cross Validation Accuracy:\n",
      "6.8006988048553465\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "1.6833668811345521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 07:22:57,514] Trial 2 finished with value: 0.08277424499392509 and parameters: {'lr': 0.09468862730110439, 'optimizer': 'SGD', 'activation_function': 'relu', 'filters': 256, 'lstm_units': 256, 'dropout_rate': 0.30318571224381896}. Best is trial 2 with value: 0.08277424499392509.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.06907238066196442, 0.12134446948766708, 0.057663511484861374, 0.09161197394132614, 0.07417888939380646]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.08277424499392509\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.022171774091725487\n"
     ]
    }
   ],
   "source": [
    "n_batch = 1\n",
    "nb_epoch = 100\n",
    "\n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def mape (y_true, y_pred):\n",
    "    return 100*K.mean(K.sqrt(K.square(y_true - y_pred))/y_true)\n",
    "    \n",
    "def pearson (y_true, y_pred):\n",
    "    return (K.square(K.mean((y_true - K.mean(y_true))*(y_pred - K.mean(y_pred)))))/(K.mean(K.square(y_true - K.mean(y_true)))*K.mean(K.square(y_pred - K.mean(y_pred))))\n",
    " \n",
    "# fit a CNN-LSTM network to training data\n",
    "#Adapted from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def create_model(trial, X, y, n_lag, n_seq, n_batch, nb_epoch):\n",
    "    \n",
    "    #Parameters:\n",
    "    #trial (array-like): Optuna parameters.\n",
    "    #train (array-like): Target values.\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations\n",
    "    #nb_epoch (int): Maximum number of epochs\n",
    "    \n",
    "    # Hyperparameters to be tuned by Optuna (taken from Javier Leon's dissertation 'Fruit Prices')\n",
    "    lr = trial.suggest_float('lr', 1e-3, 1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=lr)\n",
    "    \n",
    "    #Optuna will try either Rectified Linear Unit (ReLU) = max(0, x), tanh, or sigmoid functions\n",
    "    activation_function = trial.suggest_categorical('activation_function', ['relu', 'tanh', 'sigmoid'])\n",
    "    \n",
    "    filters = trial.suggest_categorical('filters', [256, 512, 1024]) # Used for CNN-LSTM model\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [256, 512, 1024])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=1, activation='relu', input_shape=(X.shape[1], X.shape[2]))) # CNN-LSTM only\n",
    "    model.add(MaxPooling1D(pool_size=1)) # CNN-LSTM Only\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(256, input_dim=y.shape[1], activation=activation_function))\n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    model.add(Dense(n_seq))\n",
    "    model.compile(loss=rmse, optimizer=optimizer)\n",
    "        \n",
    "    return model\n",
    "    \n",
    "def objective(trial):\n",
    "    \n",
    "    cv_accuracies = []\n",
    "        \n",
    "    for i in range(5):\n",
    "        train1 = train[i]\n",
    "        test1 = test[i]\n",
    "        validation1 = validation[i]\n",
    "\n",
    "        X, y = train1[:, 0:-n_seq], train1[:, -n_seq:]\n",
    "        X_test, y_test = test1[:, 0:-n_seq], test1[:, -n_seq:]\n",
    "        X_val, y_val = validation1[:, 0:-n_seq], validation1[:, -n_seq:]\n",
    "        X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "        X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "        X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "    \n",
    "        model = create_model(trial, X, y, n_lag, n_seq, n_batch, nb_epoch)\n",
    "\n",
    "        history = model.fit(X, y, validation_data=(X_val, y_val), epochs=nb_epoch, verbose=0)\n",
    "\n",
    "        cv_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "        loss = history.history['val_loss'][-1]\n",
    "    \n",
    "        # Plotting the training and validation loss\n",
    "        #plt.figure(figsize=(10, 4))\n",
    "        #plt.plot(history.history['loss'], label='Training Loss')\n",
    "        #plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        #title1 = \"CNN-LSTM Training and Validation Loss for Fold \" + str(i+1)\n",
    "        #plt.title(title1)\n",
    "        #plt.xlabel('Epoch')\n",
    "        #plt.ylabel('Root Mean Squared Error Loss')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "            \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "    \n",
    "    print(\"Cross Validation Accuracies:\")\n",
    "    print(cv_accuracies)\n",
    "    print(\"Mean Cross Validation Accuracy:\")\n",
    "    print(np.mean(cv_accuracies))\n",
    "    print(\"Standard Deviation of Cross Validation Accuracy:\")\n",
    "    print(np.std(cv_accuracies))\n",
    "        \n",
    "    return np.mean(cv_accuracies)\n",
    "\n",
    "n_batch = 1\n",
    "nb_epoch = 100\n",
    "\n",
    "# optimize and fit model\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "\n",
    "print('Best trial:', study.best_trial.params)\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Fold \"+str(i+1)+\":\")\n",
    "    train1 = train[i]\n",
    "    test1 = test[i]\n",
    "    validation1 = validation[i]\n",
    "\n",
    "    X, y = train1[:, 0:-n_seq], train1[:, -n_seq:]\n",
    "    X_test, y_test = test1[:, 0:-n_seq], test1[:, -n_seq:]\n",
    "    X_val, y_val = validation1[:, 0:-n_seq], validation1[:, -n_seq:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "    X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "\n",
    "    best_model = create_model(optuna.trial.FixedTrial(best_params), X, y, n_lag, n_seq, n_batch, nb_epoch)\n",
    "    history = best_model.fit(X, y, epochs=100, batch_size=n_batch, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    title1 = \"CNN-LSTM Training and Validation Loss for Best Model\"\n",
    "    plt.title(title1)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Root Mean Squared Error Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f'Test Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e0a49-8fc2-4e2f-8bf4-7bb9dde70c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e54bcc-51b3-4db4-adf7-a8041e2b237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16289125-ffc3-4177-a011-ecba68a7ffe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ec920-1d52-457b-b12f-421c8bfa1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study, params=['optimizer', 'activation_function', 'lstm_units'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe2ed1d-7fe4-445f-91bc-135f6dec0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study, params=['lr', 'dropout_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c7a5a-83c7-4dac-9e14-04bbba312ade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def kmape (y_true, y_pred):\n",
    "    return K.mean(K.sqrt(K.square(y_true - y_pred))/y_true)\n",
    "    \n",
    "def kpearson (y_true, y_pred):\n",
    "    mean_true = K.mean(y_true)\n",
    "    mean_pred = K.mean(y_pred)\n",
    "    return (K.square(K.mean((y_true - mean_true)*(y_pred - mean_pred))))/(K.mean(K.square(y_true - mean_true))*K.mean(K.square(y_pred - mean_pred)))\n",
    "\n",
    "# fit the best CNN-LSTM model using parameters found by Optuna\n",
    "def create_best_model(X, y, n_lag, n_seq, n_batch, nb_epoch):\n",
    "    \n",
    "    #Parameters:\n",
    "    #trial (array-like): Optuna parameters.\n",
    "    #train (array-like): Target values.\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations\n",
    "    #nb_epoch (int): Maximum number of epochs\n",
    "    \n",
    "    # Hyperparameters from the best model\n",
    "    filters = 512\n",
    "    lr = 0.003916316513762676\n",
    "    optimizer = SGD(learning_rate = lr)\n",
    "    activation_function = 'tanh'\n",
    "    lstm_units = 1024\n",
    "    dropout_rate = 0.2468857003842267\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=1, activation='relu', input_shape=(X.shape[1], X.shape[2]))) # CNN-LSTM only\n",
    "    model.add(MaxPooling1D(pool_size=1)) # CNN-LSTM only\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(256, input_dim=y.shape[1], activation=activation_function))\n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    model.add(Dense(n_seq))\n",
    "    model.compile(loss=rmse, optimizer=optimizer, metrics=['accuracy', 'mae', rmse, mape, pearson])\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Make one forecast with a CNN-LSTM\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def forecast_cnnlstm(model, X, n_seq, n_test):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, X.shape[1])\n",
    "    # make forecast\n",
    "    forecast = model.predict(X)\n",
    "    # convert to array\n",
    "    return forecast\n",
    "     \n",
    "# Evaluate the persistence model\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def make_forecasts(model, X_test, y_test, n_lag, n_seq, n_test):\n",
    "    forecasts = list()\n",
    "    for i in range(len(X_test)):\n",
    "        X, y = X_test[i, :], y_test[i, :]\n",
    "        # make forecast\n",
    "        forecast = forecast_cnnlstm(model, X, n_seq, n_test)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    "\n",
    "   \n",
    "# Invert differenced forecast\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    # Invert the first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob)\n",
    "    # Propagate the difference forecast using the inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted\n",
    "     \n",
    "# Inverse data transform on forecasts\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def inverse_transform(series, forecasts, scaler, n_test, n_seq):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create an array from the forecast\n",
    "        forecast = forecasts[i]\n",
    "        forecast = forecast.reshape(1, n_seq)\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i\n",
    "        last_ob = series.values[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted\n",
    "     \n",
    "# Evaluate the RMSE for each forecast time step\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def evaluate_forecasts(actual, forecasts, n_lag, n_seq):\n",
    "\n",
    "    t = np.array(actual).transpose()\n",
    "    f = np.array(forecasts).transpose()\n",
    "\n",
    "    print(\"f, t:\")\n",
    "    print(t)\n",
    "    print(f)\n",
    "    rmse_list = []\n",
    "    mae_list = []\n",
    "    skmape_list = []\n",
    "    mape_list = []\n",
    "    r2_list = []\n",
    "    pearson_list = []\n",
    "    actual_list = []\n",
    "\n",
    "    for i in range(n_seq):\n",
    "        act = [row for row in t[i]]\n",
    "        predicted = [forecast for forecast in f[i]]\n",
    "        rmse1 = sqrt(mean_squared_error(act, predicted))\n",
    "        rmse2 = np.array(rmse(tf.convert_to_tensor(np.array(act)), tf.convert_to_tensor(np.array(predicted))))\n",
    "        mae = mean_absolute_error(act, predicted)\n",
    "        mse = mean_squared_error(act, predicted)\n",
    "        skmape = mean_absolute_percentage_error(act, predicted)\n",
    "        kmape1 = np.array(kmape(tf.convert_to_tensor(np.array(act)), tf.convert_to_tensor(np.array(predicted))))\n",
    "        kpearson1 = np.array(kpearson(tf.convert_to_tensor(np.array(act)), tf.convert_to_tensor(np.array(predicted))))\n",
    "        r2 = r2_score(act, predicted) \n",
    "        avg_actual = np.mean(act)\n",
    "        print(\"Year at t+\"+str(i+1)+\":\")\n",
    "        print('t+%d SKRMSE: %f' % ((i+1), rmse1))\n",
    "        print('t+%d KRMSE: %f' % ((i+1), rmse2))        \n",
    "        print('t+%d MAE: %f' % ((i+1), mae))\n",
    "        print('t+%d SKMAPE: %f' % ((i+1), skmape))\n",
    "        print('t+%d R2_SCORE: %f' % ((i+1), r2))\n",
    "        print('t+%d KMAPE: %f' % ((i+1), kmape1))\n",
    "        print('t+%d PEARSON: %f' % ((i+1), kpearson1))\n",
    "        print('t+%d AVERAGE ACTUAL: %f' % ((i+1), avg_actual))\n",
    "\n",
    "    y_true = tf.convert_to_tensor(np.array(actual).flatten())\n",
    "    y_pred = tf.convert_to_tensor(np.array(forecasts).flatten())\n",
    "\n",
    "    rmse1 = sqrt(mean_squared_error(np.array(y_true), np.array(y_pred)))\n",
    "    rmse2 = np.array(rmse(y_true, y_pred))\n",
    "    mae = mean_absolute_error(np.array(y_true), np.array(y_pred))\n",
    "    mse = mean_squared_error(np.array(y_true), np.array(y_pred))\n",
    "    skmape = mean_absolute_percentage_error(np.array(y_true), np.array(y_pred))\n",
    "    kmape1 = np.array(kmape(y_true, y_pred))\n",
    "    kpearson1 = np.array(kpearson(y_true, y_pred))\n",
    "    r2 = r2_score(np.array(y_true), np.array(y_pred)) \n",
    "    avg_actual = np.mean(np.array(y_true))\n",
    "    rmse1_list.append(rmse1)\n",
    "    rmse2_list.append(rmse2)\n",
    "    mae_list.append(mae)\n",
    "    skmape_list.append(skmape)\n",
    "    pearson_list.append(kpearson1)\n",
    "    mape_list.append(kmape1)\n",
    "    r2_list.append(r2)\n",
    "    actual_list.append(avg_actual)\n",
    "    print(\"Metrics for this Fold:\")\n",
    "    print('SKRMSE: %f' % (rmse1))\n",
    "    print('KRMSE: %f' % (rmse2))        \n",
    "    print('MAE: %f' % (mae))\n",
    "    print('SKMAPE: %f' % (skmape))\n",
    "    print('R2_SCORE: %f' % (r2))\n",
    "    print('KMAPE: %f' % (kmape1))\n",
    "    print('PEARSON: %f' % (kpearson1))\n",
    "    print('AVERAGE ACTUAL: %f' % (avg_actual))\n",
    "\n",
    "    return rmse1_list, rmse2_list, mae_list, skmape_list, r2_list, mape_list, pearson_list, actual_list\n",
    "     \n",
    "# Plot the forecasts in the context of the original dataset\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def plot_forecasts(series, forecasts, actual, n_test, n_seq, n_lag):\n",
    "    # Plot the actual data in blue\n",
    "    plt.plot(series[:n_lag+2].index, series[:n_lag+2].values)\n",
    "    off_s = n_lag + 1\n",
    "    off_e = off_s + len(actual) + 1\n",
    "    xaxis = [1998+x for x in range(off_s, off_e)]\n",
    "    yaxis = [series.values[off_s]] + actual\n",
    "    plt.plot(xaxis, yaxis, color='blue')\n",
    "    # Plot the forecasts in red\n",
    "    off_s = n_lag + 1\n",
    "    off_e = off_s + len(forecasts) + 1\n",
    "    xaxis = [1998+x for x in range(off_s, off_e)]\n",
    "    yaxis = [series.values[off_s]] + forecasts\n",
    "    plt.plot(xaxis, yaxis, color='red')\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Specific Humidity\")\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "\n",
    "n_batch = 1\n",
    "n_test = 2\n",
    "nb_epoch = 100\n",
    "    \n",
    "rmse1_avg_list = []\n",
    "rmse2_avg_list = []\n",
    "mae_avg_list = []\n",
    "skmape_avg_list = []\n",
    "r2_score_avg_list = []\n",
    "mape_avg_list = []\n",
    "pearson_avg_list = []\n",
    "actual_avg_list = []\n",
    "forecast_results = []\n",
    "actual_results = []\n",
    "    \n",
    "for i in range(5):\n",
    "    train1 = train[i]\n",
    "    test1 = test[i]\n",
    "    validation1 = validation[i]\n",
    "\n",
    "    X = train1[:, 0:-n_seq]\n",
    "    y = train1[:, -n_seq:]\n",
    "    X_test = test1[:, 0:-n_seq]\n",
    "    y_test = test1[:, -n_seq:]\n",
    "    X_val = validation1[:, 0:-n_seq]\n",
    "    y_val = validation1[:, -n_seq:]\n",
    "    \n",
    "    dataset_df = pd.DataFrame(val_y[i])\n",
    "    dataset_df = dataset_df.iloc[0:2, :]\n",
    "    dataset = dataset_df.values\n",
    "\n",
    "    print(dataset)\n",
    "\n",
    "    series = pd.Series(dataset[:, 0]) # Using first column (specific_humidity)\n",
    "\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    X1 = X_test\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "    X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "\n",
    "    best_model = create_best_model(X, y, n_lag, n_seq, n_batch, nb_epoch)\n",
    "    history = best_model.fit(X, y, epochs=100, batch_size=n_batch, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f'Test Loss: {loss}')\n",
    "\n",
    "    # make forecasts\n",
    "    forecasts = make_forecasts(best_model, X_test, y_test, n_lag, n_seq, n_test)\n",
    "    \n",
    "    # inverse transform forecasts and test\n",
    "    forecasts = inverse_transform(series, forecasts, scaler[i], n_test, n_seq)\n",
    "    actual = [row[-n_seq:] for row in test1]\n",
    "    actual = inverse_transform(series, actual, scaler[i], n_test, n_seq)\n",
    "\n",
    "    # Print out plots of actual and predicted values for weather stations\n",
    "    ws = [1,2]\n",
    "    print(ws)\n",
    "    a = []\n",
    "    f = []\n",
    "    for q in range(len(actual)):\n",
    "        x = actual[q]\n",
    "        a.append(x[0])\n",
    "    for q in range(len(forecasts)):\n",
    "        x = forecasts[q]\n",
    "        f.append(x[0])\n",
    "    print(a)\n",
    "    print(f)\n",
    "    # Create a DataFrame for plotting\n",
    "    results_df = pd.DataFrame({\n",
    "        'ws': ws,\n",
    "        'Actual': a,\n",
    "        'Predicted': f\n",
    "    })\n",
    "\n",
    "    print(results_df)\n",
    "    \n",
    "    print(\"Actual and Forecasts:\")\n",
    "    print(actual)\n",
    "    print(forecasts)\n",
    "    \n",
    "    for j in range(2):\n",
    "        print(\"Weather Station \"+str(j+1)+\":\")\n",
    "        print(\"Actual S.H.\\tPredicted S.H.\\tDifference\")\n",
    "        print(\"-----------\\t--------------\\t----------\")\n",
    "        for k in range(n_seq):\n",
    "            diff = forecasts[j][k] - actual[j][k]\n",
    "            print(f\"{actual[j][k]:.2f}\\t\\t{forecasts[j][k]:.2f}\\t\\t{diff:.2f}\")\n",
    "\n",
    "        # plot forecasts\n",
    "        dataset_df = pd.DataFrame(val_y[i])\n",
    "        dataset_df = dataset_df.iloc[0:2, :]\n",
    "        dataset_df = dataset_df.transpose()\n",
    "        dataset = dataset_df.values\n",
    "\n",
    "        series_ws = pd.Series(dataset[:, j]) # Using first column (specific_humidity)\n",
    "        forecasts_ws = forecasts[j]\n",
    "        actual_ws = actual[j]\n",
    "\n",
    "        # Set index starting from 1998\n",
    "        series_ws.index = range(1998, 1998 + len(series_ws))        \n",
    "        \n",
    "        plot_forecasts(series_ws, forecasts_ws, actual_ws, n_test, n_seq, n_lag)\n",
    "\n",
    "    for k in range(n_seq):\n",
    "        print(\"Predictions for (t+\"+str(k)+\"):\")\n",
    "        # Print out plots of actual and predicted values for each weather station\n",
    "        results = []\n",
    "\n",
    "        # Create a DataFrame for plotting\n",
    "        for j in range(2):\n",
    "            results.append([j, actual[j][k], forecasts[j][k]])\n",
    "               \n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.columns = ['Weather_Station', 'Actual', 'Predicted']\n",
    "\n",
    "        print(results_df)\n",
    "\n",
    "        # Plotting the results\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.scatter(results_df['Weather_Station'], results_df['Actual'], label='Actual')\n",
    "        plt.scatter(results_df['Weather_Station'], results_df['Predicted'], label='Predicted', alpha=0.7)\n",
    "        title1='CNN-LSTM Model Comparison Specific Humidity Prediction (Fold='+str(i)+', n_seq='+str(n_seq)+', features=No_GHI)'\n",
    "        plt.title(title1)\n",
    "        plt.xlabel('Weather Station')\n",
    "        plt.ylabel('Specific Humidity')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    rmse1_list = [] # list stores sklearn's root mean squared errors for each future time prediction\n",
    "    rmse2_list = [] # list stores root mean squared errors for each future time prediction\n",
    "    mae_list = [] # list stores mean absolute errors for each future time prediction\n",
    "    skmape_list = [] # list stores sklearn's mean squared percentage errors for each future time prediction\n",
    "    r2_list = [] # list stores sklearn's r2 pearson values for each future time prediction\n",
    "    mape_list = [] # list stores mean squared percentage errors for each future time prediction\n",
    "    pearson_list = [] # list stores r2 pearson values for each future time prediction\n",
    "    actual_list = [] # list stores average actual values for each future time prediction\n",
    "\n",
    "    # evaluate forecasts\n",
    "    rmse1_list, rmse2_list, mae_list, skmape_list, r2_list, mape_list, pearson_list, actual_list = evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "\n",
    "    rmse1_avg_list.append(rmse1_list)\n",
    "    rmse2_avg_list.append(rmse2_list)\n",
    "    mae_avg_list.append(mae_list)\n",
    "    skmape_avg_list.append(skmape_list)\n",
    "    r2_score_avg_list.append(r2_list)\n",
    "    mape_avg_list.append(mape_list)\n",
    "    pearson_avg_list.append(pearson_list)\n",
    "    actual_avg_list.append(actual_list)\n",
    "    forecast_results.append(forecasts)\n",
    "    actual_results.append(actual)\n",
    "            \n",
    "    best_model.summary()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    title2 = 'CNN-LSTM Training and Validation Loss (Fold='+str(i)+', n_seq='+str(n_seq)+', features=No_GHI)'\n",
    "    plt.title(title2)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "print(rmse1_avg_list)\n",
    "print(rmse2_avg_list)\n",
    "print(mae_avg_list)\n",
    "print(skmape_avg_list)\n",
    "print(r2_score_avg_list)\n",
    "print(mape_avg_list)\n",
    "print(pearson_avg_list)\n",
    "print(np.array(actual_results).flatten())\n",
    "print(np.array(forecast_results).flatten())\n",
    "a2 = np.array(actual_results).flatten()\n",
    "f2 = np.array(forecast_results).flatten()\n",
    "p1 = np.array(pearson(tf.convert_to_tensor(a2), tf.convert_to_tensor(f2)))\n",
    "print(\"Accuracy Results:\")\n",
    "print(\"RMSE for each fold:\")\n",
    "print(rmse1_avg_list)\n",
    "print(\"Average SKRMSE:\"+str(np.mean(rmse1_avg_list)))\n",
    "print(\"Average KRMSE:\"+str(np.mean(rmse2_avg_list)))\n",
    "print(\"Average MAE:\"+str(np.mean(mae_avg_list)))\n",
    "print(\"Average SKMAPE:\"+str(np.mean(skmape_avg_list)))\n",
    "print(\"Average MAPE:\"+str(np.mean(mape_avg_list)))\n",
    "print(\"Average PEARSON:\"+str(p1))\n",
    "print(\"Average R2 Score:\"+str(np.mean(r2_score_avg_list)))\n",
    "print(\"\")\n",
    "print(\"Average Specific Humidity:\"+str(np.mean(actual_avg_list)))\n",
    "print(\"RMSE % of Specific Humidity:\"+str(np.mean(rmse2_avg_list)/np.mean(actual_avg_list)))\n",
    "\n",
    "# Plot model architecture\n",
    "filename = \"cnnlstm_model_optimized_SH_S1_No_GHI.png\"\n",
    "plot_model(best_model, to_file=filename, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e613575-b288-42f3-98dc-edb4b8408822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
