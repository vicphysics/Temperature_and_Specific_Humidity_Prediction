{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa89b532-a6f0-4709-b703-9f7169b81cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\n",
      "C:\\Users\\User\\Modeling\\\n",
      "Fold 0\n",
      "Train_X Fold 0:\n",
      "84\n",
      "180\n",
      "12\n",
      "Train_y Fold 0:\n",
      "84\n",
      "180\n",
      "4.89\n",
      "Validation_X Fold 0:\n",
      "21\n",
      "180\n",
      "12\n",
      "Validation_y Fold 0:\n",
      "21\n",
      "180\n",
      "12.21\n",
      "Fold 1\n",
      "Train_X Fold 1:\n",
      "84\n",
      "180\n",
      "12\n",
      "Train_y Fold 1:\n",
      "84\n",
      "180\n",
      "12.21\n",
      "Validation_X Fold 1:\n",
      "21\n",
      "180\n",
      "12\n",
      "Validation_y Fold 1:\n",
      "21\n",
      "180\n",
      "12.2\n",
      "Fold 2\n",
      "Train_X Fold 2:\n",
      "84\n",
      "180\n",
      "12\n",
      "Train_y Fold 2:\n",
      "84\n",
      "180\n",
      "12.21\n",
      "Validation_X Fold 2:\n",
      "21\n",
      "180\n",
      "12\n",
      "Validation_y Fold 2:\n",
      "21\n",
      "180\n",
      "8.61\n",
      "Fold 3\n",
      "Train_X Fold 3:\n",
      "84\n",
      "180\n",
      "12\n",
      "Train_y Fold 3:\n",
      "84\n",
      "180\n",
      "12.21\n",
      "Validation_X Fold 3:\n",
      "21\n",
      "180\n",
      "12\n",
      "Validation_y Fold 3:\n",
      "21\n",
      "180\n",
      "8.9\n",
      "Fold 4\n",
      "Train_X Fold 4:\n",
      "84\n",
      "180\n",
      "12\n",
      "Train_y Fold 4:\n",
      "84\n",
      "180\n",
      "12.21\n",
      "Validation_X Fold 4:\n",
      "21\n",
      "180\n",
      "12\n",
      "Validation_y Fold 4:\n",
      "21\n",
      "180\n",
      "4.89\n",
      "        0      1      2      3      4      5      6      7      8      9   \\\n",
      "0     4.89  15.38   8.61  12.20   6.28  17.82   2.68   8.35  10.82  13.94   \n",
      "1     1.26  13.87   4.71  10.03   3.18  16.52  -0.02   6.22   7.37  11.96   \n",
      "2     7.78  17.63  12.22  13.86   8.73  20.35   2.97  11.74  13.36  17.06   \n",
      "3    16.09  21.45  19.87  19.79  16.83  23.81   8.93  18.44  19.59  21.80   \n",
      "4    18.79  23.27  21.91  21.91  20.62  25.45  13.31  19.68  21.22  24.02   \n",
      "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "175  24.98  26.77  25.90  27.11  25.58  28.83  22.84  24.86  26.19  27.63   \n",
      "176  20.14  26.19  23.06  24.49  19.45  27.57  18.26  21.32  23.39  25.28   \n",
      "177  12.09  24.69  16.05  21.56  12.46  26.30  13.04  17.60  19.04  21.59   \n",
      "178   9.90  21.30  12.92  17.66   9.34  22.88   9.08  13.50  14.24  17.47   \n",
      "179   2.52  13.55   6.16   9.82   2.71  15.89   2.25   6.61   7.18  11.17   \n",
      "\n",
      "     ...     74     75     76     77     78     79     80     81     82     83  \n",
      "0    ...   1.74  -0.64  -0.58   5.18  -1.34   0.56   8.35  10.17  15.17  20.37  \n",
      "1    ...  -3.44  -5.06  -2.36   2.14  -5.58  -0.21   9.86  11.41  13.38  19.20  \n",
      "2    ...   1.09  -1.23   2.51   7.35  -2.11   4.57   9.50   9.10  17.13  21.82  \n",
      "3    ...   8.72   8.10   8.61  10.99   8.82  10.29  14.08  12.21  22.18  25.18  \n",
      "4    ...  12.81  13.08  14.28  15.68  13.11  14.59  20.55  16.30  24.75  26.08  \n",
      "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
      "175  ...  23.36  22.38  23.67  24.45  21.06  24.39  27.20  24.16  27.40  29.61  \n",
      "176  ...  16.57  14.75  18.32  19.01  13.95  19.15  23.73  22.34  26.58  28.58  \n",
      "177  ...   6.37   4.23  10.86  12.58   3.28   9.72  19.08  18.89  24.43  27.72  \n",
      "178  ...   4.03   1.57   3.24   5.54   1.06   2.72  10.36  12.47  21.03  25.53  \n",
      "179  ...   0.69  -2.44  -1.11   2.63  -2.72  -0.52   7.63  10.36  12.70  20.58  \n",
      "\n",
      "[180 rows x 84 columns]\n",
      "[     temperatures\n",
      "0            4.89\n",
      "1            1.26\n",
      "2            7.78\n",
      "3           16.09\n",
      "4           18.79\n",
      "..            ...\n",
      "175         24.98\n",
      "176         20.14\n",
      "177         12.09\n",
      "178          9.90\n",
      "179          2.52\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           15.38\n",
      "1           13.87\n",
      "2           17.63\n",
      "3           21.45\n",
      "4           23.27\n",
      "..            ...\n",
      "175         26.77\n",
      "176         26.19\n",
      "177         24.69\n",
      "178         21.30\n",
      "179         13.55\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            8.61\n",
      "1            4.71\n",
      "2           12.22\n",
      "3           19.87\n",
      "4           21.91\n",
      "..            ...\n",
      "175         25.90\n",
      "176         23.06\n",
      "177         16.05\n",
      "178         12.92\n",
      "179          6.16\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           12.20\n",
      "1           10.03\n",
      "2           13.86\n",
      "3           19.79\n",
      "4           21.91\n",
      "..            ...\n",
      "175         27.11\n",
      "176         24.49\n",
      "177         21.56\n",
      "178         17.66\n",
      "179          9.82\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            6.28\n",
      "1            3.18\n",
      "2            8.73\n",
      "3           16.83\n",
      "4           20.62\n",
      "..            ...\n",
      "175         25.58\n",
      "176         19.45\n",
      "177         12.46\n",
      "178          9.34\n",
      "179          2.71\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           17.82\n",
      "1           16.52\n",
      "2           20.35\n",
      "3           23.81\n",
      "4           25.45\n",
      "..            ...\n",
      "175         28.83\n",
      "176         27.57\n",
      "177         26.30\n",
      "178         22.88\n",
      "179         15.89\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            2.68\n",
      "1           -0.02\n",
      "2            2.97\n",
      "3            8.93\n",
      "4           13.31\n",
      "..            ...\n",
      "175         22.84\n",
      "176         18.26\n",
      "177         13.04\n",
      "178          9.08\n",
      "179          2.25\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            8.35\n",
      "1            6.22\n",
      "2           11.74\n",
      "3           18.44\n",
      "4           19.68\n",
      "..            ...\n",
      "175         24.86\n",
      "176         21.32\n",
      "177         17.60\n",
      "178         13.50\n",
      "179          6.61\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           10.82\n",
      "1            7.37\n",
      "2           13.36\n",
      "3           19.59\n",
      "4           21.22\n",
      "..            ...\n",
      "175         26.19\n",
      "176         23.39\n",
      "177         19.04\n",
      "178         14.24\n",
      "179          7.18\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           13.94\n",
      "1           11.96\n",
      "2           17.06\n",
      "3           21.80\n",
      "4           24.02\n",
      "..            ...\n",
      "175         27.63\n",
      "176         25.28\n",
      "177         21.59\n",
      "178         17.47\n",
      "179         11.17\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           11.49\n",
      "1            8.39\n",
      "2           14.48\n",
      "3           20.43\n",
      "4           22.29\n",
      "..            ...\n",
      "175         26.72\n",
      "176         24.62\n",
      "177         19.28\n",
      "178         15.28\n",
      "179          9.03\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           14.71\n",
      "1           12.69\n",
      "2           18.83\n",
      "3           23.18\n",
      "4           24.88\n",
      "..            ...\n",
      "175         29.02\n",
      "176         25.61\n",
      "177         21.07\n",
      "178         18.17\n",
      "179         11.99\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           13.42\n",
      "1           11.17\n",
      "2           17.59\n",
      "3           21.99\n",
      "4           23.71\n",
      "..            ...\n",
      "175         28.95\n",
      "176         25.37\n",
      "177         20.43\n",
      "178         17.33\n",
      "179         10.97\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           14.05\n",
      "1           11.41\n",
      "2           17.76\n",
      "3           22.34\n",
      "4           24.53\n",
      "..            ...\n",
      "175         29.72\n",
      "176         25.32\n",
      "177         20.85\n",
      "178         18.17\n",
      "179         12.06\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           12.14\n",
      "1            8.93\n",
      "2           15.61\n",
      "3           20.93\n",
      "4           23.63\n",
      "..            ...\n",
      "175         28.82\n",
      "176         22.88\n",
      "177         17.40\n",
      "178         14.91\n",
      "179          8.45\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           11.31\n",
      "1            8.32\n",
      "2           15.43\n",
      "3           20.99\n",
      "4           23.48\n",
      "..            ...\n",
      "175         28.80\n",
      "176         23.02\n",
      "177         17.29\n",
      "178         14.68\n",
      "179          8.40\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           11.69\n",
      "1            8.69\n",
      "2           15.84\n",
      "3           21.51\n",
      "4           24.24\n",
      "..            ...\n",
      "175         29.31\n",
      "176         23.01\n",
      "177         17.26\n",
      "178         14.50\n",
      "179          7.93\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           12.95\n",
      "1           12.07\n",
      "2           16.85\n",
      "3           22.98\n",
      "4           26.72\n",
      "..            ...\n",
      "175         30.10\n",
      "176         22.57\n",
      "177         19.73\n",
      "178         16.01\n",
      "179          9.26\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           10.56\n",
      "1            9.74\n",
      "2           14.43\n",
      "3           20.82\n",
      "4           25.48\n",
      "..            ...\n",
      "175         30.24\n",
      "176         22.51\n",
      "177         18.55\n",
      "178         14.67\n",
      "179          7.69\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            6.77\n",
      "1            9.55\n",
      "2           12.16\n",
      "3           18.20\n",
      "4           23.71\n",
      "..            ...\n",
      "175         29.42\n",
      "176         23.17\n",
      "177         17.57\n",
      "178         12.01\n",
      "179          4.36\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           12.04\n",
      "1           12.81\n",
      "2           10.78\n",
      "3           14.20\n",
      "4           18.47\n",
      "..            ...\n",
      "175         27.81\n",
      "176         26.21\n",
      "177         22.97\n",
      "178         16.25\n",
      "179         14.20\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            8.57\n",
      "1            6.56\n",
      "2           11.36\n",
      "3           17.83\n",
      "4           19.08\n",
      "..            ...\n",
      "175         25.52\n",
      "176         21.49\n",
      "177         17.57\n",
      "178         12.96\n",
      "179          5.96\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            9.12\n",
      "1            7.28\n",
      "2           12.40\n",
      "3           19.02\n",
      "4           20.01\n",
      "..            ...\n",
      "175         24.64\n",
      "176         21.22\n",
      "177         17.73\n",
      "178         13.58\n",
      "179          6.37\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            9.32\n",
      "1            6.96\n",
      "2           11.94\n",
      "3           18.54\n",
      "4           19.86\n",
      "..            ...\n",
      "175         24.99\n",
      "176         21.19\n",
      "177         17.37\n",
      "178         13.06\n",
      "179          5.81\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            8.90\n",
      "1            6.89\n",
      "2           11.30\n",
      "3           17.61\n",
      "4           18.97\n",
      "..            ...\n",
      "175         25.27\n",
      "176         21.30\n",
      "177         17.73\n",
      "178         13.31\n",
      "179          6.13\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            5.60\n",
      "1            2.56\n",
      "2            7.90\n",
      "3           15.21\n",
      "4           16.21\n",
      "..            ...\n",
      "175         23.58\n",
      "176         19.63\n",
      "177         14.97\n",
      "178          9.92\n",
      "179          2.86\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            7.69\n",
      "1            4.56\n",
      "2           11.66\n",
      "3           18.63\n",
      "4           20.74\n",
      "..            ...\n",
      "175         25.22\n",
      "176         21.92\n",
      "177         15.15\n",
      "178         11.48\n",
      "179          4.88\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            7.02\n",
      "1            3.15\n",
      "2           10.24\n",
      "3           17.27\n",
      "4           19.26\n",
      "..            ...\n",
      "175         24.50\n",
      "176         20.83\n",
      "177         13.72\n",
      "178         10.90\n",
      "179          4.35\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            7.64\n",
      "1            3.61\n",
      "2           10.44\n",
      "3           18.02\n",
      "4           19.36\n",
      "..            ...\n",
      "175         24.39\n",
      "176         20.81\n",
      "177         13.88\n",
      "178         11.33\n",
      "179          4.80\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            8.77\n",
      "1            5.33\n",
      "2           12.77\n",
      "3           19.58\n",
      "4           22.42\n",
      "..            ...\n",
      "175         27.53\n",
      "176         21.63\n",
      "177         15.20\n",
      "178         12.95\n",
      "179          5.56\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            8.93\n",
      "1            6.70\n",
      "2           14.14\n",
      "3           20.53\n",
      "4           23.53\n",
      "..            ...\n",
      "175         28.20\n",
      "176         21.55\n",
      "177         15.37\n",
      "178         12.49\n",
      "179          5.81\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            7.96\n",
      "1            5.46\n",
      "2           11.14\n",
      "3           18.74\n",
      "4           22.29\n",
      "..            ...\n",
      "175         29.11\n",
      "176         22.36\n",
      "177         15.76\n",
      "178         12.58\n",
      "179          5.31\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            7.65\n",
      "1            9.70\n",
      "2            9.44\n",
      "3           14.86\n",
      "4           21.87\n",
      "..            ...\n",
      "175         28.89\n",
      "176         24.56\n",
      "177         19.07\n",
      "178         10.42\n",
      "179          7.22\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            4.68\n",
      "1            2.16\n",
      "2            6.83\n",
      "3           13.61\n",
      "4           17.86\n",
      "..            ...\n",
      "175         25.29\n",
      "176         20.76\n",
      "177         15.22\n",
      "178         11.02\n",
      "179          4.01\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            6.76\n",
      "1            3.90\n",
      "2            7.91\n",
      "3           14.26\n",
      "4           18.07\n",
      "..            ...\n",
      "175         25.45\n",
      "176         21.17\n",
      "177         16.50\n",
      "178         11.98\n",
      "179          5.74\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            2.58\n",
      "1            0.49\n",
      "2            4.98\n",
      "3           12.00\n",
      "4           16.28\n",
      "..            ...\n",
      "175         25.10\n",
      "176         20.13\n",
      "177         14.09\n",
      "178          9.85\n",
      "179          3.26\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            6.57\n",
      "1            2.67\n",
      "2            7.39\n",
      "3           15.88\n",
      "4           16.99\n",
      "..            ...\n",
      "175         23.61\n",
      "176         19.55\n",
      "177         14.13\n",
      "178         10.21\n",
      "179          3.01\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            4.25\n",
      "1            0.65\n",
      "2            7.57\n",
      "3           16.24\n",
      "4           18.24\n",
      "..            ...\n",
      "175         24.34\n",
      "176         20.27\n",
      "177         12.54\n",
      "178          9.72\n",
      "179          2.55\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            4.21\n",
      "1           -0.09\n",
      "2            4.76\n",
      "3           13.41\n",
      "4           16.01\n",
      "..            ...\n",
      "175         23.19\n",
      "176         19.39\n",
      "177         13.18\n",
      "178          9.71\n",
      "179          2.12\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            5.33\n",
      "1            1.14\n",
      "2            6.25\n",
      "3           15.00\n",
      "4           17.08\n",
      "..            ...\n",
      "175         22.84\n",
      "176         19.21\n",
      "177         13.21\n",
      "178          9.27\n",
      "179          1.66\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            4.26\n",
      "1            0.29\n",
      "2            5.21\n",
      "3           13.86\n",
      "4           16.25\n",
      "..            ...\n",
      "175         23.24\n",
      "176         19.48\n",
      "177         12.64\n",
      "178          8.80\n",
      "179          1.29\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            3.62\n",
      "1           -0.32\n",
      "2            6.05\n",
      "3           14.63\n",
      "4           17.49\n",
      "..            ...\n",
      "175         22.76\n",
      "176         18.86\n",
      "177         11.52\n",
      "178          8.88\n",
      "179          1.00\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           -8.24\n",
      "1           -8.23\n",
      "2           -4.99\n",
      "3            1.49\n",
      "4            6.26\n",
      "..            ...\n",
      "175         13.64\n",
      "176          8.14\n",
      "177          4.40\n",
      "178         -2.79\n",
      "179         -8.49\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           -3.73\n",
      "1           -5.32\n",
      "2            0.66\n",
      "3            7.71\n",
      "4           12.86\n",
      "..            ...\n",
      "175         20.97\n",
      "176         13.33\n",
      "177          6.06\n",
      "178          1.14\n",
      "179         -7.30\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            1.95\n",
      "1           -1.30\n",
      "2            3.74\n",
      "3           11.19\n",
      "4           16.17\n",
      "..            ...\n",
      "175         24.41\n",
      "176         17.94\n",
      "177          9.36\n",
      "178          6.86\n",
      "179          0.03\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            9.57\n",
      "1           11.93\n",
      "2           12.22\n",
      "3           18.78\n",
      "4           27.14\n",
      "..            ...\n",
      "175         35.74\n",
      "176         30.93\n",
      "177         23.99\n",
      "178         15.18\n",
      "179          9.49\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            9.04\n",
      "1            9.30\n",
      "2            8.59\n",
      "3           11.94\n",
      "4           15.21\n",
      "..            ...\n",
      "175         19.96\n",
      "176         19.38\n",
      "177         17.16\n",
      "178         10.00\n",
      "179          7.95\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            3.20\n",
      "1            0.40\n",
      "2            3.78\n",
      "3           10.01\n",
      "4           14.33\n",
      "..            ...\n",
      "175         24.03\n",
      "176         19.69\n",
      "177         14.37\n",
      "178         10.30\n",
      "179          3.96\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            1.61\n",
      "1           -2.64\n",
      "2            1.57\n",
      "3            9.74\n",
      "4           14.16\n",
      "..            ...\n",
      "175         20.57\n",
      "176         16.58\n",
      "177         10.42\n",
      "178          7.19\n",
      "179         -0.37\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            3.41\n",
      "1            0.60\n",
      "2            5.66\n",
      "3           12.28\n",
      "4           16.43\n",
      "..            ...\n",
      "175         22.99\n",
      "176         18.02\n",
      "177         12.53\n",
      "178          8.25\n",
      "179          1.43\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            3.53\n",
      "1            0.26\n",
      "2            5.27\n",
      "3           11.85\n",
      "4           15.75\n",
      "..            ...\n",
      "175         23.54\n",
      "176         17.81\n",
      "177         13.04\n",
      "178          8.65\n",
      "179          1.46\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            0.81\n",
      "1           -2.67\n",
      "2            2.12\n",
      "3            9.84\n",
      "4           13.65\n",
      "..            ...\n",
      "175         21.44\n",
      "176         16.01\n",
      "177         10.39\n",
      "178          6.36\n",
      "179         -0.54\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            2.44\n",
      "1            0.26\n",
      "2            5.14\n",
      "3           11.66\n",
      "4           16.18\n",
      "..            ...\n",
      "175         23.37\n",
      "176         18.41\n",
      "177         12.64\n",
      "178          8.18\n",
      "179          1.23\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            1.35\n",
      "1           -2.14\n",
      "2            1.39\n",
      "3            8.55\n",
      "4           14.41\n",
      "..            ...\n",
      "175         22.11\n",
      "176         17.05\n",
      "177         11.04\n",
      "178          7.59\n",
      "179          0.58\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            3.02\n",
      "1           -0.83\n",
      "2            4.49\n",
      "3           11.84\n",
      "4           16.30\n",
      "..            ...\n",
      "175         21.47\n",
      "176         17.42\n",
      "177         11.59\n",
      "178          8.31\n",
      "179          1.15\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            2.48\n",
      "1           -1.68\n",
      "2            2.51\n",
      "3           10.73\n",
      "4           14.06\n",
      "..            ...\n",
      "175         21.27\n",
      "176         16.84\n",
      "177         11.05\n",
      "178          6.99\n",
      "179          0.11\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            5.34\n",
      "1            1.18\n",
      "2            5.35\n",
      "3           13.92\n",
      "4           16.26\n",
      "..            ...\n",
      "175         22.52\n",
      "176         18.77\n",
      "177         12.36\n",
      "178          8.24\n",
      "179          1.57\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            4.13\n",
      "1            0.14\n",
      "2            4.07\n",
      "3           12.27\n",
      "4           14.54\n",
      "..            ...\n",
      "175         22.13\n",
      "176         17.95\n",
      "177         12.27\n",
      "178          7.92\n",
      "179          1.07\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            2.67\n",
      "1           -1.08\n",
      "2            3.47\n",
      "3           11.76\n",
      "4           15.22\n",
      "..            ...\n",
      "175         22.50\n",
      "176         18.06\n",
      "177         11.32\n",
      "178          7.88\n",
      "179          0.99\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            5.25\n",
      "1            1.59\n",
      "2            8.07\n",
      "3           16.34\n",
      "4           18.64\n",
      "..            ...\n",
      "175         23.66\n",
      "176         19.95\n",
      "177         12.50\n",
      "178          9.85\n",
      "179          2.57\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            4.07\n",
      "1            0.19\n",
      "2            5.93\n",
      "3           14.44\n",
      "4           16.84\n",
      "..            ...\n",
      "175         22.81\n",
      "176         19.27\n",
      "177         12.27\n",
      "178          9.00\n",
      "179          1.44\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            2.47\n",
      "1           -1.56\n",
      "2            3.56\n",
      "3           11.73\n",
      "4           14.84\n",
      "..            ...\n",
      "175         22.39\n",
      "176         17.52\n",
      "177         10.23\n",
      "178          7.53\n",
      "179          0.78\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            0.98\n",
      "1           -2.72\n",
      "2            2.10\n",
      "3           10.23\n",
      "4           14.29\n",
      "..            ...\n",
      "175         20.87\n",
      "176         15.61\n",
      "177          9.28\n",
      "178          6.89\n",
      "179         -0.26\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            0.17\n",
      "1           -3.53\n",
      "2            2.86\n",
      "3           12.58\n",
      "4           16.41\n",
      "..            ...\n",
      "175         22.66\n",
      "176         16.84\n",
      "177          8.34\n",
      "178          5.35\n",
      "179         -2.72\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            1.13\n",
      "1           -2.80\n",
      "2            3.20\n",
      "3           13.00\n",
      "4           17.88\n",
      "..            ...\n",
      "175         22.79\n",
      "176         17.19\n",
      "177          8.35\n",
      "178          5.37\n",
      "179         -2.15\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           -1.49\n",
      "1           -4.44\n",
      "2            0.03\n",
      "3            7.70\n",
      "4           12.10\n",
      "..            ...\n",
      "175         20.25\n",
      "176         16.01\n",
      "177          9.53\n",
      "178          5.15\n",
      "179         -0.55\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           -4.15\n",
      "1           -6.29\n",
      "2           -1.02\n",
      "3            7.56\n",
      "4           14.14\n",
      "..            ...\n",
      "175         19.71\n",
      "176         14.29\n",
      "177          8.40\n",
      "178          4.96\n",
      "179         -2.01\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           -1.66\n",
      "1           -6.04\n",
      "2           -0.65\n",
      "3            6.92\n",
      "4           13.12\n",
      "..            ...\n",
      "175         19.98\n",
      "176         14.16\n",
      "177          6.96\n",
      "178          6.08\n",
      "179         -1.22\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           -2.76\n",
      "1           -8.76\n",
      "2           -1.53\n",
      "3            7.29\n",
      "4           11.75\n",
      "..            ...\n",
      "175         19.29\n",
      "176         13.01\n",
      "177          4.29\n",
      "178          3.04\n",
      "179         -4.79\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            1.02\n",
      "1           -3.48\n",
      "2            1.64\n",
      "3            9.30\n",
      "4           13.28\n",
      "..            ...\n",
      "175         22.44\n",
      "176         17.44\n",
      "177          8.96\n",
      "178          7.71\n",
      "179         -0.12\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           -0.18\n",
      "1           -5.34\n",
      "2            0.54\n",
      "3            8.81\n",
      "4           12.97\n",
      "..            ...\n",
      "175         21.70\n",
      "176         16.30\n",
      "177          7.98\n",
      "178          6.91\n",
      "179         -0.05\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            0.49\n",
      "1           -3.86\n",
      "2            2.52\n",
      "3           11.75\n",
      "4           15.70\n",
      "..            ...\n",
      "175         22.36\n",
      "176         16.04\n",
      "177          8.02\n",
      "178          5.76\n",
      "179         -2.34\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           -1.25\n",
      "1           -6.16\n",
      "2            0.48\n",
      "3           11.07\n",
      "4           15.78\n",
      "..            ...\n",
      "175         23.26\n",
      "176         17.00\n",
      "177          6.97\n",
      "178          4.01\n",
      "179         -3.39\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           -2.63\n",
      "1           -7.91\n",
      "2           -0.04\n",
      "3            9.66\n",
      "4           13.92\n",
      "..            ...\n",
      "175         20.19\n",
      "176         13.89\n",
      "177          4.82\n",
      "178          3.44\n",
      "179         -4.00\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            1.74\n",
      "1           -3.44\n",
      "2            1.09\n",
      "3            8.72\n",
      "4           12.81\n",
      "..            ...\n",
      "175         23.36\n",
      "176         16.57\n",
      "177          6.37\n",
      "178          4.03\n",
      "179          0.69\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           -0.64\n",
      "1           -5.06\n",
      "2           -1.23\n",
      "3            8.10\n",
      "4           13.08\n",
      "..            ...\n",
      "175         22.38\n",
      "176         14.75\n",
      "177          4.23\n",
      "178          1.57\n",
      "179         -2.44\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           -0.58\n",
      "1           -2.36\n",
      "2            2.51\n",
      "3            8.61\n",
      "4           14.28\n",
      "..            ...\n",
      "175         23.67\n",
      "176         18.32\n",
      "177         10.86\n",
      "178          3.24\n",
      "179         -1.11\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            5.18\n",
      "1            2.14\n",
      "2            7.35\n",
      "3           10.99\n",
      "4           15.68\n",
      "..            ...\n",
      "175         24.45\n",
      "176         19.01\n",
      "177         12.58\n",
      "178          5.54\n",
      "179          2.63\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           -1.34\n",
      "1           -5.58\n",
      "2           -2.11\n",
      "3            8.82\n",
      "4           13.11\n",
      "..            ...\n",
      "175         21.06\n",
      "176         13.95\n",
      "177          3.28\n",
      "178          1.06\n",
      "179         -2.72\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            0.56\n",
      "1           -0.21\n",
      "2            4.57\n",
      "3           10.29\n",
      "4           14.59\n",
      "..            ...\n",
      "175         24.39\n",
      "176         19.15\n",
      "177          9.72\n",
      "178          2.72\n",
      "179         -0.52\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0            8.35\n",
      "1            9.86\n",
      "2            9.50\n",
      "3           14.08\n",
      "4           20.55\n",
      "..            ...\n",
      "175         27.20\n",
      "176         23.73\n",
      "177         19.08\n",
      "178         10.36\n",
      "179          7.63\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           10.17\n",
      "1           11.41\n",
      "2            9.10\n",
      "3           12.21\n",
      "4           16.30\n",
      "..            ...\n",
      "175         24.16\n",
      "176         22.34\n",
      "177         18.89\n",
      "178         12.47\n",
      "179         10.36\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           15.17\n",
      "1           13.38\n",
      "2           17.13\n",
      "3           22.18\n",
      "4           24.75\n",
      "..            ...\n",
      "175         27.40\n",
      "176         26.58\n",
      "177         24.43\n",
      "178         21.03\n",
      "179         12.70\n",
      "\n",
      "[180 rows x 1 columns],      temperatures\n",
      "0           20.37\n",
      "1           19.20\n",
      "2           21.82\n",
      "3           25.18\n",
      "4           26.08\n",
      "..            ...\n",
      "175         29.61\n",
      "176         28.58\n",
      "177         27.72\n",
      "178         25.53\n",
      "179         20.58\n",
      "\n",
      "[180 rows x 1 columns]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'list'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 392\u001b[0m\n\u001b[0;32m    389\u001b[0m df_y[i] \u001b[38;5;241m=\u001b[39m df2\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_y[i])\n\u001b[1;32m--> 392\u001b[0m dataset_df\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    393\u001b[0m dataset\u001b[38;5;241m.\u001b[39mappend(dataset_df[i]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:380\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    378\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:446\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    443\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ndims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m sample, objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sample_object(objs, ndims, keys, names, levels)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# Standardize axis parameter to int\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:487\u001b[0m, in \u001b[0;36m_Concatenator._get_ndims\u001b[1;34m(self, objs)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001b[0;32m    483\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    484\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot concatenate object of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly Series and DataFrame objs are valid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         )\n\u001b[1;32m--> 487\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m    489\u001b[0m     ndims\u001b[38;5;241m.\u001b[39madd(obj\u001b[38;5;241m.\u001b[39mndim)\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ndims\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'list'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Activation, Dropout\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from numpy import array\n",
    "import keras.backend as K\n",
    "    \n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "    \n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    \n",
    "def mape (y_true, y_pred):\n",
    "    return 100*K.mean(K.sqrt(K.square(y_true - y_pred))/y_true)\n",
    "    \n",
    "def pearson (y_true, y_pred):\n",
    "    return (K.square(K.mean((y_true - K.mean(y_true))*(y_pred - K.mean(y_pred)))))/(K.mean(K.square(y_true - K.mean(y_true)))*K.mean(K.square(y_pred - K.mean(y_pred))))\n",
    "    \n",
    "# convert time series into a supervised learning problem\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def series_to_supervised(data, n_in=180, n_out=46, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list\n",
    "    \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    \n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(2)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(2)]\n",
    "    \n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "     \n",
    "# create a differenced series\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    "     \n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(data):\n",
    "    \n",
    "    #Prepare data for time series forecasting.\n",
    "    print(len(data))\n",
    "        \n",
    "    #Parameters:\n",
    "    #x (array-like): Input features.\n",
    "    #y (array-like): Target values.\n",
    "    #n_test (int): Number of test samples.\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations.\n",
    "        \n",
    "    #Returns:\n",
    "    #tuple: Training and test datasets.\n",
    "    \n",
    "    n_lag = 180 # Number of time steps to use for training\n",
    "    n_seq = 1 # Number of sequence observations\n",
    "    n_test = 47 #Number of test samples \n",
    "        \n",
    "    # transform data to be stationary\n",
    "    diff_series = difference(data, 1)\n",
    "    diff_values = []\n",
    "    for i in range(len(diff_series)):\n",
    "        diff_values.append([diff_series[i][0]])\n",
    "        \n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(diff_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "        \n",
    "    # split into train and test sets\n",
    "    supervised_train, supervised_test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "        \n",
    "    # rescale values to 0, 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(supervised_train)\n",
    "    train = scaler.transform(supervised_train)\n",
    "    test = scaler.transform(supervised_test)\n",
    "    \n",
    "    return scaler, train, test\n",
    "    \n",
    "def plot_kfold(cv, X, y, ax, n_splits, xlim_max=105):\n",
    "    \n",
    "    #Plots the indices for a cross-validation object.\n",
    "    #Taken from https://www.geeksforgeeks.org/cross-validation-using-k-fold-with-scikit-learn/\n",
    "    \n",
    "    #Parameters:\n",
    "    #cv: Cross-validation object\n",
    "    #X: Feature set\n",
    "    #y: Target variable\n",
    "    #ax: Matplotlib axis object\n",
    "    #n_splits: Number of folds in the cross-validation\n",
    "    #xlim_max: Maximum limit for the x-axis\n",
    "        \n",
    "    # Set color map for the plot\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "    cv_split = cv.split(X=X, y=y)\n",
    "        \n",
    "    for i_split, (train_idx, test_idx) in enumerate(cv_split):\n",
    "        # Create an array of NaNs and fill in training/testing indices\n",
    "        indices = np.full(len(X), np.nan)\n",
    "        indices[test_idx], indices[train_idx] = 1, 0\n",
    "            \n",
    "        # Plot the training and testing indices\n",
    "        ax_x = range(len(indices))\n",
    "        ax_y = [i_split + 0.5] * len(indices)\n",
    "        ax.scatter(ax_x, ax_y, c=indices, marker=\"_\", \n",
    "                   lw=10, cmap=cmap_cv, vmin=-0.2, vmax=1.2)\n",
    "    \n",
    "        # Set y-ticks and labels\n",
    "        y_ticks = np.arange(n_splits) + 0.5\n",
    "        ax.set(yticks=y_ticks, yticklabels=range(n_splits),\n",
    "               xlabel=\"Weather Station index (file_id)\", ylabel=\"Fold\",\n",
    "               ylim=[n_splits, -0.2], xlim=[0, xlim_max])\n",
    "    \n",
    "        # Set plot title and create legend\n",
    "        ax.set_title(\"KFold\", fontsize=14)\n",
    "        legend_patches = [Patch(color=cmap_cv(0.8), label=\"Testing set\"), \n",
    "                          Patch(color=cmap_cv(0.02), label=\"Training set\")]\n",
    "        ax.legend(handles=legend_patches, loc=(1.03, 0.8))\n",
    "    \n",
    "#Main\n",
    "    \n",
    "# Create 2D array with file_ids to use for sample creation\n",
    "array = np.array([\n",
    "    6501, 6541, 6640, 6668, 6678, \n",
    "    6687, 6697, 6714, 6744, 6772, \n",
    "    6783, 6840, 6844, 6854, 6870, \n",
    "    6891, 6895, 6899, 6901, 6909, \n",
    "    6929, 6950, 6963, 6969, 6994, \n",
    "    7032, 7057, 7094, 7095, 7100, \n",
    "    7108, 7116, 7119, 7131, 7139, \n",
    "    7152, 7155, 7156, 7182, 7193, \n",
    "    7202, 7239, 7280, 7286, 7287, \n",
    "    7311, 7321, 7329, 7347, 7350, \n",
    "    7354, 7357, 7361, 7414, 7423, \n",
    "    7424, 7432, 7463, 7482, 7489, \n",
    "    7528, 7531, 7534, 7538, 7549, \n",
    "    7553, 7555, 7562, 7571, 7573, \n",
    "    7574, 7575, 7585, 7599, 7603, \n",
    "    7606, 7622, 7652, 7671, 7704, \n",
    "    7786, 7805, 7816, 7838, 7861, \n",
    "    7862, 7863, 7870, 7892, 7907, \n",
    "    7938, 7962, 7979, 7987, 7999, \n",
    "    8000, 8034, 8083, 8120, 8133, \n",
    "    8184, 8186, 8247, 8248, 9858])\n",
    "    \n",
    "#Create arrays holding the 5-fold cross-validation indices gathered for consistency across models\n",
    "train_array = []\n",
    "test_array = []\n",
    "    \n",
    "train_array.append([1, 2, 3, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22, \n",
    "                        23, 24, 25, 27, 28, 29, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, \n",
    "                        43, 44, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, \n",
    "                        62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 81, \n",
    "                        82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 95, 97, 98, 100, 101, 102, 103])\n",
    "test_array.append([0, 4, 10, 12, 18, 26, 30, 31, 33, 45, 47, 53, 64, 65, 77, 80, 89, 94, 96, 99, 104])\n",
    "    \n",
    "train_array.append([0, 1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 17, 18, 19, 20, 21, 23, \n",
    "                        24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 36, 37, 38, 41, 43, 45, \n",
    "                        46, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 59, 60, 61, 63, 64, \n",
    "                        65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77, 80, 81, 82, 83, 84, \n",
    "                        86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104])\n",
    "test_array.append([5, 9, 11, 15, 16, 22, 28, 35, 39, 40, 42, 44, 55, 56, 62, 72, 76, 78, 79, 85, 103])\n",
    "    \n",
    "train_array.append([0, 1, 2, 4, 5, 9, 10, 11, 12, 14, 15, 16, 18, 20, 21, 22, 23, 26, \n",
    "                    28, 29, 30, 31, 32, 33, 35, 36, 37, 39, 40, 41, 42, 44, 45, 46, 47, \n",
    "                    48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, \n",
    "                    70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87, 88, 89, \n",
    "                    90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104])\n",
    "test_array.append([3, 6, 7, 8, 13, 17, 19, 24, 25, 27, 34, 38, 43, 49, 66, 67, 68, 69, 73, 81, 84])\n",
    "    \n",
    "train_array.append([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \n",
    "                        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, \n",
    "                        35, 37, 38, 39, 40, 42, 43, 44, 45, 47, 49, 51, 52, 53, 55, 56, \n",
    "                        60, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 76, 77, 78, 79, \n",
    "                        80, 81, 82, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 99, 102, 103, 104])\n",
    "test_array.append([32, 36, 41, 46, 48, 50, 54, 57, 58, 59, 61, 63, 70, 75, 83, 90, 91, 97, 98, 100, 101])\n",
    "    \n",
    "train_array.append([0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 22,\n",
    "                        24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, \n",
    "                        42, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55, 56, 57, 58, 59, \n",
    "                        61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 75, 76, 77, 78, \n",
    "                        79, 80, 81, 83, 84, 85, 89, 90, 91, 94, 96, 97, 98, 99, 100, 101, 103, 104])\n",
    "test_array.append([1, 2, 14, 20, 21, 23, 29, 37, 51, 52, 60, 71, 74, 82, 86, 87, 88, 92, 93, 95, 102])\n",
    "    \n",
    "# Equations for three Principal Components from PCA using response variables combined with other predictors\n",
    "#PC1=-0.0002714X1+0.02612X2+0.03858X3-0.007658X4+0.001592X5-0.02087X6+0.8564X7-0.1468X8+0.01192X9-0.0001049X10+0.01913X11+0.02076X12\n",
    "#PC2=0.0003944X1+0.002204X2+0.01052X3+0.3248X4-0.0009976X5-0.04421X6+2.3406X7+0.06103X8+0.08841X9+0.00009018X10+0.05678X11-0.002022X12\n",
    "#PC3=-0.00007998X1-0.0006124X2-0.001063X3-0.01855X4+0.00001956X5+0.01170X6+0.6076X7+0.4664X8-0.002995X9+0.008185X10+0.8815X11-0.0004730X12\n",
    "    \n",
    "# Equations for three Principal Components from PCA omitting both response variables,\n",
    "#PC-1=-0.0004514X1+0.03194X2-0.04343X3+0.002243X4-0.02252X5+0.9877X6-0.2265X7+0.006144X8-0.0001488X9+0.02943X10\n",
    "#PC-2=0.0001702X1+0.005484X2+0.2057X3-0.0003188X4-0.02584X5+1.6963X6-0.05890X7+0.05809X8+1.9748X9+0.03686X10\n",
    "#PC-3=-0.00006323X1-0.001180X2-0.02384X3-0.00002833X4+0.01170X5+0.5204X6+0.4791X7-0.004318X8+0.008271X9+0.8765X10\n",
    "    \n",
    "# Get the current working directory \n",
    "current_directory = os.getcwd() \n",
    "    \n",
    "# Print the current working directory \n",
    "print(current_directory)\n",
    "    \n",
    "# Define the directory containing the files \n",
    "path = current_directory+\"\\\\Modeling\\\\\"\n",
    "print(path)\n",
    "    \n",
    "filename = path + 'Final_Monthly_Dataset.csv'\n",
    "    \n",
    "# load dataset\n",
    "df = read_csv(filename, header=0, parse_dates=[0], index_col=0, date_format='%Y-%m')\n",
    "    \n",
    "df = df.rename(columns={'Unnamed: 0' : 'indices'})\n",
    "    \n",
    "#Remove unused columns\n",
    "df = df.drop(['Day', 'vapor_pressure'], axis=1)\n",
    "    \n",
    "# Round numbers in columns to reasonable precision,\n",
    "df['temperatures'] = np.round(df['temperatures'], 2)\n",
    "df['slp'] = np.round(df['slp'], 2)\n",
    "df['wet_bulb_temperature'] = np.round(df['wet_bulb_temperature'], 2)\n",
    "df['specific_humidity'] = np.round(df['specific_humidity'], 2)\n",
    "df['GHI'] = np.round(df['GHI'], 2)\n",
    "df['PRCP'] = np.round(df['PRCP'], 2)\n",
    "df['SNDP'] = np.round(df['SNDP'], 2)\n",
    "df['solar_activity'] = np.round(df['solar_activity'], 2)\n",
    "df['ONI'] = np.round(df['ONI'], 2)\n",
    "df['water'] = np.round(df['water'], 0)\n",
    "df['region'] = np.round(df['region'], 0)\n",
    "    \n",
    "df_trimmed = df[df['file_id'] != 7533] # Remove file_id 7533 so there are 105 weather stations for 5-fold CV\n",
    "df_trimmed = df_trimmed.drop(['Year', 'Month', 'date', 'latitude', 'longitude', 'elevation'], axis=1)\n",
    "    \n",
    "X = []\n",
    "y = []\n",
    "    \n",
    "for i in array:\n",
    "    add_to_X = [] # create list to store each column to add to X\n",
    "    new_df = df_trimmed[df_trimmed['file_id'] == i].drop(['file_id'], axis=1)\n",
    "    new_df = new_df.iloc[:180, :]\n",
    "    add_to_y = []\n",
    "    for j in range(new_df.shape[0]):\n",
    "        add_to_y.append(new_df['temperatures'].iloc[j])\n",
    "    y.append(add_to_y)\n",
    "    #new_df = new_df.drop(['temperatures'], axis=1)\n",
    "    columns_list = new_df.columns.tolist()\n",
    "    for j in range(new_df.shape[0]):\n",
    "        l=0\n",
    "        new_row = []\n",
    "        for m in columns_list:\n",
    "            new_row.append(new_df.iloc[j, l])\n",
    "            l += 1\n",
    "        add_to_X.append(new_row)\n",
    "    X.append(add_to_X)\n",
    "    \n",
    "#Perform k-fold cross-validation\n",
    "#Taken from: https://www.geeksforgeeks.org/cross-validation-using-k-fold-with-scikit-learn/\n",
    "    \n",
    "#k = 5  # Number of folds\n",
    "#kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "#for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "#    print(f\"Fold {i}:\")\n",
    "#    print(f\"  Training dataset index: {train_index}\")\n",
    "#    print(f\"  Test dataset index: {test_index}\")\n",
    "    \n",
    "#for train_indices, test_indices in kf.split(X):\n",
    "#    print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    \n",
    "# Create figure and axis\n",
    "#fig, ax = plt.subplots(figsize=(6, 3))\n",
    "#plot_kfold(kf, X, y, ax, k)\n",
    "#plt.tight_layout()\n",
    "#fig.subplots_adjust(right=0.6)\n",
    "    \n",
    "#Create train and test sets for each cross-validation split\n",
    "train_X = []\n",
    "train_y = []\n",
    "val_X = []\n",
    "val_y = []\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i}\")\n",
    "    #Add each corresponding sample for each entry of train index \n",
    "    train_X_rows = [] # Stores all the samples for one fold of train_X\n",
    "    train_y_rows = [] # Stores all the samples for one fold of train_y\n",
    "    for j in train_array[i]:\n",
    "        train_X_rows.append(X[j])\n",
    "        train_y_rows.append(y[j])\n",
    "    # Stores one fold of train dataset\n",
    "    train_X.append(train_X_rows)\n",
    "    train_y.append(train_y_rows)\n",
    "    #Add each corresponding sample for each entry of the validation index \n",
    "    val_X_rows = [] # Stores all the samples for one fold of val_X\n",
    "    val_y_rows = [] # Stores all the samples for one fold of val_y\n",
    "    for j in test_array[i]: \n",
    "            val_X_rows.append(X[j])\n",
    "            val_y_rows.append(y[j])\n",
    "    # Stores one fold of validation dataset\n",
    "    val_X.append(val_X_rows)\n",
    "    val_y.append(val_y_rows) \n",
    "    \n",
    "    print(\"Train_X Fold \"+str(i)+\":\")\n",
    "    print(len(train_X[i]))\n",
    "    print(len(train_X[i][0]))\n",
    "    print(len(train_X[i][0][0])) \n",
    "    print(\"Train_y Fold \"+str(i)+\":\")\n",
    "    print(len(train_y[i]))\n",
    "    print(len(train_y[i][0]))\n",
    "    print(train_y[i][0][0])\n",
    "    print(\"Validation_X Fold \"+str(i)+\":\")\n",
    "    print(len(val_X[i]))\n",
    "    print(len(val_X[i][0]))\n",
    "    print(len(val_X[i][0][0]))\n",
    "    print(\"Validation_y Fold \"+str(i)+\":\")\n",
    "    print(len(val_y[i]))\n",
    "    print(len(val_y[i][0]))\n",
    "    print(val_y[i][0][0])\n",
    "    \n",
    "#Convert 3D arrays to DataFrames\n",
    "df_X = []\n",
    "df_y = []\n",
    "dataset = []\n",
    "dataset_df = []\n",
    "scaler = []\n",
    "train = []\n",
    "test = []\n",
    "    \n",
    "for i in range(5):\n",
    "    df1 = []\n",
    "    df_X.append(pd.DataFrame(train_X[i]))\n",
    "    X_t = df_X[i].transpose()\n",
    "    for k in range(84):\n",
    "        X = np.array(X_t.iloc[:, k])\n",
    "        df = pd.DataFrame()\n",
    "        for j in range(180):\n",
    "            new_row = pd.DataFrame(X[j]).transpose()\n",
    "            new_row.columns = new_df.columns\n",
    "            # Add the new row\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.columns = new_df.columns\n",
    "        df1.append(df)\n",
    "    df_X[i] = df1\n",
    "\n",
    "    df2 = []\n",
    "    df_y.append(pd.DataFrame(train_y[i]))\n",
    "    y_t = df_y[i].transpose()\n",
    "    print(y_t)\n",
    "    for j in range(84):\n",
    "        y = np.array(y_t.iloc[:, j])\n",
    "        y = pd.DataFrame(y)\n",
    "        y.columns = ['temperatures']\n",
    "        df2.append(y)\n",
    "    df_y[i] = df2\n",
    "    print(df_y[i])\n",
    "    \n",
    "    #dataset_df.append(pd.concat([df_X[i], df_y[i]], axis=1))\n",
    "    #dataset.append(dataset_df[i].values)\n",
    "    #print(\"Dataset \"+str(i)+\":\")\n",
    "    #print(dataset[i])\n",
    "    \n",
    "    #Initialize scaler, train, test row containers\n",
    "    #scaler_rows = []\n",
    "    #train_rows = []\n",
    "    #test_rows = []\n",
    "    \n",
    "    # prepare data\n",
    "    scaler[i], train[i], test[i] = prepare_data(dataset[i])\n",
    "\n",
    "    #scaler.append(scaler_rows)\n",
    "    #train.append(train_rows)\n",
    "    #test.append(test_rows)\n",
    "    #train1 = train[i]\n",
    "    #test1 = test[i],\n",
    "    \n",
    "    #X_train = train1[:][:-1]\n",
    "    #y_train = train1[:][-1:]\n",
    "    #X_test = test1[:][:-1]\n",
    "    #y_test = test1[:][-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486c6d2-1927-44f2-a221-c0a3587ad155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))\n",
    "print(len(X[0]))\n",
    "print(len(X[0][0]))\n",
    "print(len(train_X))\n",
    "print(len(train_X[0]))\n",
    "print(len(train_X[0][0]))\n",
    "print(len(train_X[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f225f-b6ff-40ac-a583-7361f2cbda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train[:, :]))\n",
    "print(2001912/11)\n",
    "print(len(train_X))\n",
    "print(train_X[0][0][0])\n",
    "print(train_X[4][0][10])\n",
    "print(train_X[4][2002139][10])\n",
    "print(181992/24)\n",
    "fold_feature = []\n",
    "    for k in range(2002140):\n",
    "        fold_feature.append(dataset[0][k][0])\n",
    "print(fold_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ce180e-454f-448a-ac0f-9e410656f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387719d5-eab7-456d-93a2-ddfe9d9799c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an LSTM network to training data\n",
    "#Adapted from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def create_model(trial):\n",
    "    \n",
    "    #Parameters:\n",
    "    #trial (array-like): Optuna parameters.\n",
    "    #train (array-like): Target values.\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations\n",
    "    #nb_epoch (int): Maximum number of epochs\n",
    "    \n",
    "    \n",
    "    n_lag = 180 # Number of time steps to use for training\n",
    "    n_seq = 47 # Number of sequence observations\n",
    "    n_test = 21 # 105 Weather Stations/cross-validation splits=5 \n",
    "    n_batch = 84 # Number of batches\n",
    "    nb_epoch = 500 # Maximum number of epochs to try\n",
    "        \n",
    "    # Hyperparameters to be tuned by Optuna (taken from Javier Leon's dissertation 'Fruit Prices')\n",
    "    lr = trial.suggest_float('lr', 1e-3, 1e-1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=lr)\n",
    "    \n",
    "    #Optuna will try either Rectified Linear Unit (ReLU) = max(0, x), tanh, or sigmoid functions\n",
    "    activation_function = trial.suggest_categorical('activation_function', ['relu', 'tanh', 'sigmoid'])\n",
    "    \n",
    "    #filters = trial.suggest_categorical('filters', [32, 64, 128]) #Used for CNN-LSTM model later\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [32, 64, 128])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    \n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X1 = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    \n",
    "    print(X1.shape)\n",
    "        \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, return_sequences=True, batch_input_shape=(n_batch, X1.shape[1], X1.shape[2]), stateful=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(16, input_dim=y_train.shape[0], activation=activation_function))\n",
    "    model.add(Dense(8, activation=activation_function))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=rmse, optimizer=optimizer)\n",
    "        \n",
    "    return model\n",
    "    \n",
    "def objective(trial):\n",
    "    \n",
    "    accuracies = []\n",
    "        \n",
    "    n_batch=1\n",
    "    nb_epoch = 100\n",
    "    \n",
    "    for i in range(5):\n",
    "        train1 = train[i]\n",
    "        test1 = test[i]\n",
    "            \n",
    "        X_train = train1[:, :-1]\n",
    "        y_train = train1[:, -1:]\n",
    "        X_test = test1[:, :-1]\n",
    "        y_test = test1[:, -1:]\n",
    "    \n",
    "        model = create_model(trial)\n",
    "    \n",
    "        X1 = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "        history=model.fit(X1, y_train, epochs=trial.suggest_int('epochs', 10, 500), verbose=0)\n",
    "        accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "        # fit network\n",
    "        #for i in range(nb_epoch):\n",
    "        #    history = model.fit(X1, y_train, epochs=i, batch_size=n_batch, validation_data=(X_test, y_test), verbose=False, shuffle=False)\n",
    "        #    model.reset_states()\n",
    "        \n",
    "        loss = history.history['val_loss'][-1]\n",
    "    \n",
    "        # Plotting the training and validation loss\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        title1 = \\CNN-LSTM Training and Validation Loss for Fold \\ + str(i)\n",
    "        plt.title(title1)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Mean Squared Error Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "            \n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "        #history = model.fit(X_train, y_train, epochs=100, batch_size=1, validation_data=(X_test, y_test), verbose=False, shuffle=False)\n",
    "        #score = cross_val_score(model, X_train, y_train, cv=5, scoring=mean_quared_error)\n",
    "        #return score\n",
    "        \n",
    "    return np.mean(accuracies)\n",
    "    \n",
    "# optimize and fit model\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "    \n",
    "best_params = study.best_params\n",
    "print(\\Best hyperparameters:\\, best_params)\n",
    "best_model = create_model(optuna.trial.FixedTrial(best_params))\n",
    "best_model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
    "validation_data=(X_test, y_test))\n",
    "    \n",
    "#model = create_model(trial)\n",
    "    \n",
    "optuna.visualization.plot_optimization_history(study)\n",
    "    \n",
    "optuna.visualization.plot_parallel_coordinate(study)\n",
    "    \n",
    "optuna.visualization.plot_slice(study, params=['lr', 'optimizer', 'activation_function', 'lstm_units', 'dropout_rate'])\n",
    "    \n",
    "optuna.visualization.plot_param_importances(study)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d21e001-8860-4e76-a5bf-1ed688391baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one forecast with an LSTM,\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]\n",
    "     \n",
    "# evaluate the persistence model\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    "     \n",
    "# invert differenced forecast\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    # invert first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob)\n",
    "    # propagate difference forecast using inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted\n",
    "     \n",
    "# inverse data transform on forecasts\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i - 1\n",
    "        last_ob = series.values[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted\n",
    "     \n",
    "# evaluate the RMSE for each forecast time step\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    for i in range(n_seq):\n",
    "        actual = [row[i] for row in test]\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "        print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "     \n",
    "# plot the forecasts in the context of the original dataset\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "    # plot the entire dataset in blue\n",
    "    plt.plot(series.values)\n",
    "    # plot the forecasts in red\n",
    "    for i in range(len(forecasts)):\n",
    "        off_s = len(series) - n_test + i - 1\n",
    "        off_e = off_s + len(forecasts[i]) + 1\n",
    "        xaxis = [x for x in range(off_s, off_e)]\n",
    "        yaxis = [series.values[off_s]] + forecasts[i]\n",
    "        plt.plot(xaxis, yaxis, color='red')\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    \n",
    "# Predicting\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "# Evaluating the model\n",
    "mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
    "mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_rescaled, y_pred_rescaled)\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "print(f'R-squared Score (R2): {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e4e69-89f8-42b0-bc86-809bd3aca90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the learning curves\n",
    "history = best_model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
    "validation_data=(X_test, y_test))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Test')\n",
    "plt.title('LSTM Training and Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "    \n",
    "# Extract the dates corresponding to the test set predictions\n",
    "test_dates = df.index[-len(y_test_rescaled):]\n",
    "    \n",
    "# Create a DataFrame for plotting\n",
    "results_df = pd.DataFrame({\n",
    "'Date': test_dates,\n",
    "'Actual': y_test_rescaled.flatten(),\n",
    "'Predicted': y_pred_rescaled.flatten()\n",
    "})\n",
    "    \n",
    "# Plotting the results\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(results_df['Date'], results_df['Actual'], label='Actual Temp')\n",
    "plt.plot(results_df['Date'], results_df['Predicted'], label='Predicted Temp', alpha=0.7)\n",
    "plt.title('LSTM Model Comparison Temperature Prediction')\n",
    "    \n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62729405-988c-4153-b5c3-6dd85cb04127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "    \n",
    "# Define the create_model function with the optimal parameters\n",
    "best_model = create_model(optuna.trial.FixedTrial(best_params))\n",
    "# Plot model architecture\n",
    "plot_model(best_model, to_file='lstm_model_optimized.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "# make forecasts\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "# inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, forecasts, scaler, n_test+2)\n",
    "actual = [row[n_lag:] for row in test]\n",
    "actual = inverse_transform(series, actual, scaler, n_test+2)\n",
    "# evaluate forecasts\n",
    "evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "# plot forecasts\n",
    "plot_forecasts(series, forecasts, n_test+2)\n",
    "model.summary()\n",
    "    \n",
    "print(forecasts)\n",
    "    \n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
