{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db0008f5-916a-4ef8-a990-bd9896704778",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\n",
      "C:\\Users\\User\\Modeling\\\n",
      "     temperatures      slp  wet_bulb_temperature  specific_humidity  water  \\\n",
      "0           12.21  1018.53                  7.76               5.39    1.0   \n",
      "1            8.17  1021.23                  4.33               4.30    1.0   \n",
      "2           15.68  1018.97                 10.49               6.51    1.0   \n",
      "3           22.46  1014.69                 16.98              10.21    1.0   \n",
      "4           23.66  1014.24                 18.68              11.74    1.0   \n",
      "..            ...      ...                   ...                ...    ...   \n",
      "222         27.66  1016.01                 23.90              17.03    1.0   \n",
      "223         28.00  1016.53                 22.80              15.20    1.0   \n",
      "224         23.86  1013.56                 20.33              13.73    1.0   \n",
      "225         20.59  1020.43                 14.60               8.35    1.0   \n",
      "226         15.82  1017.05                 13.00               9.05    1.0   \n",
      "\n",
      "        GHI      WDSP  PRCP  SNDP  region  solar_activity   ONI  \n",
      "0    108.34  6.348387  0.23   0.0     4.0           20.90 -0.91  \n",
      "1    126.00  4.871429  0.17   0.0     4.0            5.71 -0.67  \n",
      "2    180.94  6.396774  0.17   0.0     4.0           17.29 -0.71  \n",
      "3    241.31  5.590000  0.11   0.0     4.0           50.27 -0.32  \n",
      "4    260.04  4.967742  0.11   0.0     4.0           37.23 -0.09  \n",
      "..      ...       ...   ...   ...     ...             ...   ...  \n",
      "222     NaN  3.596774  0.13   0.0     4.0          196.55  0.05  \n",
      "223     NaN  3.306452  0.03   0.0     4.0          215.52 -0.11  \n",
      "224     NaN  3.570000  0.15   0.0     4.0          141.37 -0.25  \n",
      "225     NaN  3.474194  0.00   0.0     4.0          166.39 -0.26  \n",
      "226     NaN  4.653333  0.17   0.0     4.0          152.47 -0.19  \n",
      "\n",
      "[227 rows x 12 columns]\n",
      "Agg:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "     var1(t-132)  var2(t-132)  var3(t-132)  var4(t-132)  var5(t-132)  \\\n",
      "132     0.279447     0.657553     0.313706     0.360825          0.0   \n",
      "133     0.830234     0.318524     0.800508     0.622522          0.0   \n",
      "134     0.795422     0.180451     0.817259     0.740682          0.0   \n",
      "135     0.529328     0.442242     0.574112     0.568596          0.0   \n",
      "136     0.647592     0.611073     0.629949     0.594766          0.0   \n",
      "137     0.602766     0.518113     0.572589     0.533703          0.0   \n",
      "138     0.394850     0.366370     0.498985     0.534496          0.0   \n",
      "139     0.177873     0.488038     0.213198     0.101507          0.0   \n",
      "140     0.238436     0.547505     0.263452     0.237906          0.0   \n",
      "141     0.210300     0.665755     0.253807     0.279144          0.0   \n",
      "142     0.319027     0.741627     0.340102     0.357653          0.0   \n",
      "143     0.374344     0.495557     0.389848     0.389374          0.0   \n",
      "144     0.498808     0.199590     0.461929     0.396511          0.0   \n",
      "145     0.932761     0.498975     0.937563     0.734338          0.0   \n",
      "146     0.418693     0.259057     0.436041     0.409199          0.0   \n",
      "147     0.789223     0.522215     0.845178     0.810468          0.0   \n",
      "148     0.642823     0.288448     0.652792     0.659001          0.0   \n",
      "149     0.452551     0.530417     0.530457     0.558287          0.0   \n",
      "150     0.649022     0.445660     0.546193     0.440127          0.0   \n",
      "151     0.222699     0.625427     0.317766     0.272006          0.0   \n",
      "152     0.214592     0.418319     0.216751     0.146709          0.0   \n",
      "153     0.195041     0.811347     0.211168     0.208565          0.0   \n",
      "154     0.307582     0.341763     0.350761     0.380650          0.0   \n",
      "155     0.283262     0.740260     0.273096     0.305313          0.0   \n",
      "156     0.650453     0.066302     0.659391     0.528945          0.0   \n",
      "157     0.663805     0.450444     0.661421     0.550357          0.0   \n",
      "158     0.641869     0.388927     0.657360     0.574941          0.0   \n",
      "159     0.723414     0.211893     0.741117     0.704203          0.0   \n",
      "160     0.675727     0.688312     0.692386     0.693894          0.0   \n",
      "161     0.553648     0.485988     0.533503     0.486915          0.0   \n",
      "162     0.367191     0.315789     0.471066     0.490880          0.0   \n",
      "163     0.307582     0.660971     0.345178     0.275971          0.0   \n",
      "164     0.189795     0.821599     0.163452     0.076130          0.0   \n",
      "165     0.204578     0.427204     0.201523     0.210151          0.0   \n",
      "166     0.320458     0.517430     0.354315     0.400476          0.0   \n",
      "167     0.392465     0.438141     0.390863     0.369548          0.0   \n",
      "168     0.683834     0.544771     0.668020     0.533703          0.0   \n",
      "169     0.591798     0.288448     0.636548     0.562252          0.0   \n",
      "170     0.662852     0.225564     0.652792     0.552736          0.0   \n",
      "171     0.674297     0.443609     0.770051     0.777161          0.0   \n",
      "172     0.744874     0.306220     0.700508     0.693894          0.0   \n",
      "173     0.432523     0.639098     0.479188     0.448057          0.0   \n",
      "174     0.460658     0.584416     0.487817     0.455194          0.0   \n",
      "175     0.341917     0.384142     0.386294     0.335448          0.0   \n",
      "176     0.115403     0.507177     0.134518     0.065821          0.0   \n",
      "177     0.336671     0.742994     0.304569     0.249009          0.0   \n",
      "178     0.134955     0.451811     0.151777     0.256146          0.0   \n",
      "179     0.399619     0.624744     0.398477     0.407613          0.0   \n",
      "\n",
      "     var6(t-132)  var7(t-132)  var8(t-132)  var9(t-132)  var10(t-132)  ...  \\\n",
      "132     0.551782     0.118135     0.461538          0.0           0.0  ...   \n",
      "133     0.740484     0.803567     0.519231          0.0           0.0  ...   \n",
      "134     0.767969     0.271139     0.461538          0.0           0.0  ...   \n",
      "135     0.557198     0.313265     0.519231          0.0           0.0  ...   \n",
      "136     0.668405     0.000000     0.461538          0.0           0.0  ...   \n",
      "137     0.370014     0.834064     0.480769          0.0           0.0  ...   \n",
      "138     0.330532     0.304354     0.605769          0.0           0.0  ...   \n",
      "139     0.356398     0.426949     0.528846          0.0           0.0  ...   \n",
      "140     0.156256     0.617741     0.548077          0.0           0.0  ...   \n",
      "141     0.258453     0.406304     0.490385          0.0           0.0  ...   \n",
      "142     0.339087     0.557376     0.634615          0.0           0.0  ...   \n",
      "143     0.400030     0.481104     0.442308          0.0           0.0  ...   \n",
      "144     0.694928     0.710616     0.423077          0.0           0.0  ...   \n",
      "145     0.722262     0.218451     0.538462          0.0           0.0  ...   \n",
      "146     0.689512     0.464337     0.615385          0.0           0.0  ...   \n",
      "147     0.583215     0.159100     0.442308          0.0           0.0  ...   \n",
      "148     0.547176     0.497379     0.500000          0.0           0.0  ...   \n",
      "149     0.234359     0.238735     0.730769          0.0           0.0  ...   \n",
      "150     0.676605     0.495833     0.269231          0.0           0.0  ...   \n",
      "151     0.067777     0.380283     0.586538          0.0           0.0  ...   \n",
      "152     0.286546     0.570142     0.500000          0.0           0.0  ...   \n",
      "153     0.260782     0.621325     0.557692          0.0           0.0  ...   \n",
      "154     0.295303     0.560347     0.538462          0.0           0.0  ...   \n",
      "155     0.528650     0.513508     0.461538          0.0           0.0  ...   \n",
      "156     0.616268     0.666386     0.596154          0.0           0.0  ...   \n",
      "157     0.725957     0.483618     0.471154          0.0           0.0  ...   \n",
      "158     0.642488     0.328264     0.644231          0.0           0.0  ...   \n",
      "159     0.659951     0.347461     0.413462          0.0           0.0  ...   \n",
      "160     0.630543     0.337936     0.538462          0.0           0.0  ...   \n",
      "161     0.440727     0.308331     0.384615          0.0           0.0  ...   \n",
      "162     0.103361     0.333076     0.740385          0.0           0.0  ...   \n",
      "163     0.344655     0.605099     0.548077          0.0           0.0  ...   \n",
      "164     0.393956     0.178149     0.278846          0.0           0.0  ...   \n",
      "165     0.138085     0.746965     0.596154          0.0           0.0  ...   \n",
      "166     0.268880     0.821347     0.634615          0.0           0.0  ...   \n",
      "167     0.618698     0.341913     0.365385          0.0           0.0  ...   \n",
      "168     0.626594     0.598174     0.500000          0.0           0.0  ...   \n",
      "169     0.563778     0.497332     0.653846          0.0           0.0  ...   \n",
      "170     0.823092     0.499589     0.500000          0.0           0.0  ...   \n",
      "171     0.327749     0.064195     0.721154          0.0           0.0  ...   \n",
      "172     0.943157     0.264290     0.173077          0.0           0.0  ...   \n",
      "173     0.154788     0.479189     0.663462          0.0           0.0  ...   \n",
      "174     0.520399     0.269004     0.403846          0.0           0.0  ...   \n",
      "175     0.027131     0.485375     0.759615          0.0           0.0  ...   \n",
      "176     0.184450     0.733856     0.836538          0.0           0.0  ...   \n",
      "177     0.520652     0.271164     0.000000          0.0           0.0  ...   \n",
      "178     0.259972     0.775000     0.692308          0.0           0.0  ...   \n",
      "179     0.587467     0.422187     0.451923          0.0           0.0  ...   \n",
      "\n",
      "     var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)  var7(t-1)  var8(t-1)  \\\n",
      "132   0.572081   0.498017        0.0   0.516603   0.581999   0.509615   \n",
      "133   0.609137   0.507534        0.0   0.666127   0.488863   0.557692   \n",
      "134   0.554822   0.474227        0.0   0.735473   0.494702   0.471154   \n",
      "135   0.723858   0.671689        0.0   0.652308   0.344908   0.663462   \n",
      "136   0.568528   0.543220        0.0   0.607309   0.299886   0.538462   \n",
      "137   0.694416   0.733545        0.0   0.513920   0.314738   0.451923   \n",
      "138   0.611675   0.622522        0.0   0.576888   0.329320   0.365385   \n",
      "139   0.443655   0.404441        0.0   0.018273   0.422923   0.788462   \n",
      "140   0.318782   0.170500        0.0   0.564841   0.286507   0.278846   \n",
      "141   0.226904   0.170500        0.0   0.214466   0.708841   0.490385   \n",
      "142   0.281218   0.278351        0.0   0.194675   0.492519   0.519231   \n",
      "143   0.182234   0.220460        0.0   0.323244   0.543334   0.701923   \n",
      "144   0.321320   0.373513        0.0   0.594199   0.565060   0.375000   \n",
      "145   0.853299   0.696273        0.0   0.274094   0.623687   1.000000   \n",
      "146   0.579188   0.436955        0.0   1.000000   0.479920   0.192308   \n",
      "147   0.517766   0.468676        0.0   0.630694   0.381215   0.471154   \n",
      "148   1.000000   1.000000        0.0   0.750557   0.106713   0.384615   \n",
      "149   0.623858   0.663759        0.0   0.533661   0.555854   0.701923   \n",
      "150   0.539086   0.559080        0.0   0.276422   0.167004   0.365385   \n",
      "151   0.426396   0.331483        0.0   0.444219   0.607775   0.519231   \n",
      "152   0.427919   0.388580        0.0   0.036799   0.324091   0.557692   \n",
      "153   0.182234   0.067407        0.0   0.385149   0.644008   0.557692   \n",
      "154   0.038071   0.046788        0.0   0.220541   0.629647   0.500000   \n",
      "155   0.439594   0.403648        0.0   0.355689   0.428300   0.673077   \n",
      "156   0.386802   0.374306        0.0   0.499038   0.507616   0.384615   \n",
      "157   0.669543   0.590801        0.0   0.426250   0.570636   0.692308   \n",
      "158   0.477665   0.386201        0.0   0.928579   0.388626   0.394231   \n",
      "159   0.785279   0.681998        0.0   0.604424   0.484958   0.673077   \n",
      "160   0.789848   0.789056        0.0   0.678174   0.303445   0.365385   \n",
      "161   0.579188   0.574148        0.0   0.549251   0.323354   0.509615   \n",
      "162   0.585279   0.606661        0.0   0.496609   0.529121   0.519231   \n",
      "163   0.509645   0.452815        0.0   0.396993   0.112138   0.346154   \n",
      "164   0.428934   0.302934        0.0   0.361460   0.498705   0.567308   \n",
      "165   0.075635   0.023791        0.0   0.000000   0.654245   0.682692   \n",
      "166   0.082234   0.121332        0.0   0.317676   0.510587   0.384615   \n",
      "167   0.505076   0.481364        0.0   0.295455   0.409643   0.567308   \n",
      "168   0.473604   0.440127        0.0   0.412584   0.612930   0.567308   \n",
      "169   0.476142   0.418715        0.0   0.670530   0.685103   0.596154   \n",
      "170   0.822335   0.708168        0.0   0.498279   0.498042   0.596154   \n",
      "171   0.487310   0.419508        0.0   0.951863   0.216273   0.538462   \n",
      "172   0.718274   0.682791        0.0   0.678629   0.379178   0.230769   \n",
      "173   0.710660   0.745440        0.0   0.504707   0.463748   0.615385   \n",
      "174   0.634010   0.695480        0.0   0.512756   0.210504   0.519231   \n",
      "175   0.387817   0.268834        0.0   0.346123   0.485522   0.490385   \n",
      "176   0.372081   0.317209        0.0   0.117129   0.533736   0.567308   \n",
      "177   0.159391   0.065821        0.0   0.248684   0.468240   0.509615   \n",
      "178   0.309645   0.269627        0.0   0.419771   0.419585   0.423077   \n",
      "179   0.224873   0.270420        0.0   0.249899   0.733365   0.644231   \n",
      "\n",
      "     var9(t-1)  var10(t-1)  var11(t-1)  var12(t-1)  \n",
      "132        0.0         0.0    0.509105    0.614173  \n",
      "133        0.0         0.0    0.441430    0.834646  \n",
      "134        0.0         0.0    0.355829    0.645669  \n",
      "135        0.0         0.0    0.575183    0.700787  \n",
      "136        0.0         0.0    0.312559    0.677165  \n",
      "137        0.0         0.0    0.440680    0.535433  \n",
      "138        0.0         0.0    0.424629    0.496063  \n",
      "139        0.0         0.0    0.576591    0.236220  \n",
      "140        0.0         0.0    0.542332    0.346457  \n",
      "141        0.0         0.0    0.151586    0.456693  \n",
      "142        0.0         0.0    0.367655    0.244094  \n",
      "143        0.0         0.0    0.461047    0.535433  \n",
      "144        0.0         0.0    0.425286    0.566929  \n",
      "145        0.0         0.0    0.474282    0.716535  \n",
      "146        0.0         0.0    0.360710    0.543307  \n",
      "147        0.0         0.0    0.498780    0.787402  \n",
      "148        0.0         0.0    0.477379    0.803150  \n",
      "149        0.0         0.0    0.460860    0.755906  \n",
      "150        0.0         0.0    0.306645    0.614173  \n",
      "151        0.0         0.0    0.504881    0.535433  \n",
      "152        0.0         0.0    0.387460    0.858268  \n",
      "153        0.0         0.0    0.453069    0.897638  \n",
      "154        0.0         0.0    0.437864    0.559055  \n",
      "155        0.0         0.0    0.421063    0.551181  \n",
      "156        0.0         0.0    0.480946    0.370079  \n",
      "157        0.0         0.0    0.373287    0.606299  \n",
      "158        0.0         0.0    0.518678    0.629921  \n",
      "159        0.0         0.0    0.434672    0.417323  \n",
      "160        0.0         0.0    0.445748    0.503937  \n",
      "161        0.0         0.0    0.356017    0.480315  \n",
      "162        0.0         0.0    0.435142    0.480315  \n",
      "163        0.0         0.0    0.434297    0.314961  \n",
      "164        0.0         0.0    0.443777    0.559055  \n",
      "165        0.0         0.0    0.431293    0.905512  \n",
      "166        0.0         0.0    0.438708    0.590551  \n",
      "167        0.0         0.0    0.447813    0.559055  \n",
      "168        0.0         0.0    0.481791    0.622047  \n",
      "169        0.0         0.0    0.381828    0.377953  \n",
      "170        0.0         0.0    0.449878    0.645669  \n",
      "171        0.0         0.0    0.472874    0.464567  \n",
      "172        0.0         0.0    0.390933    0.062992  \n",
      "173        0.0         0.0    0.490895    0.503937  \n",
      "174        0.0         0.0    0.440116    0.590551  \n",
      "175        0.0         0.0    0.451286    0.330709  \n",
      "176        0.0         0.0    0.373381    0.370079  \n",
      "177        0.0         0.0    0.569833    0.228346  \n",
      "178        0.0         0.0    0.624554    0.425197  \n",
      "179        0.0         0.0    0.330956    0.771654  \n",
      "\n",
      "[48 rows x 1584 columns]\n",
      "Test columns:\n",
      "     var12(t-1)\n",
      "132    0.614173\n",
      "133    0.834646\n",
      "134    0.645669\n",
      "135    0.700787\n",
      "136    0.677165\n",
      "137    0.535433\n",
      "138    0.496063\n",
      "139    0.236220\n",
      "140    0.346457\n",
      "141    0.456693\n",
      "142    0.244094\n",
      "143    0.535433\n",
      "144    0.566929\n",
      "145    0.716535\n",
      "146    0.543307\n",
      "147    0.787402\n",
      "148    0.803150\n",
      "149    0.755906\n",
      "150    0.614173\n",
      "151    0.535433\n",
      "152    0.858268\n",
      "153    0.897638\n",
      "154    0.559055\n",
      "155    0.551181\n",
      "156    0.370079\n",
      "157    0.606299\n",
      "158    0.629921\n",
      "159    0.417323\n",
      "160    0.503937\n",
      "161    0.480315\n",
      "162    0.480315\n",
      "163    0.314961\n",
      "164    0.559055\n",
      "165    0.905512\n",
      "166    0.590551\n",
      "167    0.559055\n",
      "168    0.622047\n",
      "169    0.377953\n",
      "170    0.645669\n",
      "171    0.464567\n",
      "172    0.062992\n",
      "173    0.503937\n",
      "174    0.590551\n",
      "175    0.330709\n",
      "176    0.370079\n",
      "177    0.228346\n",
      "178    0.425197\n",
      "179    0.771654\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Train:\n",
      "(12, 1584)\n",
      "[[0.27944683 0.65755297 0.31370558 ... 0.         0.50910456 0.61417323]\n",
      " [0.83023367 0.31852358 0.80050761 ... 0.         0.44143045 0.83464567]\n",
      " [0.79542203 0.18045113 0.81725888 ... 0.         0.3558288  0.64566929]\n",
      " ...\n",
      " [0.21030043 0.6657553  0.25380711 ... 0.         0.15158626 0.45669291]\n",
      " [0.31902718 0.74162679 0.34010152 ... 0.         0.36765534 0.24409449]\n",
      " [0.3743443  0.49555707 0.38984772 ... 0.         0.46104749 0.53543307]]\n",
      "Test:\n",
      "(36, 1584)\n",
      "[[0.49880782 0.19958988 0.46192893 ... 0.         0.42528628 0.56692913]\n",
      " [0.93276109 0.49897471 0.93756345 ... 0.         0.47428196 0.71653543]\n",
      " [0.41869337 0.25905673 0.43604061 ... 0.         0.36070959 0.54330709]\n",
      " ...\n",
      " [0.33667144 0.74299385 0.30456853 ... 0.         0.56983293 0.22834646]\n",
      " [0.1349547  0.45181135 0.15177665 ... 0.         0.62455416 0.42519685]\n",
      " [0.3996185  0.62474368 0.39847716 ... 0.         0.33095551 0.77165354]]\n",
      "12\n",
      "12\n",
      "1\n",
      "1583\n",
      "(12, 1, 1583)\n",
      "(12, 1)\n",
      "[[0.61417323]\n",
      " [0.83464567]\n",
      " [0.64566929]\n",
      " [0.7007874 ]\n",
      " [0.67716535]\n",
      " [0.53543307]\n",
      " [0.49606299]\n",
      " [0.23622047]\n",
      " [0.34645669]\n",
      " [0.45669291]\n",
      " [0.24409449]\n",
      " [0.53543307]]\n",
      "(12, 1584)\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.5626 - accuracy: 0.0000e+00 - mae: 0.5344 - rmse: 0.5626 - mape: 101.6431 - pearson: nan\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5388 - accuracy: 0.0000e+00 - mae: 0.5092 - rmse: 0.5388 - mape: 96.0989 - pearson: nan\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5157 - accuracy: 0.0000e+00 - mae: 0.4847 - rmse: 0.5157 - mape: 90.7088 - pearson: nan\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4960 - accuracy: 0.0000e+00 - mae: 0.4637 - rmse: 0.4960 - mape: 86.0802 - pearson: nan\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4825 - accuracy: 0.0000e+00 - mae: 0.4492 - rmse: 0.4825 - mape: 82.8766 - pearson: nan\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4746 - accuracy: 0.0000e+00 - mae: 0.4407 - rmse: 0.4746 - mape: 81.0099 - pearson: nan\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4658 - accuracy: 0.0000e+00 - mae: 0.4312 - rmse: 0.4658 - mape: 78.9152 - pearson: nan\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4562 - accuracy: 0.0000e+00 - mae: 0.4208 - rmse: 0.4562 - mape: 76.6362 - pearson: nan\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4479 - accuracy: 0.0000e+00 - mae: 0.4118 - rmse: 0.4479 - mape: 74.6496 - pearson: nan\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4392 - accuracy: 0.0000e+00 - mae: 0.4024 - rmse: 0.4392 - mape: 72.5610 - pearson: nan\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4306 - accuracy: 0.0000e+00 - mae: 0.3929 - rmse: 0.4306 - mape: 70.4856 - pearson: nan\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4232 - accuracy: 0.0000e+00 - mae: 0.3848 - rmse: 0.4232 - mape: 68.6975 - pearson: nan\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4137 - accuracy: 0.0000e+00 - mae: 0.3744 - rmse: 0.4137 - mape: 66.3951 - pearson: nan\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4052 - accuracy: 0.0000e+00 - mae: 0.3649 - rmse: 0.4052 - mape: 64.3136 - pearson: nan\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3938 - accuracy: 0.0000e+00 - mae: 0.3522 - rmse: 0.3938 - mape: 61.5039 - pearson: nan\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3842 - accuracy: 0.0000e+00 - mae: 0.3414 - rmse: 0.3842 - mape: 59.1366 - pearson: nan\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3736 - accuracy: 0.0000e+00 - mae: 0.3295 - rmse: 0.3736 - mape: 56.5168 - pearson: nan\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3635 - accuracy: 0.0000e+00 - mae: 0.3180 - rmse: 0.3635 - mape: 53.9821 - pearson: nan\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3528 - accuracy: 0.0000e+00 - mae: 0.3057 - rmse: 0.3528 - mape: 51.2730 - pearson: nan\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3416 - accuracy: 0.0000e+00 - mae: 0.2926 - rmse: 0.3416 - mape: 48.3914 - pearson: nan\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3313 - accuracy: 0.0000e+00 - mae: 0.2827 - rmse: 0.3313 - mape: 46.6030 - pearson: nan\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3196 - accuracy: 0.0000e+00 - mae: 0.2734 - rmse: 0.3196 - mape: 45.4714 - pearson: nan\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3067 - accuracy: 0.0000e+00 - mae: 0.2629 - rmse: 0.3067 - mape: 44.1925 - pearson: nan\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2946 - accuracy: 0.0000e+00 - mae: 0.2530 - rmse: 0.2946 - mape: 42.9822 - pearson: nan\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2833 - accuracy: 0.0000e+00 - mae: 0.2435 - rmse: 0.2833 - mape: 41.8147 - pearson: nan\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2702 - accuracy: 0.0000e+00 - mae: 0.2322 - rmse: 0.2702 - mape: 40.4369 - pearson: nan\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2569 - accuracy: 0.0000e+00 - mae: 0.2202 - rmse: 0.2569 - mape: 38.9737 - pearson: nan\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2438 - accuracy: 0.0000e+00 - mae: 0.2100 - rmse: 0.2438 - mape: 38.0468 - pearson: nan\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2323 - accuracy: 0.0000e+00 - mae: 0.2014 - rmse: 0.2323 - mape: 37.4751 - pearson: nan\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2209 - accuracy: 0.0000e+00 - mae: 0.1923 - rmse: 0.2209 - mape: 36.8691 - pearson: nan\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2104 - accuracy: 0.0000e+00 - mae: 0.1832 - rmse: 0.2104 - mape: 36.2624 - pearson: nan\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2003 - accuracy: 0.0000e+00 - mae: 0.1733 - rmse: 0.2003 - mape: 35.6023 - pearson: nan\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1917 - accuracy: 0.0000e+00 - mae: 0.1634 - rmse: 0.1917 - mape: 34.9426 - pearson: nan\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1847 - accuracy: 0.0000e+00 - mae: 0.1559 - rmse: 0.1847 - mape: 34.8105 - pearson: nan\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1798 - accuracy: 0.0000e+00 - mae: 0.1494 - rmse: 0.1798 - mape: 34.8720 - pearson: nan\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1770 - accuracy: 0.0000e+00 - mae: 0.1453 - rmse: 0.1770 - mape: 35.3954 - pearson: nan\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1423 - rmse: 0.1761 - mape: 36.0672 - pearson: nan\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1771 - accuracy: 0.0000e+00 - mae: 0.1427 - rmse: 0.1771 - mape: 37.2966 - pearson: nan\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1792 - accuracy: 0.0000e+00 - mae: 0.1451 - rmse: 0.1792 - mape: 38.7365 - pearson: nan\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1818 - accuracy: 0.0000e+00 - mae: 0.1472 - rmse: 0.1818 - mape: 39.9472 - pearson: nan\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1843 - accuracy: 0.0000e+00 - mae: 0.1487 - rmse: 0.1843 - mape: 40.8462 - pearson: nan\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1861 - accuracy: 0.0000e+00 - mae: 0.1497 - rmse: 0.1861 - mape: 41.4342 - pearson: nan\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1870 - accuracy: 0.0000e+00 - mae: 0.1501 - rmse: 0.1870 - mape: 41.7144 - pearson: nan\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1871 - accuracy: 0.0000e+00 - mae: 0.1501 - rmse: 0.1871 - mape: 41.7230 - pearson: nan\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1863 - accuracy: 0.0000e+00 - mae: 0.1498 - rmse: 0.1863 - mape: 41.4871 - pearson: nan\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1848 - accuracy: 0.0000e+00 - mae: 0.1490 - rmse: 0.1848 - mape: 41.0237 - pearson: nan\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1829 - accuracy: 0.0000e+00 - mae: 0.1479 - rmse: 0.1829 - mape: 40.3806 - pearson: nan\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1810 - accuracy: 0.0000e+00 - mae: 0.1466 - rmse: 0.1810 - mape: 39.5944 - pearson: nan\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1791 - accuracy: 0.0000e+00 - mae: 0.1451 - rmse: 0.1791 - mape: 38.7154 - pearson: nan\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1777 - accuracy: 0.0000e+00 - mae: 0.1435 - rmse: 0.1777 - mape: 37.7838 - pearson: nan\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1767 - accuracy: 0.0000e+00 - mae: 0.1419 - rmse: 0.1767 - mape: 36.8396 - pearson: nan\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1418 - rmse: 0.1762 - mape: 36.1809 - pearson: nan\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1432 - rmse: 0.1762 - mape: 35.8638 - pearson: nan\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1765 - accuracy: 0.0000e+00 - mae: 0.1445 - rmse: 0.1765 - mape: 35.5831 - pearson: nan\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1771 - accuracy: 0.0000e+00 - mae: 0.1456 - rmse: 0.1771 - mape: 35.3468 - pearson: nan\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1777 - accuracy: 0.0000e+00 - mae: 0.1464 - rmse: 0.1777 - mape: 35.1660 - pearson: nan\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1782 - accuracy: 0.0000e+00 - mae: 0.1469 - rmse: 0.1782 - mape: 35.0409 - pearson: nan\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1784 - accuracy: 0.0000e+00 - mae: 0.1472 - rmse: 0.1784 - mape: 34.9754 - pearson: nan\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1785 - accuracy: 0.0000e+00 - mae: 0.1473 - rmse: 0.1785 - mape: 34.9636 - pearson: nan\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1783 - accuracy: 0.0000e+00 - mae: 0.1471 - rmse: 0.1783 - mape: 35.0013 - pearson: nan\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1780 - accuracy: 0.0000e+00 - mae: 0.1468 - rmse: 0.1780 - mape: 35.0814 - pearson: nan\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1776 - accuracy: 0.0000e+00 - mae: 0.1462 - rmse: 0.1776 - mape: 35.1973 - pearson: nan\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1771 - accuracy: 0.0000e+00 - mae: 0.1456 - rmse: 0.1771 - mape: 35.3402 - pearson: nan\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1767 - accuracy: 0.0000e+00 - mae: 0.1449 - rmse: 0.1767 - mape: 35.5008 - pearson: nan\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1764 - accuracy: 0.0000e+00 - mae: 0.1441 - rmse: 0.1764 - mape: 35.6698 - pearson: nan\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1433 - rmse: 0.1762 - mape: 35.8401 - pearson: nan\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1426 - rmse: 0.1761 - mape: 36.0026 - pearson: nan\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1419 - rmse: 0.1762 - mape: 36.1506 - pearson: nan\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1763 - accuracy: 0.0000e+00 - mae: 0.1413 - rmse: 0.1763 - mape: 36.2778 - pearson: nan\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1764 - accuracy: 0.0000e+00 - mae: 0.1413 - rmse: 0.1764 - mape: 36.4630 - pearson: nan\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1765 - accuracy: 0.0000e+00 - mae: 0.1416 - rmse: 0.1765 - mape: 36.6689 - pearson: nan\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1766 - accuracy: 0.0000e+00 - mae: 0.1419 - rmse: 0.1766 - mape: 36.7956 - pearson: nan\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1767 - accuracy: 0.0000e+00 - mae: 0.1419 - rmse: 0.1767 - mape: 36.8459 - pearson: nan\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1767 - accuracy: 0.0000e+00 - mae: 0.1419 - rmse: 0.1767 - mape: 36.8246 - pearson: nan\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1766 - accuracy: 0.0000e+00 - mae: 0.1418 - rmse: 0.1766 - mape: 36.7415 - pearson: nan\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1765 - accuracy: 0.0000e+00 - mae: 0.1415 - rmse: 0.1765 - mape: 36.6037 - pearson: nan\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1764 - accuracy: 0.0000e+00 - mae: 0.1412 - rmse: 0.1764 - mape: 36.4210 - pearson: nan\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1763 - accuracy: 0.0000e+00 - mae: 0.1413 - rmse: 0.1763 - mape: 36.2881 - pearson: nan\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1417 - rmse: 0.1762 - mape: 36.2033 - pearson: nan\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1421 - rmse: 0.1762 - mape: 36.1170 - pearson: nan\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1424 - rmse: 0.1761 - mape: 36.0356 - pearson: nan\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1428 - rmse: 0.1761 - mape: 35.9592 - pearson: nan\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1431 - rmse: 0.1762 - mape: 35.8947 - pearson: nan\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1433 - rmse: 0.1762 - mape: 35.8397 - pearson: nan\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1435 - rmse: 0.1762 - mape: 35.8041 - pearson: nan\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1763 - accuracy: 0.0000e+00 - mae: 0.1436 - rmse: 0.1763 - mape: 35.7791 - pearson: nan\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1763 - accuracy: 0.0000e+00 - mae: 0.1436 - rmse: 0.1763 - mape: 35.7740 - pearson: nan\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1763 - accuracy: 0.0000e+00 - mae: 0.1436 - rmse: 0.1763 - mape: 35.7795 - pearson: nan\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1435 - rmse: 0.1762 - mape: 35.7978 - pearson: nan\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1434 - rmse: 0.1762 - mape: 35.8243 - pearson: nan\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1432 - rmse: 0.1762 - mape: 35.8640 - pearson: nan\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1430 - rmse: 0.1762 - mape: 35.9046 - pearson: nan\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1428 - rmse: 0.1762 - mape: 35.9496 - pearson: nan\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1426 - rmse: 0.1761 - mape: 35.9933 - pearson: nan\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1424 - rmse: 0.1761 - mape: 36.0337 - pearson: nan\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1423 - rmse: 0.1761 - mape: 36.0713 - pearson: nan\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1421 - rmse: 0.1762 - mape: 36.1012 - pearson: nan\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1420 - rmse: 0.1762 - mape: 36.1240 - pearson: nan\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1420 - rmse: 0.1762 - mape: 36.1378 - pearson: nan\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1419 - rmse: 0.1762 - mape: 36.1448 - pearson: nan\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1419 - rmse: 0.1762 - mape: 36.1444 - pearson: nan\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1420 - rmse: 0.1762 - mape: 36.1366 - pearson: nan\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1420 - rmse: 0.1762 - mape: 36.1229 - pearson: nan\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1762 - accuracy: 0.0000e+00 - mae: 0.1421 - rmse: 0.1762 - mape: 36.1042 - pearson: nan\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1422 - rmse: 0.1761 - mape: 36.0830 - pearson: nan\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1423 - rmse: 0.1761 - mape: 36.0603 - pearson: nan\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1424 - rmse: 0.1761 - mape: 36.0373 - pearson: nan\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0151 - pearson: nan\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1426 - rmse: 0.1761 - mape: 35.9961 - pearson: nan\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1427 - rmse: 0.1761 - mape: 35.9796 - pearson: nan\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1427 - rmse: 0.1761 - mape: 35.9679 - pearson: nan\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1428 - rmse: 0.1761 - mape: 35.9599 - pearson: nan\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1428 - rmse: 0.1761 - mape: 35.9571 - pearson: nan\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1428 - rmse: 0.1761 - mape: 35.9569 - pearson: nan\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1428 - rmse: 0.1761 - mape: 35.9618 - pearson: nan\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1427 - rmse: 0.1761 - mape: 35.9703 - pearson: nan\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1427 - rmse: 0.1761 - mape: 35.9809 - pearson: nan\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1426 - rmse: 0.1761 - mape: 35.9921 - pearson: nan\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1426 - rmse: 0.1761 - mape: 36.0042 - pearson: nan\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0167 - pearson: nan\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0284 - pearson: nan\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1424 - rmse: 0.1761 - mape: 36.0378 - pearson: nan\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1424 - rmse: 0.1761 - mape: 36.0458 - pearson: nan\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1424 - rmse: 0.1761 - mape: 36.0493 - pearson: nan\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1423 - rmse: 0.1761 - mape: 36.0555 - pearson: nan\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1423 - rmse: 0.1761 - mape: 36.0559 - pearson: nan\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1423 - rmse: 0.1761 - mape: 36.0543 - pearson: nan\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1424 - rmse: 0.1761 - mape: 36.0507 - pearson: nan\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1424 - rmse: 0.1761 - mape: 36.0462 - pearson: nan\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1424 - rmse: 0.1761 - mape: 36.0392 - pearson: nan\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1424 - rmse: 0.1761 - mape: 36.0323 - pearson: nan\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0227 - pearson: nan\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0190 - pearson: nan\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0134 - pearson: nan\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1426 - rmse: 0.1761 - mape: 36.0091 - pearson: nan\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1426 - rmse: 0.1761 - mape: 36.0055 - pearson: nan\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1426 - rmse: 0.1761 - mape: 36.0008 - pearson: nan\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1426 - rmse: 0.1761 - mape: 36.0025 - pearson: nan\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1426 - rmse: 0.1761 - mape: 36.0013 - pearson: nan\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1426 - rmse: 0.1761 - mape: 36.0064 - pearson: nan\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1426 - rmse: 0.1761 - mape: 36.0094 - pearson: nan\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0125 - pearson: nan\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0156 - pearson: nan\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0193 - pearson: nan\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0207 - pearson: nan\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0273 - pearson: nan\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0294 - pearson: nan\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0304 - pearson: nan\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0308 - pearson: nan\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0310 - pearson: nan\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0302 - pearson: nan\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0299 - pearson: nan\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0279 - pearson: nan\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0262 - pearson: nan\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0231 - pearson: nan\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0216 - pearson: nan\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0200 - pearson: nan\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0152 - pearson: nan\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0168 - pearson: nan\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0161 - pearson: nan\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0163 - pearson: nan\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0167 - pearson: nan\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0168 - pearson: nan\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0165 - pearson: nan\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0175 - pearson: nan\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0196 - pearson: nan\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0204 - pearson: nan\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0215 - pearson: nan\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0227 - pearson: nan\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0236 - pearson: nan\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0244 - pearson: nan\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0212 - pearson: nan\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0244 - pearson: nan\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0213 - pearson: nan\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0242 - pearson: nan\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0236 - pearson: nan\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0230 - pearson: nan\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0229 - pearson: nan\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0218 - pearson: nan\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0212 - pearson: nan\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0178 - pearson: nan\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0195 - pearson: nan\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0204 - pearson: nan\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0195 - pearson: nan\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0195 - pearson: nan\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0176 - pearson: nan\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0208 - pearson: nan\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0209 - pearson: nan\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0216 - pearson: nan\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0219 - pearson: nan\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0224 - pearson: nan\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0227 - pearson: nan\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0195 - pearson: nan\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0224 - pearson: nan\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0230 - pearson: nan\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0229 - pearson: nan\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0192 - pearson: nan\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0224 - pearson: nan\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0208 - pearson: nan\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.0000e+00 - mae: 0.1425 - rmse: 0.1761 - mape: 36.0208 - pearson: nan\n",
      "(12, 1584)\n",
      "(36, 1584)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[0.49880782 0.19958988 0.46192893 ... 0.         0.42528628 0.56692913]\n",
      " [0.93276109 0.49897471 0.93756345 ... 0.         0.47428196 0.71653543]\n",
      " [0.41869337 0.25905673 0.43604061 ... 0.         0.36070959 0.54330709]\n",
      " ...\n",
      " [0.33667144 0.74299385 0.30456853 ... 0.         0.56983293 0.22834646]\n",
      " [0.1349547  0.45181135 0.15177665 ... 0.         0.62455416 0.42519685]\n",
      " [0.3996185  0.62474368 0.39847716 ... 0.         0.33095551 0.77165354]]\n",
      "[[24.87902913570404], [20.859030327796937], [12.099035334587096], [15.479272918701172], [7.22928412437439], [8.749540305137634], [14.329205327033996], [18.459351596832274], [24.64919662475586], [27.858861646652223], [30.079129152297973], [27.898685455322266], [24.92912283420563], [19.248989081382753], [12.29939534664154], [10.209346590042115], [11.849354100227355], [13.379460315704346], [15.74933156967163], [18.369457812309264], [23.838985385894777], [27.61898908138275], [29.159004101753236], [29.879077892303467], [25.64898419380188], [20.589222850799562], [13.729300336837769], [11.129336576461792], [6.009224162101746], [13.889330377578736], [15.639266600608826], [20.409102926254274], [25.318877859115602], [27.928796677589418], [28.808789167404175], [29.14875292778015]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 346\u001b[0m\n\u001b[0;32m    343\u001b[0m forecasts \u001b[38;5;241m=\u001b[39m inverse_transform(series, forecasts, scaler, n_test\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# evaluate forecasts\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m \u001b[43mevaluate_forecasts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecasts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_lag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# plot forecasts\u001b[39;00m\n\u001b[0;32m    349\u001b[0m plot_forecasts(series, forecasts, n_test\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 249\u001b[0m, in \u001b[0;36mevaluate_forecasts\u001b[1;34m(test, forecasts, n_lag, n_seq)\u001b[0m\n\u001b[0;32m    247\u001b[0m i\u001b[38;5;241m=\u001b[39mn_seq\n\u001b[0;32m    248\u001b[0m actual \u001b[38;5;241m=\u001b[39m [row[i] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m test]\n\u001b[1;32m--> 249\u001b[0m predicted \u001b[38;5;241m=\u001b[39m [forecast[i] \u001b[38;5;28;01mfor\u001b[39;00m forecast \u001b[38;5;129;01min\u001b[39;00m forecasts]\n\u001b[0;32m    250\u001b[0m rmse \u001b[38;5;241m=\u001b[39m sqrt(mean_squared_error(actual, predicted))\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt+\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m RMSE: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m ((i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), rmse))\n",
      "Cell \u001b[1;32mIn[20], line 249\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    247\u001b[0m i\u001b[38;5;241m=\u001b[39mn_seq\n\u001b[0;32m    248\u001b[0m actual \u001b[38;5;241m=\u001b[39m [row[i] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m test]\n\u001b[1;32m--> 249\u001b[0m predicted \u001b[38;5;241m=\u001b[39m [\u001b[43mforecast\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m forecast \u001b[38;5;129;01min\u001b[39;00m forecasts]\n\u001b[0;32m    250\u001b[0m rmse \u001b[38;5;241m=\u001b[39m sqrt(mean_squared_error(actual, predicted))\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt+\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m RMSE: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m ((i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), rmse))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Activation\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "import keras.backend as K\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def mape (y_true, y_pred):\n",
    "    return 100*K.mean(K.sqrt(K.square(y_true - y_pred))/y_true)\n",
    "\n",
    "def pearson (y_true, y_pred):\n",
    "    return (K.square(K.mean((y_true - K.mean(y_true))*(y_pred - K.mean(y_pred)))))/(K.mean(K.square(y_true - K.mean(y_true)))*K.mean(K.square(y_pred - K.mean(y_pred))))\n",
    " \n",
    "# convert time series into a supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    cols, names = list(), list()\n",
    "    df = DataFrame(data)\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "    \tcols.append(df.shift(i))\n",
    "    \tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(n_out, n_out):\n",
    "        cols.append(df[0].shift(-i)) # df[0] for temperature, df[1] for specific humidity\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (1))] # % (1) for temperature, % (2) for specific humidity\n",
    "        else:            \n",
    "            names += [('var%d(t+%d)' % (1, i))] # % (1) for temperature, % (2) for specific humidity\n",
    "    \n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "\n",
    "    if dropnan:\n",
    "        #Drop rows containing NaN\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    print(\"Agg:\")\n",
    "    agg.columns = names\n",
    "    print(type(agg))\n",
    "    print(agg)\n",
    "\n",
    "    print(\"Test columns:\")\n",
    "    print(agg.iloc[:, -1:]) # Column containing response actual values (temperature) at time t\n",
    "\n",
    "    return agg\n",
    " \n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    " \n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(data, n_test, n_lag, n_seq):\n",
    "\n",
    "    #Prepare data for time series forecasting.\n",
    "    \n",
    "    #Parameters:\n",
    "    #x (array-like): Input features.\n",
    "    #y (array-like): Target values.\n",
    "    #n_test (int): Number of test samples.\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations.\n",
    "    \n",
    "    #Returns:\n",
    "    #tuple: Training and test datasets.\n",
    "\n",
    "    #print(\"Prepare data input (difference input)\")\n",
    "    #print(data.shape)\n",
    "    #print(data)\n",
    "\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "\n",
    "\t# transform data to be stationary\n",
    "    diff_series = difference(data, 1)\n",
    "    diff_values = []\n",
    "    for i in range(len(diff_series)):\n",
    "        diff_values_row = []\n",
    "        for j in range(len(diff_series[0])):\n",
    "            diff_values_row.append(diff_series[i][j])\n",
    "        diff_values.append(diff_values_row)\n",
    "    #print(\"Diff Series:\")\n",
    "    #print(diff_series.shape)\n",
    "    #print(diff_series)\n",
    "    #print(\"Diff_Values:\")\n",
    "    #print(len(diff_values))\n",
    "    #print(diff_values)\n",
    "    \n",
    "    # split into train and test sets\n",
    "    train_values, test_values = diff_values[0:-n_test], diff_values[-n_test:]\n",
    "    \n",
    "    #print(train_values)\n",
    "    \n",
    "    # rescale values to 0, 1\n",
    "    scaler_all_features =  MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler =  MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled_values = scaler_all_features.fit_transform(train_values)\n",
    "    response_train_values = []\n",
    "    for i in range(len(train_values)):\n",
    "        response_train_values.append(train_values[i][0]) # Uses first column (temperatures) as response variable\n",
    "    response_train_values = np.array(response_train_values)\n",
    "    response_train_values = response_train_values.reshape(len(response_train_values), 1)\n",
    "    #response_train_values = response_train_values.reshape(-1, 1)\n",
    "    #print(\"Response:\")\n",
    "    #print(response_train_values)\n",
    "    # Fit the scaler for just the response variable for use later when forecasting\n",
    "    response_scaled_values = scaler.fit_transform(response_train_values) \n",
    "    #print(\"Scaled values before reshape:\")\n",
    "    #print(train_scaled_values.shape)\n",
    "    #print(train_scaled_values)\n",
    "    test_scaled_values = scaler_all_features.transform(test_values)\n",
    "    \n",
    "    #scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    scaled_values = pd.concat([pd.DataFrame(train_scaled_values), pd.DataFrame(test_scaled_values)], axis=0)\n",
    "    scaled_values = np.array(scaled_values)\n",
    "    \n",
    "    #print(\"Supervised input:\")\n",
    "    #print(scaled_values.shape)\n",
    "    #print(scaled_values)\n",
    "    \n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    print(type(supervised))\n",
    "    supervised_values = supervised.values\n",
    "\n",
    "    #print(\"Supervised going into train/test:\")\n",
    "    #print(len(supervised))\n",
    "    #print(supervised)\n",
    "\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "\n",
    "    # drop rows with NaN values\n",
    "    #train = pd.DataFrame(train)\n",
    "    #train.dropna(inplace=True)\n",
    "    #train = np.array(train)\n",
    "    \n",
    "    return scaler, scaler_all_features, train, test\n",
    " \n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:-1], train[:, -1:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    print(y.shape[0])\n",
    "    print(X.shape[0])\n",
    "    print(X.shape[1])\n",
    "    print(X.shape[2])\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, return_sequences=True, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(16, input_dim=y.shape[1], activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=rmse, optimizer='adam', metrics=['accuracy', 'mae', rmse, mape, pearson])\n",
    "    # fit network\n",
    "    #for i in range(nb_epoch):\n",
    "    #    model.fit(X, y, epochs=200, batch_size=n_batch, verbose=1)\n",
    "    #    model.reset_states()\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(y)\n",
    "    print(train.shape)\n",
    "    \n",
    "    model.fit(X, y, epochs=200, verbose=1)\n",
    "    \n",
    "    return model\n",
    " \n",
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "\t# reshape input pattern to [samples, timesteps, features]\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\t# make forecast\n",
    "\tforecast = model.predict(X, batch_size=n_batch)\n",
    "\t# convert to array\n",
    "\treturn [x for x in forecast[0, :]]\n",
    " \n",
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "\tforecasts = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\tX, y = test[i, 0:-1], test[i, -1:]\n",
    "\t\t# make forecast\n",
    "\t\tforecast = forecast_lstm(model, X, n_batch)\n",
    "\t\t# store the forecast\n",
    "\t\tforecasts.append(forecast)\n",
    "\treturn forecasts\n",
    " \n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "\t# invert first forecast\n",
    "\tinverted = list()\n",
    "\tinverted.append(forecast[0] + last_ob)\n",
    "\t# propagate difference forecast using inverted first value\n",
    "\tfor i in range(1, len(forecast)):\n",
    "\t\tinverted.append(forecast[i] + inverted[i-1])\n",
    "\treturn inverted\n",
    " \n",
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i - 1\n",
    "        last_ob = series.values[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted\n",
    " \n",
    "# evaluate the RMSE for each forecast time step\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    print(test)\n",
    "    print(forecasts)\n",
    "    for i in range(n_seq):\n",
    "        actual = [row[i] for row in test]\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "        print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    " \n",
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "\t# plot the entire dataset in blue\n",
    "\tpyplot.plot(series.values)\n",
    "\t# plot the forecasts in red\n",
    "\tfor i in range(len(forecasts)):\n",
    "\t\toff_s = len(series) - n_test + i - 1\n",
    "\t\toff_e = off_s + len(forecasts[i]) + 1\n",
    "\t\txaxis = [x for x in range(off_s, off_e)]\n",
    "\t\tyaxis = [series.values[off_s]] + forecasts[i]\n",
    "\t\tpyplot.plot(xaxis, yaxis, color='red')\n",
    "\txlim = [0, 40]\n",
    "    # show the plot\n",
    "\tpyplot.show()\n",
    "\n",
    "\n",
    "# Get the current working directory \n",
    "current_directory = os.getcwd() \n",
    "\n",
    "# Print the current working directory \n",
    "print(current_directory)\n",
    "\n",
    "# Define the directory containing the files \n",
    "path = current_directory+\"\\\\Modeling\\\\\"\n",
    "print(path)\n",
    "\n",
    "filename = path + 'Final_Monthly_Dataset.csv'\n",
    "\n",
    "# load dataset\n",
    "df = read_csv(filename, header=0, parse_dates=[0], index_col=0, date_format='%Y-%m')\n",
    "\n",
    "\n",
    "df = df.rename(columns={'Unnamed: 0' : 'indices'})\n",
    "\n",
    "#Remove unused columns\n",
    "df = df.drop(['Day', 'vapor_pressure'], axis=1)\n",
    "\n",
    "# Round numbers in columns to reasonable precision\n",
    "df['temperatures'] = np.round(df['temperatures'], 2)\n",
    "df['slp'] = np.round(df['slp'], 2)\n",
    "df['wet_bulb_temperature'] = np.round(df['wet_bulb_temperature'], 2)\n",
    "df['specific_humidity'] = np.round(df['specific_humidity'], 2)\n",
    "df['GHI'] = np.round(df['GHI'], 2)\n",
    "df['PRCP'] = np.round(df['PRCP'], 2)\n",
    "df['SNDP'] = np.round(df['SNDP'], 2)\n",
    "df['solar_activity'] = np.round(df['solar_activity'], 2)\n",
    "df['ONI'] = np.round(df['ONI'], 2)\n",
    "df['water'] = np.round(df['water'], 0)\n",
    "df['region'] = np.round(df['region'], 0)\n",
    "\n",
    "df_trimmed = df[df['file_id']==6501]\n",
    "df_trimmed = df_trimmed.drop(['Year', 'Month', 'file_id', 'date', 'latitude', 'longitude', 'elevation'], axis=1)\n",
    "\n",
    "print(df_trimmed)\n",
    "\n",
    "dataset = df_trimmed.values\n",
    "\n",
    "#print(dataset)\n",
    "\n",
    "series = pd.Series(dataset[:, 0]) # Using first column (temperatures)\n",
    "\n",
    "#print(series)\n",
    "\n",
    "# configure\n",
    "n_seq = 47\n",
    "n_lag = 179-n_seq\n",
    "n_test = 36\n",
    "n_epochs = 100\n",
    "n_batch = 1\n",
    "\n",
    "# prepare data\n",
    "scaler, scaler_all_features, train, test = prepare_data(dataset, n_test, n_lag, n_seq)\n",
    "\n",
    "print(\"Train:\")\n",
    "print(np.array(train).shape)\n",
    "print(train)\n",
    "print(\"Test:\")\n",
    "print(np.array(test).shape)\n",
    "print(test)\n",
    "\n",
    "# fit model\n",
    "model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "# make forecasts\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "\n",
    "# inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, forecasts, scaler, n_test+2)\n",
    "\n",
    "# evaluate forecasts\n",
    "evaluate_forecasts(test, forecasts, n_lag, n_seq)\n",
    "\n",
    "# plot forecasts\n",
    "plot_forecasts(series, forecasts, n_test+2)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fad724-0d01-457e-ae2e-2fe5abb5f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "09943514-ef81-41c0-81fe-2f23d2c75957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x000002A2980DFF40>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea996e00-9f29-483d-92fa-f70c03ffe79e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
