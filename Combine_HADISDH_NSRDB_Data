import csv
import os
import sys
import h5pyd
import xarray as xr
import numpy as np
import pandas as pd
import netCDF4 as nc
from scipy import stats
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from scipy.spatial import cKDTree
from scipy import stats
import math

def haversine(lat1, lon1, lat2, lon2):
   # Convert latitude and longitude from degrees to radians
   lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])

   dlat = lat2 - lat1
   dlon = lon2 - lon1
   a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2
   c = 2 * math.asin(math.sqrt(a))

   # Radius of Earth in kilometers. Use 3956 for miles.
   r = 6371

   # Calculate the result
   distance = c * r
   return distance

print(sys.version)

# Create 2D array with file_id, water, and region values for file list
array = np.array([
[6501, 4, 1],[6541, 4, 1],[6640, 6, 1],[6668, 4, 1],[6678, 6, 1],[6687, 6, 1],
[6697, 4, 1],[6714, 6, 1],[6744, 5, 1],[6772, 6, 1],[6783, 6, 1],[6840, 6, 1],
[6844, 4, 1],[6854, 4, 1],[6870, 4, 1],[6891, 4, 0],[6895, 4, 0],[6899, 4, 1],
[6901, 4, 1],[6909, 4, 1],[6929, 4, 1],[6950, 4, 1],[6963, 4, 0],[6969, 4, 0],
[6994, 2, 0],[7032, 2, 1],[7057, 6, 1],[7094, 6, 1],[7095, 6, 1],[7100, 6, 1],
[7108, 6, 0],[7116, 6, 1],[7119, 6, 1],[7131, 4, 1],[7139, 6, 1],[7152, 4, 1],
[7155, 4, 1],[7156, 4, 1],[7182, 4, 1],[7193, 4, 1],[7202, 4, 1],[7239, 2, 0],
[7280, 6, 1],[7286, 6, 1],[7287, 5, 1],[7311, 6, 1],[7321, 6, 1],[7329, 5, 1],
[7347, 4, 1],[7350, 6, 1],[7354, 6, 0],[7357, 6, 1],[7361, 4, 1],[7414, 4, 1],
[7423, 4, 1],[7424, 2, 1],[7426, 2, 1],[7432, 4, 0],[7463, 2, 0],[7482, 2, 1],
[7489, 5, 1],[7528, 5, 1],[7531, 6, 1],[7533, 6, 1],[7534, 6, 1],[7538, 5, 1],
[7549, 5, 1],[7553, 5, 0],[7555, 5, 1],[7562, 5, 1],[7571, 5, 1],[7573, 5, 0],
[7574, 6, 1],[7575, 6, 1],[7585, 5, 1],[7599, 4, 1],[7603, 6, 0],[7606, 5, 1],
[7622, 5, 1],[7652, 5, 1],[7671, 3, 1],[7704, 3, 1],[7786, 5, 1],[7805, 5, 1],
[7816, 5, 1],[7838, 5, 1],[7861, 3, 1],[7862, 5, 1],[7863, 5, 1],[7870, 3, 1],
[7892, 3, 1],[7906, 3, 1],[7907, 3, 0],[7938, 3, 1],[7962, 1, 1],[7979, 3, 1],
[7987, 1, 1],[7999, 1, 1],[8000, 1, 1],[8034, 5, 1],[8083, 3, 0],[8120, 1, 1],
[8133, 1, 1],[8184, 2, 0],[8186, 2, 0],[8247, 6, 1],[8248, 6, 1],[9858, 4, 1]])

# Set index to get file_id, water, and region values for each file
i=0

# Create empty list to store NSRDB file names
files = []

# Fill list with all possible file names
for j in range(15):
   ind = 2006+j
   files.append("/nrel/nsrdb/v3/nsrdb_"+str(ind)+".h5")

# Get the current working directory 
current_directory = os.getcwd() 

# Print the current working directory 
print(current_directory)

# Define the directory containing the files 
path = current_directory+"\\Data\\HADISDH_Humidity\\NC_Files\\"
print(path)

# Create an empty list to hold the dataframes 
dataframes = [] 

# Create datetime object for comparison
datetime_str = '1/1/06 00:00:00'

# Convert string to datetime
datetime_object = datetime.strptime(datetime_str, '%d/%m/%y %H:%M:%S')

# Loop through all filenames in filenames_only.csv 
with open("filenames_only.csv", mode='r') as file: 
   csv_reader = csv.reader(file)
   for row in csv_reader:
      filename = row[0]
      print(filename)
      array_row = array[i]
      # Open the NetCDF file
      file = path+filename
      ds = xr.open_dataset(file)
      # Convert the dataset to a DataFrame
      df = ds.to_dataframe().reset_index()
      df_filtered = df[df['time'] >= datetime_object] # Remove data older than January 1, 2006
      # Fill NaN with linearly interpolated values
      df_filtered = df_filtered.interpolate()
      df_filtered.set_index('time', inplace = True)
      # Drop unneeded columns
      df_filtered = df_filtered.drop(['input_station_id','station_id','coordinate_length'], axis=1)
      df_filtered = df_filtered.drop(['relative_humidity', 'saturation_vapor_pressure', 'dewpoints'], axis=1)
      df_filtered = df_filtered.assign(file_id = array_row[0])
      df_filtered = df_filtered.assign(region = array_row[1])
      df_filtered = df_filtered.assign(water = array_row[2])

      j=0

      # Create empty list for dataframes
      ghi_datasets = []
      wind_speed_datasets = []
      time_index_datasets = []

      # for loop to get data from NSRDB for each year
      for j in range(15):
         # Open the desired year of nsrdb data
         year = 2006 + j
         print("Year: "+str(year))
         files = "/nrel/nsrdb/v3/nsrdb_"+str(year)+".h5"
         f = h5pyd.File(files, 'r')
    
         #Lists attributes contained in file f
         list(f)

         # Locational information is stored in either 'meta' or 'coordinates'
         meta = pd.DataFrame(f['meta'][...])

         # Access the datasets for latitude and longitude
         latitude_dataset = meta['latitude']
         longitude_dataset = meta['longitude']

         # Read the data into numpy arrays
         latitude = latitude_dataset[:]
         longitude = longitude_dataset[:]

         # Print the extracted data
         print("Latitude:", latitude)
         print("Longitude:", longitude)

         indexes1 = []
         indexes2 = []
         values1 = []
         values2 = []

         #HADISDH geospatial coordinates
         latitude_h = df_filtered['latitude'].values[0]
         longitude_h = df_filtered['longitude'].values[0]

         #Range to look for NSRDB geospatial location
         latitude_min_NSRDB = latitude_h - 0.02
         latitude_max_NSRDB = latitude_h + 0.02
         longitude_min_NSRDB = longitude_h - 0.02
         longitude_max_NSRDB = longitude_h + 0.02

         print(latitude_min_NSRDB)
         print(latitude_max_NSRDB)
         print(longitude_min_NSRDB)
         print(longitude_max_NSRDB)

         # Extract datetime index for datasets
         time_index = pd.to_datetime(f['time_index'][...].astype(str))
         time_index # Temporal resolution is 30min
         print(time_index.shape[0])

         for index1 in range(2018392):
            value = latitude[index1]
            if(value < latitude_max_NSRDB and value > latitude_min_NSRDB):
               indexes1.append(index1)
               if value not in values1:
                  values1.append(value)

         for index2 in range(2018392):
            value = longitude[index2]
            if(value < longitude_max_NSRDB  and value > longitude_min_NSRDB):
               indexes2.append(index2)
               if value not in values2:
                  values2.append(value)

         # Find specific index for location from common index between latitude and longitude list
         common_indexes = []
         common_indexes = [value for value in indexes1 if value in indexes2]
         print(common_indexes)
         location_index = common_indexes[0]

         # Datasets are stored in a 2d array of time x location
         dset = f['ghi']
         print(dset.shape)
         print(dset)

         dset2 = f['wind_speed']
         print(dset2.shape)
         print(dset2)

         # Get GHI and wind speed data for specific location and times
         dset_filtered = pd.DataFrame(dset[0::2, location_index])
         dset2_filtered = pd.DataFrame(dset2[0::2, location_index])
         time_index_df = pd.DataFrame(time_index[0::2])

         ghi_datasets.append(dset_filtered)
         wind_speed_datasets.append(dset2_filtered)
         time_index_datasets.append(time_index_df)

         # Close the file
         f.close()

      # Concatenate all Dataframes
      combined_ghi_df = pd.concat(ghi_datasets, ignore_index=True)
      combined_wind_df = pd.concat(wind_speed_datasets, ignore_index=True)
      combined_time_df = pd.concat(time_index_datasets, ignore_index=True)

      # Now combined files contain all the data from GHI and wind speed files
      print(combined_ghi_df)
      print(combined_wind_df)
      print(combined_time_df)

      NSRDB_merged_df = pd.concat([combined_time_df, combined_ghi_df, combined_wind_df], axis=1)
      NSRDB_merged_df.columns = ['time', 'GHI', 'wind_speed']
      NSRDB_merged_df['NSRDB_latitude'] = round(latitude[location_index], 3)
      NSRDB_merged_df['NSRDB_longitude'] = round(longitude[location_index], 3)
      NSRDB_merged_df['distance_h_NSRDB'] = round(haversine(df_filtered['latitude'].values[0], df_filtered['longitude'].values[0], latitude[location_index], longitude[location_index]), 3)

      print(NSRDB_merged_df)
      print(NSRDB_merged_df.shape[0])

      df_filtered2 = df_filtered.reset_index()
      print(df_filtered2['time'])

      print(df_filtered2)

      for n in range(NSRDB_merged_df.shape[0]):
         datevalue = str(NSRDB_merged_df['time'].values[n])[:10]
         timevalue = str(NSRDB_merged_df['time'].values[n])[11:19]
         datetimevalue = datevalue + " " + timevalue
         NSRDB_merged_df.at[n, 'time'] = datetimevalue

      NSRDB_merged_df['time'] = pd.to_datetime(NSRDB_merged_df['time'])
      df_filtered2['time'] = pd.to_datetime(df_filtered2['time'])

      daily_NSRDB_df = NSRDB_merged_df.resample('D', on='time').mean()
      daily_df_filtered = df_filtered2.resample('D', on='time').mean()

      NSRDB_merged_df.set_index('time')
      df_filtered2.set_index('time')

      print(daily_NSRDB_df)
      print(daily_df_filtered)

      merged_df = pd.merge(daily_df_filtered, daily_NSRDB_df, on='time', how='outer')
      #merged_df['file_id'] = merged_df['file_id'].astype(int)
      #merged_df['water'] = merged_df['water'].astype(int)
      #merged_df['region'] = merged_df['region'].astype(int)
      print(merged_df)

      # Write DataFrame to CSV
      newfilename = "HADISDH_NSRDB_Merged_File_" + str(merged_df['file_id'].values[0]) + ".csv"
      merged_df.to_csv(newfilename, index=True)

      dataframes.append(merged_df)
      i += 1

# Concatenate all Dataframes
combined_df = pd.concat(dataframes, ignore_index=False)

# Now combined_df contains all the data from NC files
print(combined_df)

# Display the dataframe
print(combined_df.head())

# Write DataFrame to CSV
newfilename = "HADISDH_NSRDB_Merged_File_All_Data.csv"
combined_df.to_csv(newfilename, index=True)
