{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5baac998-16fc-4132-83cf-437ab3206a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "n_lag (number of input time steps): 15\n",
      "n_seq (number of output/future prediction time steps): 10\n",
      "C:\\Users\\User\n",
      "C:\\Users\\User\\Modeling\\\n",
      "     file_id  temperatures      slp  wet_bulb_temperature  specific_humidity  \\\n",
      "0       6678         20.84  1016.20                 18.08              12.82   \n",
      "1       6678         20.69  1017.48                 17.71              12.33   \n",
      "2       6678         20.47  1018.26                 17.29              12.14   \n",
      "3       6678         20.30  1018.41                 17.20              11.90   \n",
      "4       6678         20.46  1017.92                 17.75              12.75   \n",
      "..       ...           ...      ...                   ...                ...   \n",
      "643     8000         13.02  1016.94                  7.64               5.15   \n",
      "644     8000         13.47  1016.16                  7.88               5.24   \n",
      "645     8000         11.84  1017.90                  7.13               5.36   \n",
      "646     8000         12.86  1016.17                  7.66               5.43   \n",
      "647     8000         13.58  1015.12                  8.11               5.32   \n",
      "\n",
      "     water     GHI      WDSP  PRCP  SNDP  region  solar_activity   ONI  \n",
      "0      1.0  217.31  6.212756  0.12  0.00     6.0           88.05 -0.05  \n",
      "1      1.0  222.89  5.525413  0.08  0.05     6.0          136.15 -1.22  \n",
      "2      1.0  228.09  5.557681  0.09  0.00     6.0          173.81 -0.85  \n",
      "3      1.0  221.56  6.402008  0.09  0.00     6.0          170.28 -0.31  \n",
      "4      1.0  217.38  6.154090  0.16  0.00     6.0          163.61  0.66  \n",
      "..     ...     ...       ...   ...   ...     ...             ...   ...  \n",
      "643    1.0  184.81  7.025180  0.01  0.00     1.0            8.80 -0.36  \n",
      "644    1.0     NaN  6.802803  0.02  0.00     1.0           29.47 -0.72  \n",
      "645    1.0     NaN  5.979051  0.03  0.00     1.0           83.04 -0.94  \n",
      "646    1.0     NaN  5.971951  0.02  0.00     1.0          125.33  0.82  \n",
      "647    1.0     NaN  6.623975  0.03  0.00     1.0          154.37  0.45  \n",
      "\n",
      "[648 rows x 13 columns]\n",
      "Empty DataFrame\n",
      "Columns: [temperatures, slp, wet_bulb_temperature, specific_humidity, water, GHI, WDSP, PRCP, SNDP, region, solar_activity, ONI]\n",
      "Index: []\n",
      "[]\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 1:\n",
      "12\n",
      "19\n",
      "<class 'list'>\n",
      "12\n",
      "5\n",
      "<class 'list'>\n",
      "(19, 1, 1, 190)\n",
      "(2, 1, 1, 190)\n",
      "(3, 1, 1, 190)\n",
      "(19, 1, 190)\n",
      "(2, 1, 190)\n",
      "(3, 1, 190)\n",
      "(19, 190)\n",
      "(2, 190)\n",
      "(3, 190)\n",
      "[[0.41503268 0.46007605 0.35622318 ... 0.40359477 0.60947712 0.69117647]\n",
      " [0.48529412 0.46007605 0.38769671 ... 0.59313725 0.54901961 0.62581699]\n",
      " [0.19771242 0.5095057  0.17310443 ... 0.48529412 0.63562092 0.70588235]\n",
      " ...\n",
      " [0.35947712 0.40969582 0.32474964 ... 0.66503268 0.26633987 0.88235294]\n",
      " [0.13071895 0.81939163 0.16452074 ... 0.64052288 0.24346405 0.75653595]\n",
      " [0.39215686 1.         0.25751073 ... 0.62745098 0.2875817  0.72058824]]\n",
      "Fold 2:\n",
      "12\n",
      "19\n",
      "<class 'list'>\n",
      "12\n",
      "5\n",
      "<class 'list'>\n",
      "(19, 1, 1, 190)\n",
      "(2, 1, 1, 190)\n",
      "(3, 1, 1, 190)\n",
      "(19, 1, 190)\n",
      "(2, 1, 190)\n",
      "(3, 1, 190)\n",
      "(19, 190)\n",
      "(2, 190)\n",
      "(3, 190)\n",
      "[[0.59090909 0.44771863 0.50704225 ... 0.58806818 0.64630682 0.671875  ]\n",
      " [0.55255682 0.46007605 0.45198464 ... 0.64630682 0.60795455 0.67471591]\n",
      " [0.52982955 0.45057034 0.41997439 ... 0.53977273 0.65198864 0.58806818]\n",
      " ...\n",
      " [0.24431818 0.81939163 0.25224072 ... 0.6875     0.34232955 0.78835227]\n",
      " [0.53551136 0.57889734 0.45326504 ... 0.68181818 0.46448864 0.68181818]\n",
      " [0.47159091 1.         0.33546735 ... 0.67613636 0.38068182 0.75710227]]\n",
      "Fold 3:\n",
      "12\n",
      "19\n",
      "<class 'list'>\n",
      "12\n",
      "5\n",
      "<class 'list'>\n",
      "(19, 1, 1, 190)\n",
      "(2, 1, 1, 190)\n",
      "(3, 1, 1, 190)\n",
      "(19, 1, 190)\n",
      "(2, 1, 190)\n",
      "(3, 1, 190)\n",
      "(19, 190)\n",
      "(2, 190)\n",
      "(3, 190)\n",
      "[[0.59090909 0.40895219 0.50704225 ... 0.58806818 0.64630682 0.671875  ]\n",
      " [0.49147727 0.42217701 0.42381562 ... 0.48153409 0.66051136 0.73153409]\n",
      " [0.52982955 0.41200407 0.41997439 ... 0.53977273 0.65198864 0.58806818]\n",
      " ...\n",
      " [0.24431818 0.80671414 0.25224072 ... 0.6875     0.34232955 0.78835227]\n",
      " [0.53551136 0.54933876 0.45326504 ... 0.68181818 0.46448864 0.68181818]\n",
      " [0.47159091 1.         0.33546735 ... 0.67613636 0.38068182 0.75710227]]\n",
      "Fold 4:\n",
      "12\n",
      "20\n",
      "<class 'list'>\n",
      "12\n",
      "4\n",
      "<class 'list'>\n",
      "(20, 1, 1, 190)\n",
      "(2, 1, 1, 190)\n",
      "(2, 1, 1, 190)\n",
      "(20, 1, 190)\n",
      "(2, 1, 190)\n",
      "(2, 1, 190)\n",
      "(20, 190)\n",
      "(2, 190)\n",
      "(2, 190)\n",
      "[[0.61176471 0.44771863 0.64285714 ... 0.60882353 0.66911765 0.69558824]\n",
      " [0.50882353 0.46007605 0.53733766 ... 0.49852941 0.68382353 0.75735294]\n",
      " [0.57205882 0.46007605 0.57305195 ... 0.66911765 0.62941176 0.69852941]\n",
      " ...\n",
      " [0.25294118 0.81939163 0.31980519 ... 0.71176471 0.35441176 0.81617647]\n",
      " [0.55441176 0.57889734 0.57467532 ... 0.70588235 0.48088235 0.70588235]\n",
      " [0.48823529 1.         0.42532468 ... 0.7        0.39411765 0.78382353]]\n",
      "Fold 5:\n",
      "12\n",
      "19\n",
      "<class 'list'>\n",
      "12\n",
      "5\n",
      "<class 'list'>\n",
      "(19, 1, 1, 190)\n",
      "(2, 1, 1, 190)\n",
      "(3, 1, 1, 190)\n",
      "(19, 1, 190)\n",
      "(2, 1, 190)\n",
      "(3, 1, 190)\n",
      "(19, 190)\n",
      "(2, 190)\n",
      "(3, 190)\n",
      "[[0.59090909 0.7536     0.50704225 ... 0.58806818 0.64630682 0.671875  ]\n",
      " [0.49147727 0.7744     0.42381562 ... 0.48153409 0.66051136 0.73153409]\n",
      " [0.55255682 0.7744     0.45198464 ... 0.64630682 0.60795455 0.67471591]\n",
      " ...\n",
      " [0.50426136 0.6912     0.42381562 ... 0.67755682 0.42897727 0.78551136]\n",
      " [0.44318182 0.6896     0.39564661 ... 0.70880682 0.36221591 0.89772727]\n",
      " [0.53551136 0.9744     0.45326504 ... 0.68181818 0.46448864 0.68181818]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Activation, Dropout\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from numpy import array\n",
    "import keras.backend as K\n",
    "import itertools\n",
    "#!pip install pydot\n",
    "    \n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "    \n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    \n",
    "def mape (y_true, y_pred):\n",
    "    return 100*K.mean(K.sqrt(K.square(y_true - y_pred))/y_true)\n",
    "    \n",
    "def pearson (y_true, y_pred):\n",
    "    return (K.square(K.mean((y_true - K.mean(y_true))*(y_pred - K.mean(y_pred)))))/(K.mean(K.square(y_true - K.mean(y_true)))*K.mean(K.square(y_pred - K.mean(y_pred))))\n",
    "    \n",
    "# convert time series into a supervised learning problem\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "# Convert time series into a supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    cols, names = list(), list()\n",
    "    df = DataFrame(data)\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "    \tcols.append(df.shift(i))\n",
    "    \tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df[0].shift(-i)) # df[0] for temperature\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (1))] # % (1) for temperature\n",
    "        else:            \n",
    "            names += [('var%d(t+%d)' % (1, i))] # % (1) for temperature\n",
    "    \n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "\n",
    "    if dropnan:\n",
    "        #Drop rows containing NaN\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    #print(\"Agg:\")\n",
    "    agg.columns = names\n",
    "    #print(type(agg))\n",
    "    #print(agg)\n",
    "\n",
    "    #print(\"Test columns:\")\n",
    "    #print(agg.iloc[:, -36]) # Column containing response actual values (temperature) at time t\n",
    "\n",
    "    return agg\n",
    "     \n",
    "# create a differenced series\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    "     \n",
    "# transform series into training sets for supervised learning\n",
    "def prepare_training_data(data, n_lag, n_seq, n_time_steps):\n",
    "    \n",
    "    #Prepare data for time series forecasting.\n",
    "        \n",
    "    #Parameters:\n",
    "    #x (array-like): Input features.\n",
    "    #y (array-like): Target values.\n",
    "    #n_test (int): Number of test samples (rows).\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations.\n",
    "    #n_train (int): Number of training samples (rows).\n",
    "        \n",
    "    #Returns:\n",
    "    #tuple: Training and test datasets.\n",
    "    \n",
    "    n_vars = len(data[0][0])\n",
    "\n",
    "    print(n_vars)\n",
    "    print(len(data))\n",
    "    print(type(data))\n",
    "\n",
    "    # Each weather station has 27 time steps (the first 23 have no nan values)\n",
    "    # Loop through data, grabbing one weather station (ws) at a time, \n",
    "    # differencing on each ws and separating by training (first 26-n_lag-n_seq-n_test time steps) \n",
    "    # and testing (n_test time steps) to scale data on training only.\n",
    "    # We then recombine the training and testing datasets to change each ws to a supervised learning problem by taking all the first 23 time steps for all 12 predictors\n",
    "    # and changing these to (t-n_lag) to (t-1) since we lose one row through differencing. We then shift forward only one dependent variable (temperature or specific humidity)\n",
    "    # for time steps t to (t+n_seq)\n",
    "\n",
    "\n",
    "    diff_values = []\n",
    "    \n",
    "    for ws in range(len(data)):\n",
    "        \n",
    "        # transform data to be stationary\n",
    "        diff_series = difference(data[ws], 1)\n",
    "        for i in range(len(diff_series)):\n",
    "            diff_values_row = []\n",
    "            for j in range(len(diff_series[0])):\n",
    "                diff_values_row.append(diff_series[i][j])\n",
    "            diff_values.append(diff_values_row)\n",
    "\n",
    "    #print(\"Diff values:\")\n",
    "    #print(len(diff_values))\n",
    "    #print(len(diff_values[0]))\n",
    "    #print(len(diff_values_for_training))\n",
    "    #print(len(diff_values_for_training[0]))\n",
    "    \n",
    "    # rescale values to 0, 1\n",
    "    scaler_all_features =  MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler =  MinMaxScaler(feature_range=(0, 1))\n",
    "    #diff_values_training = np.array(diff_values_for_training)\n",
    "    #diff_values_training = diff_values_training.reshape(1, -1) \n",
    "    train_scaled_values = scaler_all_features.fit_transform(diff_values)\n",
    "    response_train_values = []\n",
    "    for i in range(len(diff_values)):\n",
    "        response_train_values.append(diff_values[i][0]) # Uses first column (temperatures) as response variable\n",
    "    response_train_values = np.array(response_train_values)\n",
    "    response_train_values = response_train_values.reshape(len(response_train_values), 1)\n",
    "\n",
    "    # Fit the scaler for just the response variable for use later when forecasting\n",
    "    response_scaled_values = scaler.fit_transform(response_train_values) \n",
    "    scaled_values = scaler_all_features.transform(diff_values)\n",
    "\n",
    "    #print(\"Scaled values rows:\")\n",
    "    #print(len(scaled_values))\n",
    "\n",
    "    train = []\n",
    "\n",
    "    # Transform each weather station as a separate \"batch\"\n",
    "    for ws in range(len(data)):\n",
    "        # transform into a supervised learning problem X, y\n",
    "        first = (n_time_steps-1)*ws\n",
    "        last = (n_time_steps-1)*ws+(n_time_steps-2)\n",
    "        #print(\"Batch \"+str(ws+1)+\":\")\n",
    "        #print(\"Range: \"+str(first)+\"-\"+str(last))\n",
    "        scaled_values_batch = scaled_values[first:last]\n",
    "        supervised = series_to_supervised(scaled_values_batch, n_lag, n_seq)\n",
    "        supervised_values = supervised.values\n",
    "        train.append([supervised_values])\n",
    "        #print(\"Supervised count:\")\n",
    "        #print(len(supervised_values))\n",
    "        #print(len(supervised_values[0]))\n",
    "    \n",
    "    return scaler, scaler_all_features, train\n",
    "\n",
    "# transform series into testing and validation sets for supervised learning\n",
    "def prepare_testing_and_validation_data(data, n_lag, n_seq, n_time_steps, scaler_all_features):\n",
    "    \n",
    "    # Prepare data for time series forecasting.\n",
    "        \n",
    "    #Parameters:\n",
    "    #x (array-like): Input features.\n",
    "    #y (array-like): Target values.\n",
    "    #n_test (int): Number of test samples (rows).\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations.\n",
    "    #n_train (int): Number of training samples (rows).\n",
    "        \n",
    "    #Returns:\n",
    "    #tuple: Training and test datasets.\n",
    "    \n",
    "    n_vars = len(data[0][0])\n",
    "\n",
    "    print(n_vars)\n",
    "    print(len(data))\n",
    "    print(type(data))\n",
    "\n",
    "    # Each weather station has 227 time steps (the first 180 have no nan values)\n",
    "    # Loop through data, grabbing one weather station (ws) at a time, \n",
    "    # differencing on each ws and separating by training (first 226-n_lag-n_seq-n_test time steps) \n",
    "    # and testing (n_test time steps) to scale data on training only.\n",
    "    # We then recombine the training and testing datasets to change each ws to a supervised learning problem by taking all the first 180 time steps for all 12 predictors\n",
    "    # and changing these to (t-n_lag) to (t-1) since we lose one row through differencing. We then shift forward only one dependent variable (temperature or specific humidity)\n",
    "    # for time steps t to (t+n_seq)\n",
    "\n",
    "\n",
    "    diff_values = []\n",
    "    \n",
    "    for ws in range(len(data)):\n",
    "        \n",
    "        # transform data to be stationary\n",
    "        diff_series = difference(data[ws], 1)\n",
    "        for i in range(len(diff_series)):\n",
    "            diff_values_row = []\n",
    "            for j in range(len(diff_series[0])):\n",
    "                diff_values_row.append(diff_series[i][j])\n",
    "            diff_values.append(diff_values_row)\n",
    "\n",
    "    #print(\"Diff values:\")\n",
    "    #print(len(diff_values))\n",
    "    #print(len(diff_values[0]))\n",
    "    #print(len(diff_values_for_training))\n",
    "    #print(len(diff_values_for_training[0]))\n",
    "    \n",
    "    # rescale values to 0, 1\n",
    "    scaled_values = scaler_all_features.transform(diff_values)\n",
    "\n",
    "    validation = []\n",
    "    test = []\n",
    "\n",
    "    \n",
    "    \n",
    "    # Transform each weather station as a separate \"batch\"\n",
    "    for ws in range(len(data)):\n",
    "        # transform into a supervised learning problem X, y\n",
    "        first = (n_time_steps-1)*ws\n",
    "        last = (n_time_steps-1)*ws+(n_time_steps-2)\n",
    "        #print(\"Batch \"+str(ws+1)+\":\")\n",
    "        #print(\"Range: \"+str(first)+\"-\"+str(last))\n",
    "        scaled_values_batch = scaled_values[first:last]\n",
    "        supervised = series_to_supervised(scaled_values_batch, n_lag, n_seq)\n",
    "        supervised_values = supervised.values\n",
    "        # training/test/validation split is 80%/10%/10%\n",
    "        if ws < 2:\n",
    "            test.append([supervised_values])\n",
    "        else:\n",
    "            validation.append([supervised_values])\n",
    "        #print(\"Supervised count:\")\n",
    "        #print(len(supervised_values))\n",
    "        #print(len(supervised_values[0]))\n",
    "    \n",
    "    return validation, test\n",
    "    \n",
    "def plot_kfold(cv, X, y, ax, n_splits, xlim_max=105):\n",
    "    \n",
    "    #Plots the indices for a cross-validation object.\n",
    "    #Taken from https://www.geeksforgeeks.org/cross-validation-using-k-fold-with-scikit-learn/\n",
    "    \n",
    "    #Parameters:\n",
    "    #cv: Cross-validation object\n",
    "    #X: Feature set\n",
    "    #y: Target variable\n",
    "    #ax: Matplotlib axis object\n",
    "    #n_splits: Number of folds in the cross-validation\n",
    "    #xlim_max: Maximum limit for the x-axis\n",
    "        \n",
    "    # Set color map for the plot\n",
    "    cmap_cv = plt.cm.coolwarm\n",
    "    cv_split = cv.split(X=X, y=y)\n",
    "        \n",
    "    for i_split, (train_idx, test_idx) in enumerate(cv_split):\n",
    "        # Create an array of NaNs and fill in training/testing indices\n",
    "        indices = np.full(len(X), np.nan)\n",
    "        indices[test_idx], indices[train_idx] = 1, 0\n",
    "            \n",
    "        # Plot the training and testing indices\n",
    "        ax_x = range(len(indices))\n",
    "        ax_y = [i_split + 0.5] * len(indices)\n",
    "        ax.scatter(ax_x, ax_y, c=indices, marker=\"_\", \n",
    "                   lw=10, cmap=cmap_cv, vmin=-0.2, vmax=1.2)\n",
    "    \n",
    "        # Set y-ticks and labels\n",
    "        y_ticks = np.arange(n_splits) + 0.5\n",
    "        ax.set(yticks=y_ticks, yticklabels=range(n_splits),\n",
    "               xlabel=\"Weather Station index (file_id)\", ylabel=\"Fold\",\n",
    "               ylim=[n_splits, -0.2], xlim=[0, xlim_max])\n",
    "    \n",
    "        # Set plot title and create legend\n",
    "        ax.set_title(\"KFold\", fontsize=14)\n",
    "        legend_patches = [Patch(color=cmap_cv(0.8), label=\"Testing set\"), \n",
    "                          Patch(color=cmap_cv(0.02), label=\"Training set\")]\n",
    "        ax.legend(handles=legend_patches, loc=(1.03, 0.8))\n",
    "    \n",
    "#Main\n",
    "\n",
    "#Configure\n",
    "n_seq = 10\n",
    "if n_seq > 2:\n",
    "    n_lag = 25 - n_seq\n",
    "else:\n",
    "    n_lag = 22\n",
    "n_time_steps = 27\n",
    "n_test = 1\n",
    "\n",
    "print(\"Model Parameters:\")\n",
    "print(\"n_lag (number of input time steps): \"+str(n_lag))\n",
    "print(\"n_seq (number of output/future prediction time steps): \"+str(n_seq))\n",
    "\n",
    "# Create 2D array with file_ids to use for sample creation\n",
    "array = np.array([\n",
    "    6501, 6541, 6640, 6668, 6678, \n",
    "    6687, 6697, 6714, 6744, 6772, \n",
    "    6783, 6840, 6844, 6854, 6870, \n",
    "    6891, 6895, 6899, 6901, 6909, \n",
    "    6929, 6950, 6963, 6969, 6994, \n",
    "    7032, 7057, 7094, 7095, 7100, \n",
    "    7108, 7116, 7119, 7131, 7139, \n",
    "    7152, 7155, 7156, 7182, 7193, \n",
    "    7202, 7239, 7280, 7286, 7287, \n",
    "    7311, 7321, 7329, 7347, 7350, \n",
    "    7354, 7357, 7361, 7414, 7423, \n",
    "    7424, 7432, 7463, 7482, 7489, \n",
    "    7528, 7531, 7534, 7538, 7549, \n",
    "    7553, 7555, 7562, 7571, 7573, \n",
    "    7574, 7575, 7585, 7599, 7603, \n",
    "    7606, 7622, 7652, 7671, 7704, \n",
    "    7786, 7805, 7816, 7838, 7861, \n",
    "    7862, 7863, 7870, 7892, 7907, \n",
    "    7938, 7962, 7979, 7987, 7999, \n",
    "    8000, 8034, 8083, 8120, 8133, \n",
    "    8184, 8186, 8247, 8248, 9858])\n",
    "    \n",
    "#Create arrays holding the 5-fold cross-validation indices gathered for consistency across models\n",
    "train_array = []\n",
    "test_array = []\n",
    "    \n",
    "train_array.append([5, 9, 21, 22, 41, 42, 45, 46, 52, 66, 68, 72, 79, 81, 82, 83, 90, 92, 95])\n",
    "test_array.append([4, 10, 47, 53, 94])\n",
    "    \n",
    "train_array.append([4, 9, 10, 21, 41, 45, 46, 47, 52, 53, 66, 68, 81, 82, 83, 90, 92, 94, 95])\n",
    "test_array.append([5, 22, 42, 72, 79])\n",
    "    \n",
    "train_array.append([4, 5, 10, 21, 22, 41, 42, 46, 47, 52, 53, 72, 79, 82, 83, 90, 92, 94, 95])\n",
    "test_array.append([9, 45, 66, 68, 81])\n",
    "    \n",
    "train_array.append([4, 5, 9, 10, 21, 22, 42, 45, 47, 52, 53, 66, 68, 72, 79, 81, 82, 92, 94, 95])\n",
    "test_array.append([41, 46, 83, 90])\n",
    "    \n",
    "train_array.append([4, 5, 9, 10, 22, 41, 42, 45, 46, 47, 53, 66, 68, 72, 79, 81, 83, 90, 94])\n",
    "test_array.append([21, 52, 82, 92, 95])\n",
    "    \n",
    "# Equations for three Principal Components from PCA using response variables combined with other predictors\n",
    "#PC1=-0.0002714X1+0.02612X2+0.03858X3-0.007658X4+0.001592X5-0.02087X6+0.8564X7-0.1468X8+0.01192X9-0.0001049X10+0.01913X11+0.02076X12\n",
    "#PC2=0.0003944X1+0.002204X2+0.01052X3+0.3248X4-0.0009976X5-0.04421X6+2.3406X7+0.06103X8+0.08841X9+0.00009018X10+0.05678X11-0.002022X12\n",
    "#PC3=-0.00007998X1-0.0006124X2-0.001063X3-0.01855X4+0.00001956X5+0.01170X6+0.6076X7+0.4664X8-0.002995X9+0.008185X10+0.8815X11-0.0004730X12\n",
    "    \n",
    "# Equations for three Principal Components from PCA omitting both response variables,\n",
    "#PC-1=-0.0004514X1+0.03194X2-0.04343X3+0.002243X4-0.02252X5+0.9877X6-0.2265X7+0.006144X8-0.0001488X9+0.02943X10\n",
    "#PC-2=0.0001702X1+0.005484X2+0.2057X3-0.0003188X4-0.02584X5+1.6963X6-0.05890X7+0.05809X8+1.9748X9+0.03686X10\n",
    "#PC-3=-0.00006323X1-0.001180X2-0.02384X3-0.00002833X4+0.01170X5+0.5204X6+0.4791X7-0.004318X8+0.008271X9+0.8765X10\n",
    "    \n",
    "# Get the current working directory \n",
    "current_directory = os.getcwd() \n",
    "    \n",
    "# Print the current working directory \n",
    "print(current_directory)\n",
    "    \n",
    "# Define the directory containing the files \n",
    "path = current_directory+\"\\\\Modeling\\\\\"\n",
    "print(path)\n",
    "    \n",
    "filename = path + 'Final_Yearly_Dataset.csv'\n",
    "    \n",
    "# load dataset\n",
    "df = read_csv(filename, header=0, parse_dates=[0], index_col=0, date_format='%Y-%m')\n",
    "    \n",
    "df = df.rename(columns={'Unnamed: 0' : 'indices'})\n",
    "    \n",
    "#Remove unused columns\n",
    "df = df.drop(['vapor_pressure'], axis=1)\n",
    "    \n",
    "# Round numbers in columns to reasonable precision,\n",
    "df['temperatures'] = np.round(df['temperatures'], 2)\n",
    "df['slp'] = np.round(df['slp'], 2)\n",
    "df['wet_bulb_temperature'] = np.round(df['wet_bulb_temperature'], 2)\n",
    "df['specific_humidity'] = np.round(df['specific_humidity'], 2)\n",
    "df['GHI'] = np.round(df['GHI'], 2)\n",
    "df['PRCP'] = np.round(df['PRCP'], 2)\n",
    "df['SNDP'] = np.round(df['SNDP'], 2)\n",
    "df['solar_activity'] = np.round(df['solar_activity'], 2)\n",
    "df['ONI'] = np.round(df['ONI'], 2)\n",
    "df['water'] = np.round(df['water'], 0)\n",
    "df['region'] = np.round(df['region'], 0)\n",
    "    \n",
    "df_trimmed = df[df['file_id'] != 7533] # Remove file_id 7533 so there are 105 weather stations for 5-fold CV\n",
    "df_trimmed = df_trimmed.drop(['Year', 'date', 'latitude', 'longitude', 'elevation'], axis=1)\n",
    "    \n",
    "X = []\n",
    "y = []\n",
    "    \n",
    "for i in array:\n",
    "    add_to_X = [] # create list to store each column to add to X\n",
    "    new_df = df_trimmed[df_trimmed['file_id'] == i].drop(['file_id'], axis=1)\n",
    "    add_to_y = []\n",
    "    for j in range(new_df.shape[0]):\n",
    "        add_to_y.append(new_df['temperatures'].iloc[j])\n",
    "    y.append(add_to_y)\n",
    "    #new_df = new_df.drop(['temperatures'], axis=1)\n",
    "    columns_list = new_df.columns.tolist()\n",
    "    for j in range(new_df.shape[0]):\n",
    "        l=0\n",
    "        new_row = []\n",
    "        for m in columns_list:\n",
    "            new_row.append(new_df.iloc[j, l])\n",
    "            l += 1\n",
    "        add_to_X.append(new_row)\n",
    "    X.append(add_to_X)\n",
    "\n",
    "print(df_trimmed)\n",
    "print(new_df)\n",
    "print(X[0])\n",
    "\n",
    "#Perform k-fold cross-validation\n",
    "#Taken from: https://www.geeksforgeeks.org/cross-validation-using-k-fold-with-scikit-learn/\n",
    "    \n",
    "#k = 5  # Number of folds\n",
    "#kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "#for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "#    print(f\"Fold {i}:\")\n",
    "#    print(f\"  Training dataset index: {train_index}\")\n",
    "#    print(f\"  Test dataset index: {test_index}\")\n",
    "    \n",
    "#for train_indices, test_indices in kf.split(X):\n",
    "#    print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    \n",
    "# Create figure and axis\n",
    "#fig, ax = plt.subplots(figsize=(6, 3))\n",
    "#plot_kfold(kf, X, y, ax, k)\n",
    "#plt.tight_layout()\n",
    "#fig.subplots_adjust(right=0.6)\n",
    "    \n",
    "#Create train and test sets for each cross-validation split\n",
    "train_X = []\n",
    "train_y = []\n",
    "val_X = []\n",
    "val_y = []\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}\")\n",
    "    #Add each corresponding sample for each entry of train index \n",
    "    train_X_rows = [] # Stores all the samples for one fold of train_X\n",
    "    train_y_rows = [] # Stores all the samples for one fold of train_y\n",
    "    for j in train_array[i]:\n",
    "        train_X_rows.append(X[j])\n",
    "        train_y_rows.append(y[j])\n",
    "    # Stores one fold of the train dataset\n",
    "    train_X.append(train_X_rows)\n",
    "    train_y.append(train_y_rows)\n",
    "    #Add each corresponding sample for each entry of the validation index \n",
    "    val_X_rows = [] # Stores all the samples for one fold of val_X\n",
    "    val_y_rows = [] # Stores all the samples for one fold of val_y\n",
    "    for j in test_array[i]: \n",
    "            val_X_rows.append(X[j])\n",
    "            val_y_rows.append(y[j])\n",
    "    # Stores one fold of the validation dataset\n",
    "    val_X.append(val_X_rows)\n",
    "    val_y.append(val_y_rows) \n",
    "    \n",
    "    #print(\"Train_X Fold \"+str(i)+\":\")\n",
    "    #print(len(train_X[i]))\n",
    "    #print(len(train_X[i][0]))\n",
    "    #print(len(train_X[i][0][0])) \n",
    "    #print(\"Train_y Fold \"+str(i)+\":\")\n",
    "    #print(len(train_y[i]))\n",
    "    #print(len(train_y[i][0]))\n",
    "    #print(train_y[i][0][0])\n",
    "    #print(\"Validation_X Fold \"+str(i)+\":\")\n",
    "    #print(len(val_X[i]))\n",
    "    #print(len(val_X[i][0]))\n",
    "    #print(len(val_X[i][0][0]))\n",
    "    #print(\"Validation_y Fold \"+str(i)+\":\")\n",
    "    #print(len(val_y[i]))\n",
    "    #print(len(val_y[i][0]))\n",
    "    #print(val_y[i][0][0])\n",
    "    \n",
    "#Convert 3D arrays to DataFrames\n",
    "df_X = []\n",
    "df_y = []\n",
    "val_df_X = []\n",
    "val_df_y = []\n",
    "dataset = []\n",
    "dataset_test = []\n",
    "scaler = []\n",
    "scaler_all_features = []\n",
    "train = []\n",
    "test = []\n",
    "validation = []\n",
    "\n",
    "for i in range(5):\n",
    "    dataset_scaling = [] # Holds all 84 weather station rows to train the scaling function\n",
    "    dataset_testing = [] # Holds remaining 21 weather station rows for testing and validation\n",
    "    print(\"Fold \"+str(i+1)+\":\")\n",
    "    #Transform train_X to the correct format\n",
    "    df1 = []\n",
    "    dataset_df = [] # captures each weather station's dataset as values for training scaler mapping\n",
    "    df_X.append(pd.DataFrame(train_X[i]))\n",
    "    X_t = df_X[i].transpose()\n",
    "    if i==3:\n",
    "        train_size = 20\n",
    "        val_size = 4\n",
    "    else:\n",
    "        train_size = 19\n",
    "        val_size = 5\n",
    "        \n",
    "    for k in range(train_size):\n",
    "        X = np.array(X_t.iloc[:, k])\n",
    "        df = pd.DataFrame()\n",
    "        for j in range(n_time_steps):\n",
    "            new_row = pd.DataFrame(X[j]).transpose()\n",
    "            new_row.columns = new_df.columns\n",
    "            # Add the new row\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.columns = new_df.columns\n",
    "        df1.append(df)\n",
    "        dataset_df.append(df.values)\n",
    "        dataset_scaling.append(df.values)\n",
    "    df_X[i] = df1\n",
    "    dataset.append(dataset_df)\n",
    "     \n",
    "\n",
    "    #print(len(dataset))\n",
    "    #print(len(dataset[0]))\n",
    "    #print(len(dataset_scaling))\n",
    "    #print(len(dataset_scaling[:][0]))\n",
    "    #print(\"Stop\")\n",
    "    \n",
    "    #Transform train_y to the correct format\n",
    "    df2 = []\n",
    "    df_y.append(pd.DataFrame(train_y[i]))\n",
    "    y_t = df_y[i].transpose()\n",
    "    \n",
    "    for j in range(train_size):\n",
    "        y = np.array(y_t.iloc[:, j])\n",
    "        y = pd.DataFrame(y)\n",
    "        y.columns = ['temperatures']\n",
    "        df2.append(y)\n",
    "    df_y[i] = df2\n",
    "\n",
    "    #Transform val_X to the correct format\n",
    "    df3 = []\n",
    "    dataset_vl_df = [] # captures each weather station's dataset as values for training scaler mapping\n",
    "    val_df_X.append(pd.DataFrame(val_X[i]))\n",
    "    val_X_t = val_df_X[i].transpose()\n",
    "    for k in range(val_size):\n",
    "        vl_X = np.array(val_X_t.iloc[:, k])\n",
    "        vl_df = pd.DataFrame()\n",
    "        for j in range(n_time_steps):\n",
    "            new1_row = pd.DataFrame(vl_X[j]).transpose()\n",
    "            new1_row.columns = new_df.columns\n",
    "            # Add the new row\n",
    "            vl_df = pd.concat([vl_df, new1_row], ignore_index=True)\n",
    "        vl_df.columns = new_df.columns\n",
    "        df3.append(vl_df)\n",
    "        dataset_vl_df.append(vl_df.values)\n",
    "        dataset_testing.append(vl_df.values)\n",
    "    val_df_X[i] = df3\n",
    "    dataset_test.append(dataset_vl_df)\n",
    "\n",
    "    #Transform val_y to the correct format\n",
    "    df4 = []\n",
    "    val_df_y.append(pd.DataFrame(train_y[i]))\n",
    "    val_y_t = val_df_y[i].transpose()\n",
    "    \n",
    "    for j in range(val_size):\n",
    "        v_y = np.array(val_y_t.iloc[:, j])\n",
    "        v_y = pd.DataFrame(v_y)\n",
    "        v_y.columns = ['temperatures']\n",
    "        df4.append(v_y)\n",
    "    val_df_y[i] = df4\n",
    "    \n",
    "    scaler.append(MinMaxScaler(feature_range=(0, 1)))\n",
    "    scaler_all_features.append(MinMaxScaler(feature_range=(0, 1)))\n",
    "    train.append([1])\n",
    "    test.append([1])\n",
    "    validation.append([1])\n",
    "    \n",
    "    # prepare data\n",
    "    scaler[i], scaler_all_features[i], train[i] = prepare_training_data(dataset_scaling, n_lag, n_seq, n_time_steps)\n",
    "\n",
    "    validation[i], test[i] = prepare_testing_and_validation_data(dataset_testing, n_lag, n_seq, n_time_steps, scaler_all_features[i])\n",
    "\n",
    "    #Reshape dimensionality\n",
    "    train1 = train[i]\n",
    "    test1 = test[i]\n",
    "    validation1 = validation[i]\n",
    "    print(np.array(train1).shape)\n",
    "    print(np.array(test1).shape)\n",
    "    print(np.array(validation1).shape)\n",
    "    train2 = []\n",
    "    test2 = []\n",
    "    validation2 = []\n",
    "\n",
    "    for k in range(train_size):\n",
    "        train2.append(train1[k][0])\n",
    "        \n",
    "    for k in range(2):\n",
    "        test2.append(test1[k][0])\n",
    "\n",
    "    for k in range(val_size-2):\n",
    "        validation2.append(validation1[k][0])\n",
    "\n",
    "    print(np.array(train2).shape)\n",
    "    print(np.array(test2).shape)\n",
    "    print(np.array(validation2).shape)\n",
    "\n",
    "    train[i] = train2\n",
    "    test[i] = test2\n",
    "    validation[i] = validation2\n",
    "\n",
    "    #Reshape dimensionality (again)\n",
    "    dim_size = n_seq + 12*n_lag\n",
    "    train1 = np.array(train[i]).reshape(train_size, dim_size)\n",
    "    test1 = np.array(test[i]).reshape(2, dim_size)\n",
    "    validation1 = np.array(validation[i]).reshape(val_size-2, dim_size)\n",
    "    train2 = pd.DataFrame(train1).values\n",
    "    test2 = pd.DataFrame(test1).values\n",
    "    validation2 = pd.DataFrame(validation1).values\n",
    "    print(np.array(train2).shape)\n",
    "    print(np.array(test2).shape)\n",
    "    print(np.array(validation2).shape)\n",
    "\n",
    "    print(train2)\n",
    "\n",
    "    train[i] = train2\n",
    "    test[i] = test2\n",
    "    validation[i] = validation2\n",
    "    \n",
    "    #X_train = train1[:][:-n_seq]\n",
    "    #y_train = train1[:][-n_seq:]\n",
    "    #X_test = test1[:][:-n_seq]\n",
    "    #y_test = test1[:][-n_seq:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be74bc78-33b4-4695-9874-daa61b380616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n",
      "(19, 190)\n",
      "(2, 190)\n"
     ]
    }
   ],
   "source": [
    "print(n_seq)\n",
    "print(n_lag)\n",
    "print(np.array(train[1]).shape)\n",
    "print(np.array(test[1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ad946-c0cb-407a-8a65-8b9aef8cf7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:19:18,842] A new study created in memory with name: no-name-562ca779-906a-4fa4-8753-cc66394e8b26\n",
      "[I 2025-06-04 20:20:31,331] Trial 0 finished with value: 0.11482663005590439 and parameters: {'lr': 0.0010462811708209353, 'optimizer': 'Adam', 'activation_function': 'relu', 'lstm_units': 512, 'dropout_rate': 0.4613902986358399}. Best is trial 0 with value: 0.11482663005590439.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.18084633350372314, 0.1067575141787529, 0.09851565212011337, 0.11355292052030563, 0.07446072995662689]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.11482663005590439\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.035554994311361145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:21:50,621] Trial 1 finished with value: 0.09138915538787842 and parameters: {'lr': 0.0016752058646333093, 'optimizer': 'Adam', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.3845464249434408}. Best is trial 1 with value: 0.09138915538787842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11542405188083649, 0.09367199242115021, 0.07481645047664642, 0.10439528524875641, 0.06863799691200256]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09138915538787842\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017574508632533077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:22:46,829] Trial 2 finished with value: 0.11517435908317566 and parameters: {'lr': 0.0011444331358042036, 'optimizer': 'Adam', 'activation_function': 'tanh', 'lstm_units': 256, 'dropout_rate': 0.24082842216943107}. Best is trial 1 with value: 0.09138915538787842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.14889872074127197, 0.09513804316520691, 0.09516801685094833, 0.1504819095134735, 0.08618510514497757]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.11517435908317566\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.02837618795964351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:24:40,419] Trial 3 finished with value: 19.02795066833496 and parameters: {'lr': 0.18236710614998514, 'optimizer': 'RMSprop', 'activation_function': 'tanh', 'lstm_units': 1024, 'dropout_rate': 0.37604226038734934}. Best is trial 1 with value: 0.09138915538787842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[20.110912322998047, 18.205955505371094, 21.067167282104492, 16.017559051513672, 19.7381591796875]\n",
      "Mean Cross Validation Accuracy:\n",
      "19.02795066833496\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "1.7648648791058064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:26:07,762] Trial 4 finished with value: 0.16350207030773162 and parameters: {'lr': 0.0018012753181300368, 'optimizer': 'RMSprop', 'activation_function': 'tanh', 'lstm_units': 512, 'dropout_rate': 0.2310592645988928}. Best is trial 1 with value: 0.09138915538787842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.1481618583202362, 0.20821073651313782, 0.14378388226032257, 0.17950958013534546, 0.1378442943096161]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.16350207030773162\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.02659908162930194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:27:07,257] Trial 5 finished with value: 0.09049471914768219 and parameters: {'lr': 0.0027243722987872164, 'optimizer': 'Adam', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.2912375416151961}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11294855922460556, 0.0823507159948349, 0.08149102330207825, 0.10818266123533249, 0.06750063598155975]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09049471914768219\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017280957244206172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:28:16,638] Trial 6 finished with value: 0.16899393498897552 and parameters: {'lr': 0.0701839623877717, 'optimizer': 'Adam', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.4943403847573362}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.21667438745498657, 0.18725545704364777, 0.12524402141571045, 0.15344633162021637, 0.16234947741031647]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.16899393498897552\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.031005936398734467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:30:43,595] Trial 7 finished with value: 0.9244011759757995 and parameters: {'lr': 0.032917739481936034, 'optimizer': 'RMSprop', 'activation_function': 'sigmoid', 'lstm_units': 1024, 'dropout_rate': 0.15924671146380276}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.91639244556427, 0.9293385148048401, 0.9747983813285828, 0.867315948009491, 0.934160590171814]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.9244011759757995\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.034590070258103546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:32:19,012] Trial 8 finished with value: 0.10154557675123214 and parameters: {'lr': 0.001004503415929118, 'optimizer': 'Adam', 'activation_function': 'relu', 'lstm_units': 1024, 'dropout_rate': 0.4212313156594475}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.16699983179569244, 0.09611789882183075, 0.0680055022239685, 0.10085933655500412, 0.07574531435966492]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.10154557675123214\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.034942181544676355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:34:08,826] Trial 9 finished with value: 115.36971540153027 and parameters: {'lr': 0.3610700806581219, 'optimizer': 'RMSprop', 'activation_function': 'relu', 'lstm_units': 1024, 'dropout_rate': 0.29687991131107205}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.25838711857795715, 0.1994762420654297, 0.20057810842990875, 575.868408203125, 0.32172733545303345]\n",
      "Mean Cross Validation Accuracy:\n",
      "115.36971540153027\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "230.2493507925617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:35:07,695] Trial 10 finished with value: 0.0906759649515152 and parameters: {'lr': 0.00845564514659213, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.13099534719841632}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11421477794647217, 0.0901370570063591, 0.07481592893600464, 0.10530617088079453, 0.06890588998794556]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.0906759649515152\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017285172668219207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:36:06,092] Trial 11 finished with value: 0.09058334678411484 and parameters: {'lr': 0.006974094880621282, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.10078035840995339}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11413122713565826, 0.08997903764247894, 0.07485217601060867, 0.10497827082872391, 0.0689760223031044]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09058334678411484\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.01718387559296343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:37:05,318] Trial 12 finished with value: 0.09055291712284089 and parameters: {'lr': 0.007034288807820131, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.18761278710977664}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11450541764497757, 0.08987994492053986, 0.07470406591892242, 0.10521737486124039, 0.06845778226852417]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09055291712284089\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017484865240108213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:38:05,673] Trial 13 finished with value: 0.09065482616424561 and parameters: {'lr': 0.0068894454042155604, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.2100045762507721}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11444901674985886, 0.0899161770939827, 0.07530016452074051, 0.10495100170373917, 0.0686577707529068]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09065482616424561\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017266880575746952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:38:59,964] Trial 14 finished with value: 0.090768101811409 and parameters: {'lr': 0.004137117373617651, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.3012357343251271}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11444257944822311, 0.08970393985509872, 0.07535755634307861, 0.10534488409757614, 0.06899154931306839]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.090768101811409\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017238323956858734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:39:53,299] Trial 15 finished with value: 0.09089085757732392 and parameters: {'lr': 0.018356579800846234, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.30494093492869734}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.1134205311536789, 0.0907762423157692, 0.07631535083055496, 0.10489199310541153, 0.06905017048120499]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09089085757732392\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.016691867085181628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:40:53,983] Trial 16 finished with value: 0.09052975475788116 and parameters: {'lr': 0.003293337220854691, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 512, 'dropout_rate': 0.18408772618733582}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11414065957069397, 0.08975476771593094, 0.0750148668885231, 0.10510539263486862, 0.06863308697938919]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09052975475788116\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.01726673365704337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:42:00,491] Trial 17 finished with value: 0.09437453001737595 and parameters: {'lr': 0.0034458151156105655, 'optimizer': 'Adam', 'activation_function': 'sigmoid', 'lstm_units': 512, 'dropout_rate': 0.261743395104897}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.12653768062591553, 0.09932099282741547, 0.07095993310213089, 0.10746210813522339, 0.06759193539619446]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09437453001737595\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.022341759893685215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:43:07,881] Trial 18 finished with value: 0.09335739463567734 and parameters: {'lr': 0.02054739015776662, 'optimizer': 'Adam', 'activation_function': 'relu', 'lstm_units': 512, 'dropout_rate': 0.3475115466885418}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11739447712898254, 0.0908898115158081, 0.07075563818216324, 0.11026248335838318, 0.07748456299304962]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09335739463567734\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.018068970668553033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:44:10,656] Trial 19 finished with value: 0.32677978873252866 and parameters: {'lr': 0.0029439925071787316, 'optimizer': 'SGD', 'activation_function': 'tanh', 'lstm_units': 512, 'dropout_rate': 0.17669066608647432}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.2780362367630005, 0.378471702337265, 0.3468554615974426, 0.3224700391292572, 0.3080655038356781]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.32677978873252866\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.03411691320312566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:45:13,444] Trial 20 finished with value: 0.11849746704101563 and parameters: {'lr': 0.053838161304869285, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 512, 'dropout_rate': 0.2700913719160924}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.1153460443019867, 0.14244917035102844, 0.11030074208974838, 0.12242785841226578, 0.10196352005004883]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.11849746704101563\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.013708546502740341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:46:08,376] Trial 21 finished with value: 0.09065844565629959 and parameters: {'lr': 0.012178977892874556, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.18695474389876277}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11402525007724762, 0.09002309292554855, 0.0749603882431984, 0.10537292808294296, 0.06891056895256042]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09065844565629959\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017218166933151556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:47:02,853] Trial 22 finished with value: 0.09060006737709045 and parameters: {'lr': 0.004160134551384418, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.15020502308391132}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11409487575292587, 0.08997564017772675, 0.07479333877563477, 0.10534046590328217, 0.06879601627588272]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09060006737709045\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017291296031007285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:48:06,350] Trial 23 finished with value: 0.0935567632317543 and parameters: {'lr': 0.9477744397697343, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 512, 'dropout_rate': 0.21396224706390266}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11568742245435715, 0.09243017435073853, 0.07563264667987823, 0.11486950516700745, 0.06916406750679016]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.0935567632317543\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.019295314497713812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:49:05,656] Trial 24 finished with value: 0.14214089661836624 and parameters: {'lr': 0.00253301353696239, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.1120868755811848}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.15075959265232086, 0.09270861744880676, 0.13947544991970062, 0.10491561144590378, 0.22284521162509918]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.14214089661836624\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.045659270840002535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:50:18,452] Trial 25 finished with value: 0.09481630623340606 and parameters: {'lr': 0.0060773614315966775, 'optimizer': 'Adam', 'activation_function': 'sigmoid', 'lstm_units': 512, 'dropout_rate': 0.1849481750730795}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.12298890203237534, 0.09234584122896194, 0.07687927782535553, 0.10448770225048065, 0.07737980782985687]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09481630623340606\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017430448743492265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:51:17,147] Trial 26 finished with value: 0.09094591289758683 and parameters: {'lr': 0.012781480309648832, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.3337789963686824}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11439865082502365, 0.09063947200775146, 0.07453279942274094, 0.10571442544460297, 0.0694442167878151]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09094591289758683\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017320202162206484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:52:23,847] Trial 27 finished with value: 0.16142771244049073 and parameters: {'lr': 0.002136398907898572, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.2500998246631624}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11621643602848053, 0.24766647815704346, 0.1467941254377365, 0.10105821490287781, 0.1954033076763153]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.16142771244049073\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.05384077154026914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:54:14,436] Trial 28 finished with value: 0.29542393088340757 and parameters: {'lr': 0.004357090675289263, 'optimizer': 'RMSprop', 'activation_function': 'tanh', 'lstm_units': 1024, 'dropout_rate': 0.2181787365369411}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.2913043797016144, 0.307065486907959, 0.29346850514411926, 0.3084915578365326, 0.27678972482681274]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.29542393088340757\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.011613067880450275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:55:25,352] Trial 29 finished with value: 0.10887841731309891 and parameters: {'lr': 0.013651575017630523, 'optimizer': 'Adam', 'activation_function': 'relu', 'lstm_units': 512, 'dropout_rate': 0.141618986648831}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.1176077350974083, 0.11556791514158249, 0.10249060392379761, 0.13248279690742493, 0.07624303549528122]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.10887841731309891\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.018889094274215815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:56:41,580] Trial 30 finished with value: 0.11901537626981736 and parameters: {'lr': 0.001444253411960304, 'optimizer': 'Adam', 'activation_function': 'relu', 'lstm_units': 512, 'dropout_rate': 0.2808473120847088}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.1368304193019867, 0.11957847326993942, 0.13988995552062988, 0.11528044193983078, 0.08349759131669998]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.11901537626981736\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.020143999989045116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:57:42,781] Trial 31 finished with value: 0.09064573794603348 and parameters: {'lr': 0.007791885686014618, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.11994689099214509}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.1140495166182518, 0.0902644470334053, 0.0746697261929512, 0.10556261241436005, 0.06868238747119904]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09064573794603348\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017366733132152864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:58:45,769] Trial 32 finished with value: 0.09051685780286789 and parameters: {'lr': 0.005678403318928586, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.11231784981728177}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11400330811738968, 0.08967306464910507, 0.07500983029603958, 0.10516603291034698, 0.06873205304145813]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09051685780286789\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.01721609297827294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 20:59:51,297] Trial 33 finished with value: 0.11124971956014633 and parameters: {'lr': 0.0022963198357939542, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.1649087444074648}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.14691144227981567, 0.13658106327056885, 0.07519150525331497, 0.10536342859268188, 0.09220115840435028]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.11124971956014633\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.02687424925078755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:01:10,151] Trial 34 finished with value: 0.09055189043283463 and parameters: {'lr': 0.00470575614693932, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.19755822058827374}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11415420472621918, 0.09018600732088089, 0.07442612200975418, 0.10518154501914978, 0.0688115730881691]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09055189043283463\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017343077634905477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:02:47,596] Trial 35 finished with value: 0.13289376795291902 and parameters: {'lr': 0.001661813686804534, 'optimizer': 'Adam', 'activation_function': 'tanh', 'lstm_units': 256, 'dropout_rate': 0.4163243459079391}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.13924261927604675, 0.09777335077524185, 0.1637762486934662, 0.15919001400470734, 0.1044866070151329]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.13289376795291902\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.0272981266079158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:04:09,697] Trial 36 finished with value: 0.09063174724578857 and parameters: {'lr': 0.00469599120760953, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.13804428266337612}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.1142459511756897, 0.0897241160273552, 0.07478460669517517, 0.10534321516752243, 0.06906084716320038]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09063174724578857\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017270305753447075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:05:15,640] Trial 37 finished with value: 0.1388636887073517 and parameters: {'lr': 0.001373097001760684, 'optimizer': 'RMSprop', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.24293645457420848}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.13281327486038208, 0.14917263388633728, 0.1143631637096405, 0.16513609886169434, 0.13283327221870422]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.1388636887073517\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017145046176263046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:06:25,108] Trial 38 finished with value: 0.20085490643978118 and parameters: {'lr': 0.03368810836770799, 'optimizer': 'Adam', 'activation_function': 'tanh', 'lstm_units': 512, 'dropout_rate': 0.3310311195167001}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.18630360066890717, 0.16835430264472961, 0.22850346565246582, 0.14827236533164978, 0.27284079790115356]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.20085490643978118\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.044675193949426865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:08:38,173] Trial 39 finished with value: 0.12333502769470214 and parameters: {'lr': 0.002835000702471803, 'optimizer': 'RMSprop', 'activation_function': 'sigmoid', 'lstm_units': 1024, 'dropout_rate': 0.210919460061499}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11018455773591995, 0.13079388439655304, 0.13312390446662903, 0.131906196475029, 0.11066659539937973]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.12333502769470214\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.010567361288169968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:09:43,337] Trial 40 finished with value: 0.16639876663684844 and parameters: {'lr': 0.15445066748952013, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.3802694997335541}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.18292993307113647, 0.17619283497333527, 0.1377500742673874, 0.1697324961423874, 0.16538849472999573]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.16639876663684844\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.015505387544204097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:10:47,145] Trial 41 finished with value: 0.09053884446620941 and parameters: {'lr': 0.005563906441707295, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.18614394572709303}. Best is trial 5 with value: 0.09049471914768219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11460188031196594, 0.08963753283023834, 0.07504510879516602, 0.10505019128322601, 0.06835950911045074]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09053884446620941\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017449355318122257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:11:51,292] Trial 42 finished with value: 0.09034176021814347 and parameters: {'lr': 0.009376135927050826, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.17160486864030267}. Best is trial 42 with value: 0.09034176021814347.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11368945240974426, 0.08994325995445251, 0.07417432218790054, 0.10533606261014938, 0.06856570392847061]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09034176021814347\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017353301226853402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:12:44,762] Trial 43 finished with value: 0.09064469039440155 and parameters: {'lr': 0.010930177546698417, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.15495807136586068}. Best is trial 42 with value: 0.09034176021814347.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11415761709213257, 0.08976545929908752, 0.07502558827400208, 0.10514400899410248, 0.0691307783126831]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09064469039440155\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017150310627944036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:13:39,023] Trial 44 finished with value: 0.09016338735818863 and parameters: {'lr': 0.021959211767046075, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 256, 'dropout_rate': 0.16810026842760226}. Best is trial 44 with value: 0.09016338735818863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11317278444766998, 0.08946311473846436, 0.07415103167295456, 0.10570037364959717, 0.06832963228225708]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09016338735818863\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.01734606050081113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:15:04,441] Trial 45 finished with value: 0.0903822049498558 and parameters: {'lr': 0.0233817459111804, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 1024, 'dropout_rate': 0.12654579244889536}. Best is trial 44 with value: 0.09016338735818863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11410270631313324, 0.08946763724088669, 0.07468166947364807, 0.10524162650108337, 0.06841738522052765]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.0903822049498558\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017396902872857358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:16:58,879] Trial 46 finished with value: 0.11183049231767654 and parameters: {'lr': 0.020599611199095858, 'optimizer': 'Adam', 'activation_function': 'relu', 'lstm_units': 1024, 'dropout_rate': 0.10467359779728995}. Best is trial 44 with value: 0.09016338735818863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.12195645272731781, 0.1135898232460022, 0.10945595055818558, 0.1223318949341774, 0.09181834012269974]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.11183049231767654\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.011153783756997604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:18:45,153] Trial 47 finished with value: 0.11433520466089249 and parameters: {'lr': 0.05836526334889622, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 1024, 'dropout_rate': 0.12247429958392166}. Best is trial 44 with value: 0.09016338735818863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.12981384992599487, 0.10394042730331421, 0.09050430357456207, 0.14810822904109955, 0.09930921345949173]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.11433520466089249\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.021363282938320113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:20:37,495] Trial 48 finished with value: 0.6537213802337647 and parameters: {'lr': 0.030757323519371185, 'optimizer': 'RMSprop', 'activation_function': 'sigmoid', 'lstm_units': 1024, 'dropout_rate': 0.49429370209072737}. Best is trial 44 with value: 0.09016338735818863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.7691289782524109, 0.5826026797294617, 0.5708266496658325, 0.5965745449066162, 0.749474048614502]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.6537213802337647\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.08681319472030034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:22:01,485] Trial 49 finished with value: 0.15906976908445358 and parameters: {'lr': 0.08498525550209308, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 1024, 'dropout_rate': 0.16150255171919786}. Best is trial 44 with value: 0.09016338735818863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.20740577578544617, 0.12169186025857925, 0.14240166544914246, 0.18254244327545166, 0.14130710065364838]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.15906976908445358\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.031233898617530016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:23:12,061] Trial 50 finished with value: 0.1332777827978134 and parameters: {'lr': 0.025897696117410024, 'optimizer': 'Adam', 'activation_function': 'tanh', 'lstm_units': 256, 'dropout_rate': 0.13448263861692483}. Best is trial 44 with value: 0.09016338735818863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.17211534082889557, 0.1123085469007492, 0.15259258449077606, 0.1190151795744896, 0.11035726219415665]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.1332777827978134\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.02469776689424165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:24:57,429] Trial 51 finished with value: 0.09048675745725632 and parameters: {'lr': 0.009683723635361975, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 1024, 'dropout_rate': 0.1725016553990139}. Best is trial 44 with value: 0.09016338735818863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11358983814716339, 0.08988092839717865, 0.07505018264055252, 0.10481944680213928, 0.06909339129924774]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09048675745725632\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.016944080427769657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:26:22,829] Trial 52 finished with value: 0.09063587337732315 and parameters: {'lr': 0.008885626997441743, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 1024, 'dropout_rate': 0.23262725254162658}. Best is trial 44 with value: 0.09016338735818863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11408638209104538, 0.09008990973234177, 0.07463500648736954, 0.10553282499313354, 0.06883524358272552]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09063587337732315\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.01734035065013918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:27:44,760] Trial 53 finished with value: 0.0940044328570366 and parameters: {'lr': 0.040857013222355146, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 1024, 'dropout_rate': 0.16532478779254822}. Best is trial 44 with value: 0.09016338735818863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11323942244052887, 0.08946699649095535, 0.07596222311258316, 0.10284280776977539, 0.08851071447134018]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.0940044328570366\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.012840500755933256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 21:29:09,554] Trial 54 finished with value: 0.09078403115272522 and parameters: {'lr': 0.0162512054901091, 'optimizer': 'SGD', 'activation_function': 'sigmoid', 'lstm_units': 1024, 'dropout_rate': 0.10131128559155048}. Best is trial 44 with value: 0.09016338735818863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracies:\n",
      "[0.11454740166664124, 0.09004078805446625, 0.07530254870653152, 0.1050596535205841, 0.06896976381540298]\n",
      "Mean Cross Validation Accuracy:\n",
      "0.09078403115272522\n",
      "Standard Deviation of Cross Validation Accuracy:\n",
      "0.017231263103269258\n"
     ]
    }
   ],
   "source": [
    "n_batch = 1\n",
    "nb_epoch = 100\n",
    "\n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def mape (y_true, y_pred):\n",
    "    return 100*K.mean(K.sqrt(K.square(y_true - y_pred))/y_true)\n",
    "    \n",
    "def pearson (y_true, y_pred):\n",
    "    return (K.square(K.mean((y_true - K.mean(y_true))*(y_pred - K.mean(y_pred)))))/(K.mean(K.square(y_true - K.mean(y_true)))*K.mean(K.square(y_pred - K.mean(y_pred))))\n",
    " \n",
    "# fit an LSTM network to training data\n",
    "#Adapted from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def create_model(trial, X, y, n_lag, n_seq, n_batch, nb_epoch):\n",
    "\n",
    "    cv_accuracies = []\n",
    "    \n",
    "    #Parameters:\n",
    "    #trial (array-like): Optuna parameters.\n",
    "    #train (array-like): Target values.\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations\n",
    "    #nb_epoch (int): Maximum number of epochs\n",
    "    \n",
    "    # Hyperparameters to be tuned by Optuna (taken from Javier Leon's dissertation 'Fruit Prices')\n",
    "    lr = trial.suggest_float('lr', 1e-3, 1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=lr)\n",
    "    \n",
    "    #Optuna will try either Rectified Linear Unit (ReLU) = max(0, x), tanh, or sigmoid functions\n",
    "    activation_function = trial.suggest_categorical('activation_function', ['relu', 'tanh', 'sigmoid'])\n",
    "    \n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [256, 512, 1024])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, return_sequences=True, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(256, input_dim=y.shape[1], activation=activation_function))\n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    model.add(Dense(n_seq))\n",
    "    model.compile(loss=rmse, optimizer=optimizer)\n",
    "        \n",
    "    return model\n",
    "    \n",
    "def objective(trial):\n",
    "    \n",
    "    cv_accuracies = []\n",
    "        \n",
    "    for i in range(5):\n",
    "        train1 = train[i]\n",
    "        test1 = test[i]\n",
    "        validation1 = validation[i]\n",
    "\n",
    "        X, y = train1[:, 0:-n_seq], train1[:, -n_seq:]\n",
    "        X_test, y_test = test1[:, 0:-n_seq], test1[:, -n_seq:]\n",
    "        X_val, y_val = validation1[:, 0:-n_seq], validation1[:, -n_seq:]\n",
    "        X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "        X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "        X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "    \n",
    "        model = create_model(trial, X, y, n_lag, n_seq, n_batch, nb_epoch)\n",
    "\n",
    "        history = model.fit(X, y, validation_data=(X_val, y_val), epochs=nb_epoch, verbose=0)\n",
    "\n",
    "        cv_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "        loss = history.history['val_loss'][-1]\n",
    "    \n",
    "        # Plotting the training and validation loss\n",
    "        #plt.figure(figsize=(10, 4))\n",
    "        #plt.plot(history.history['loss'], label='Training Loss')\n",
    "        #plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        #title1 = \"LSTM Training and Validation Loss for Fold \" + str(i+1)\n",
    "        #plt.title(title1)\n",
    "        #plt.xlabel('Epoch')\n",
    "        #plt.ylabel('Root Mean Squared Error Loss')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "            \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "    \n",
    "    print(\"Cross Validation Accuracies:\")\n",
    "    print(cv_accuracies)\n",
    "    print(\"Mean Cross Validation Accuracy:\")\n",
    "    print(np.mean(cv_accuracies))\n",
    "    print(\"Standard Deviation of Cross Validation Accuracy:\")\n",
    "    print(np.std(cv_accuracies))\n",
    "        \n",
    "    return np.mean(cv_accuracies)\n",
    "\n",
    "n_batch = 1\n",
    "nb_epoch = 100\n",
    "\n",
    "# optimize and fit model\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "\n",
    "print('Best trial:', study.best_trial.params)\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Fold \"+str(i+1)+\":\")\n",
    "    train1 = train[i]\n",
    "    test1 = test[i]\n",
    "    validation1 = validation[i]\n",
    "\n",
    "    X, y = train1[:, 0:-n_seq], train1[:, -n_seq:]\n",
    "    X_test, y_test = test1[:, 0:-n_seq], test1[:, -n_seq:]\n",
    "    X_val, y_val = validation1[:, 0:-n_seq], validation1[:, -n_seq:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "    X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "\n",
    "    best_model = create_model(optuna.trial.FixedTrial(best_params), X, y, n_lag, n_seq, n_batch, nb_epoch)\n",
    "    history = best_model.fit(X, y, epochs=100, batch_size=n_batch, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    title1 = \"CNN-LSTM Training and Validation Loss for Best Model\"\n",
    "    plt.title(title1)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Root Mean Squared Error Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f'Test Loss: {loss}')\n",
    "\n",
    "    # make forecasts\n",
    "    forecasts = make_forecasts(best_model, X_test, y_test, n_lag, n_seq, n_test)\n",
    "    \n",
    "    # inverse transform forecasts and test\n",
    "    forecasts = inverse_transform(series, forecasts, scaler[i], n_test+2, n_seq)\n",
    "    actual = [row[-n_seq:] for row in test1]\n",
    "    actual = inverse_transform(series, actual, scaler[i], n_test+2, n_seq)\n",
    "\n",
    "    #print(\"Forecasts:\")\n",
    "    #print(forecasts)\n",
    "    #print(\"Actual:\")\n",
    "    #print(actual)\n",
    "\n",
    "    rmse_list = [] # list stores root mean squared errors for each future time prediction\n",
    "    mae_list = [] # list stores root mean squared errors for each future time prediction\n",
    "    mape_list = [] # list stores root mean squared errors for each future time prediction\n",
    "    r2_list = [] # list stores root mean squared errors for each future time prediction\n",
    "    \n",
    "    # evaluate forecasts\n",
    "    rmse_list, mae_list, mape_list, r2_list = evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "\n",
    "    for j in range(len(X_test)):\n",
    "        print(\"Weather Station \"+str(j+1)+\":\")\n",
    "        dataset_df = pd.DataFrame(y_test[j].flatten())\n",
    "        dataset = dataset_df.values\n",
    "        dataset = dataset[:, 0]\n",
    "        dataset = np.array(dataset).reshape(-1, 1)\n",
    "        dataset = dataset.flatten()\n",
    "        dataset = pd.DataFrame(dataset)\n",
    "        series = scaler[i].inverse_transform(dataset)\n",
    "        series = pd.Series(series.flatten())\n",
    "\n",
    "    # plot forecasts\n",
    "    plot_forecasts(series, forecasts, 1)\n",
    "    \n",
    "    best_model.summary()\n",
    "\n",
    "    # Print out table of actual and predicted values for each weather station\n",
    "    for j in range(11):\n",
    "        print(\"Weather Station \"+str(j+1)+\":\")\n",
    "        print(\"Actual Temp\\tPredicted Temp\\tDifference\")\n",
    "        print(\"-----------\\t--------------\\t----------\")\n",
    "        for k in range(n_seq):\n",
    "            diff = forecasts[j][k] - actual[j][k]\n",
    "            print(f\"{actual[j][k]:.2f}\\t\\t{forecasts[j][k]:.2f}\\t\\t{diff:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e0a49-8fc2-4e2f-8bf4-7bb9dde70c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e54bcc-51b3-4db4-adf7-a8041e2b237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16289125-ffc3-4177-a011-ecba68a7ffe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ec920-1d52-457b-b12f-421c8bfa1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study, params=['optimizer', 'activation_function', 'lstm_units'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe2ed1d-7fe4-445f-91bc-135f6dec0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study, params=['lr', 'dropout_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c7a5a-83c7-4dac-9e14-04bbba312ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# fit the best LSTM model using parameters found by Optuna\n",
    "def create_best_model(X, y, n_lag, n_seq, n_batch, nb_epoch):\n",
    "    \n",
    "    #Parameters:\n",
    "    #trial (array-like): Optuna parameters.\n",
    "    #train (array-like): Target values.\n",
    "    #n_lag (int): Number of lag observations.\n",
    "    #n_seq (int): Number of sequence observations\n",
    "    #nb_epoch (int): Maximum number of epochs\n",
    "    \n",
    "    # Hyperparameters from the best model\n",
    "    lr = 0.0017884462854274694\n",
    "    optimizer = Adam(learning_rate = lr)\n",
    "    activation_function = 'sigmoid'\n",
    "    lstm_units = 256\n",
    "    dropout_rate = 0.24151358736199507\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    #model.add(Conv1D(filters=filters, kernel_size=1, activation='relu', input_shape=(X.shape[1], X.shape[2]))) # CNN-LSTM only\n",
    "    model.add(LSTM(lstm_units, return_sequences=True, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(256, input_dim=y.shape[1], activation=activation_function))\n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    model.add(Dense(n_seq))\n",
    "    model.compile(loss=rmse, optimizer=optimizer, metrics=['accuracy', 'mae', rmse, mape, pearson])\n",
    "        \n",
    "    return model\n",
    "\n",
    "# make one forecast with an LSTM,\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def forecast_lstm(model, X, n_seq, n_test):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, X.shape[1])\n",
    "    # make forecast\n",
    "    forecast = model.predict(X)\n",
    "    # convert to array\n",
    "    return forecast\n",
    "     \n",
    "# evaluate the persistence model\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def make_forecasts(model, X_test, y_test, n_lag, n_seq, n_test):\n",
    "    forecasts = list()\n",
    "    for i in range(len(X_test)):\n",
    "        X, y = X_test[i, :], y_test[i, :]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_seq, n_test)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    "     \n",
    "# invert differenced forecast\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    # invert first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob)\n",
    "    # propagate difference forecast using inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted\n",
    "     \n",
    "# inverse data transform on forecasts\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def inverse_transform(series, forecasts, scaler, n_test, n_seq):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = forecasts[i]\n",
    "        forecast = forecast.reshape(1, n_seq)\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i\n",
    "        last_ob = series.values[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted\n",
    "     \n",
    "# evaluate the RMSE for each forecast time step\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    rmse_list = []\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    r2_list = []\n",
    "    for i in range(n_seq):\n",
    "        actual = [row[i] for row in test]\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "        mae = mean_absolute_error(actual, predicted)\n",
    "        mse = mean_squared_error(actual, predicted)\n",
    "        mape = mean_absolute_percentage_error(actual, predicted)\n",
    "        r2 = r2_score(actual, predicted) \n",
    "        rmse_list.append(rmse)\n",
    "        mae_list.append(mae)\n",
    "        mape_list.append(mape)\n",
    "        r2_list.append(r2)\n",
    "        print(\"Year at t+\"+str(i+1)+\":\")\n",
    "        print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "        print('t+%d MAE: %f' % ((i+1), mae))\n",
    "        print('t+%d MAPE: %f' % ((i+1), mape))\n",
    "        print('t+%d R2_SCORE: %f' % ((i+1), r2))\n",
    "\n",
    "    return rmse_list, mae_list, mape_list, r2_list\n",
    "     \n",
    "# plot the forecasts in the context of the original dataset\n",
    "#Taken from: https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "def plot_forecasts(series, forecasts, n_test, n_seq, n_lag):\n",
    "    # plot the entire dataset in blue\n",
    "    plt.plot(series[:n_lag+2+len(forecasts)].index, series[:n_lag+2+len(forecasts)].values)\n",
    "    # plot the forecasts in red\n",
    "    off_s = n_lag + 1\n",
    "    off_e = off_s + len(forecasts) + 1\n",
    "    xaxis = [1998+x for x in range(off_s, off_e)]\n",
    "    yaxis = [series.values[off_s]] + forecasts\n",
    "    print(xaxis)\n",
    "    print(yaxis)\n",
    "    plt.plot(xaxis, yaxis, color='red')\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Temperature\")\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "\n",
    "n_batch = 1\n",
    "n_test = 2\n",
    "nb_epoch = 100\n",
    "    \n",
    "rmse_avg_list = []\n",
    "mae_avg_list = []\n",
    "mape_avg_list = []\n",
    "r2_score_avg_list = []\n",
    "forecast_results = []\n",
    "actual_results = []\n",
    "    \n",
    "for i in range(5):\n",
    "    train1 = train[i]\n",
    "    test1 = test[i]\n",
    "    validation1 = validation[i]\n",
    "\n",
    "    X = train1[:, 0:-n_seq]\n",
    "    y = train1[:, -n_seq:]\n",
    "    X_test = test1[:, 0:-n_seq]\n",
    "    y_test = test1[:, -n_seq:]\n",
    "    X_val = validation1[:, 0:-n_seq]\n",
    "    y_val = validation1[:, -n_seq:]\n",
    "    \n",
    "    dataset_df = pd.DataFrame(val_y[i])\n",
    "    dataset_df = dataset_df.iloc[0:2, :]\n",
    "    dataset = dataset_df.values\n",
    "\n",
    "    print(dataset)\n",
    "\n",
    "    series = pd.Series(dataset[:, 0]) # Using first column (temperatures)\n",
    "\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    X1 = X_test\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "    X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "\n",
    "    best_model = create_best_model(X, y, n_lag, n_seq, n_batch, nb_epoch)\n",
    "    history = best_model.fit(X, y, epochs=100, batch_size=n_batch, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f'Test Loss: {loss}')\n",
    "\n",
    "    # make forecasts\n",
    "    forecasts = make_forecasts(best_model, X_test, y_test, n_lag, n_seq, n_test)\n",
    "    \n",
    "    # inverse transform forecasts and test\n",
    "    forecasts = inverse_transform(series, forecasts, scaler[i], n_test, n_seq)\n",
    "    actual = [row[-n_seq:] for row in test1]\n",
    "    actual = inverse_transform(series, actual, scaler[i], n_test, n_seq)\n",
    "\n",
    "    rmse_list = [] # list stores root mean squared errors for each future time prediction\n",
    "    mae_list = [] # list stores root mean squared errors for each future time prediction\n",
    "    mape_list = [] # list stores root mean squared errors for each future time prediction\n",
    "    r2_list = [] # list stores root mean squared errors for each future time prediction\n",
    "\n",
    "    # evaluate forecasts\n",
    "    rmse_list, mae_list, mape_list, r2_list = evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "\n",
    "    rmse_avg_list.append(rmse_list)\n",
    "    mae_avg_list.append(mae_list)\n",
    "    mape_avg_list.append(mape_list)\n",
    "    r2_score_avg_list.append(r2_list)\n",
    "    forecast_results.append(forecasts)\n",
    "    actual_results.append(actual)\n",
    "\n",
    "    for j in range(2):\n",
    "        print(\"Weather Station \"+str(j+1)+\":\")\n",
    "        print(\"Actual Temp\\tPredicted Temp\\tDifference\")\n",
    "        print(\"-----------\\t--------------\\t----------\")\n",
    "        for k in range(n_seq):\n",
    "            diff = forecasts[j][k] - actual[j][k]\n",
    "            print(f\"{actual[j][k]:.2f}\\t\\t{forecasts[j][k]:.2f}\\t\\t{diff:.2f}\")\n",
    "    \n",
    "        # plot forecasts\n",
    "        dataset_df = pd.DataFrame(val_y[i])\n",
    "        dataset_df = dataset_df.iloc[0:2, :]\n",
    "        dataset_df = dataset_df.transpose()\n",
    "        dataset = dataset_df.values\n",
    "\n",
    "        series_ws = pd.Series(dataset[:, j]) # Using first column (temperatures)\n",
    "        forecasts_ws = forecasts[j]\n",
    "\n",
    "        # Set index starting from 1998\n",
    "        series_ws.index = range(1998, 1998 + len(series_ws))        \n",
    "        \n",
    "        plot_forecasts(series_ws, forecasts_ws, n_test, n_seq, n_lag)\n",
    "\n",
    "    # Print out plots of actual and predicted values for weather stations\n",
    "    ws = [1,2]\n",
    "    print(ws)\n",
    "    a = []\n",
    "    f = []\n",
    "    for q in range(len(actual)):\n",
    "        x = actual[q]\n",
    "        a.append(x[0])\n",
    "    for q in range(len(forecasts)):\n",
    "        x = forecasts[q]\n",
    "        f.append(x[0])\n",
    "    print(a)\n",
    "    print(f)\n",
    "    # Create a DataFrame for plotting\n",
    "    results_df = pd.DataFrame({\n",
    "        'ws': ws,\n",
    "        'Actual': a,\n",
    "        'Predicted': f\n",
    "    })\n",
    "\n",
    "    for k in range(n_seq):\n",
    "        print(\"Predictions for (t+\"+str(k)+\"):\")\n",
    "        # Print out plots of actual and predicted values for each weather station\n",
    "        results = []\n",
    "\n",
    "        # Create a DataFrame for plotting\n",
    "        for j in range(2):\n",
    "            results.append([j, actual[j][k], forecasts[j][k]])\n",
    "               \n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.columns = ['Weather_Station', 'Actual', 'Predicted']\n",
    "\n",
    "        print(results_df)\n",
    "            \n",
    "        # Plotting the results\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.scatter(results_df['Weather_Station'], results_df['Actual'], label='Actual')\n",
    "        plt.scatter(results_df['Weather_Station'], results_df['Predicted'], label='Predicted', alpha=0.7)\n",
    "        title1='LSTM Model Comparison Temperature Prediction (Fold='+str(i)+', n_seq='+str(n_seq)+', features=All)'\n",
    "        plt.title(title1)\n",
    "        plt.xlabel('Weather Station')\n",
    "        plt.ylabel('Temperature')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    best_model.summary()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    title2 = 'LSTM Training and Validation Loss (Fold='+str(i)+', n_seq='+str(n_seq)+', features=All)'\n",
    "    plt.title(title2)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "print(rmse_avg_list)\n",
    "print(mae_avg_list)\n",
    "print(mape_avg_list)\n",
    "print(r2_score_avg_list)\n",
    "print(forecast_results)\n",
    "print(actual_results)\n",
    "print(\"Accuracy Results:\")\n",
    "print(\"RMSE for each fold:\")\n",
    "print(rmse_avg_list)\n",
    "print(\"Average RMSE:\"+str(np.mean(rmse_avg_list)))\n",
    "print(\"Average MAE:\"+str(np.mean(mae_avg_list)))\n",
    "print(\"Average MAPE:\"+str(np.mean(mape_avg_list)))\n",
    "print(\"Average R2 Score:\"+str(np.mean(r2_score_avg_list)))\n",
    "\n",
    "# Plot model architecture\n",
    "filename = \"lstm_model_optimized_CV_S10.png\"\n",
    "plot_model(best_model, to_file=filename, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2698c0-5237-41ef-b2ee-cb7e2b833453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed648019-0f5f-4c80-97e4-a40c0ca88069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
